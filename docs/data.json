[{"permalink":"//","layout":"default","title":null,"content":"<a href=\"https://www.top500.org/\"><img src=\"https://www.top500.org/static//images/Top500_logo.png\" align=\"right\"></a>\n\n# CSD3 applications\n\n![CI](https://github.com/rundocs/jekyll-rtd-theme/workflows/CI/badge.svg?branch=develop)\n![jsDelivr](https://data.jsdelivr.com/v1/package/gh/rundocs/jekyll-rtd-theme/badge)\n\n## The collection covers system and other software.\n\n```mermaid\ngraph LR;\ncsd3 --> system[\"THE SYSTEM\"]\ncsd3 --> cardio[\"CARDIO\"]\ncsd3 --> applications[\"APPLICATIONS\"]\ncsd3 --> wr[\"WRITING RELATED\"]\nsystem --> System\nsystem --> Login\nsystem --> sysdot[\"...\"]\nsystem --> Contacts\ncardio --> cp[\"CSD3 partition\"]\ncardio --> lm[\"Legacy materials\"]\napplications --> ABCtoolbox\napplications --> appdot[\"...\"]\napplications --> R[\"R packages\"]\nR --> brms\nR --> Rdot[\"...\"]\nR --> xlsx\napplications --> R/xlsx\nwr --> fr[\"Front matter\"]\nwr --> wrdot[\"...\"]\nwr --> td[\"Test documentation\"]\n```\n\n{% include list.liquid all=true %}\n","dir":"/","name":"README.md","path":"README.md","url":"/"},{"sort":1,"layout":"default","title":"ABCtoolbox","content":"<h1 id=\"abctoolbox\">ABCtoolbox</h1>\n\n<p>Web page: https://bitbucket.org/wegmannlab/abctoolbox/wiki/Home</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>git clone <span class=\"nt\">--depth</span> 1 https://bitbucket.org/wegmannlab/abctoolbox.git\n<span class=\"c\"># g++ -O3 -o ABCtoolbox *.cpp</span>\nmodule load openmpi/3.1.4-gcc-7.2.0\ng++ <span class=\"nt\">-O3</span> <span class=\"nt\">-o</span> ABCtoolbox <span class=\"k\">*</span>.cpp <span class=\"nt\">-DUSE_OMP</span> <span class=\"nt\">-fopenmp</span>\n</code></pre> </div></div>\n\n<p>Note that the installation guide has misspecified the repository.</p>\n","dir":"/applications/","name":"ABCtoolbox.md","path":"applications/ABCtoolbox.md","url":"/applications/ABCtoolbox.html"},{"sort":1,"layout":"default","title":"R/brms","content":"<h1 id=\"rbrms\">R/brms</h1>\n\n<p>It is available from CRAN, so it can be installed with <code class=\"language-plaintext highlighter-rouge\">install.packages()</code>. Here is a documentation example,</p>\n\n<div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"w\">  </span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">brms</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"c1\"># Survival regression modeling the time between the first</span><span class=\"w\">\n</span><span class=\"c1\"># and second recurrence of an infection in kidney patients.</span><span class=\"w\">\n  </span><span class=\"n\">fit3</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">brm</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">cens</span><span class=\"p\">(</span><span class=\"n\">censored</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"o\">~</span><span class=\"w\"> </span><span class=\"n\">age</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">sex</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">disease</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"m\">1</span><span class=\"o\">|</span><span class=\"n\">patient</span><span class=\"p\">),</span><span class=\"w\">\n              </span><span class=\"n\">data</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">kidney</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">family</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">lognormal</span><span class=\"p\">())</span><span class=\"w\">\n  </span><span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">fit3</span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">fit3</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">ask</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"kc\">FALSE</span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">conditional_effects</span><span class=\"p\">(</span><span class=\"n\">fit3</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">ask</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"kc\">FALSE</span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"n\">fit3cox</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">brm</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"n\">cens</span><span class=\"p\">(</span><span class=\"n\">censored</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"o\">~</span><span class=\"w\"> </span><span class=\"n\">age</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">sex</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">disease</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"m\">1</span><span class=\"o\">|</span><span class=\"n\">patient</span><span class=\"p\">),</span><span class=\"w\">\n                 </span><span class=\"n\">data</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">kidney</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">family</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">family</span><span class=\"p\">(</span><span class=\"s1\">'cox'</span><span class=\"p\">))</span><span class=\"w\">\n  </span><span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">fit3cox</span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">fit3cox</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">ask</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"kc\">FALSE</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre> </div></div>\n\n<p>In case of compilng error, it is necessary to use the developmental version of rstan (2.21.3 at the time of writing), see below.</p>\n","dir":"/applications/R/","name":"brms.md","path":"applications/R/brms.md","url":"/applications/R/brms.html"},{"sort":1,"permalink":"/systems/","layout":"default","title":"The system","content":"<h1 id=\"the-system\">The system</h1>\n\n<p>source: <code class=\"language-plaintext highlighter-rouge\">systems/README.md</code></p>\n\n<ul>\n  <li><a href=\"/systems/system.html\">System</a></li>\n  <li><a href=\"/systems/login.html\">Login</a></li>\n  <li><a href=\"/systems/directories.html\">Directories</a></li>\n  <li><a href=\"/systems/policies.html\">Policies</a></li>\n  <li><a href=\"/systems/email.html\">email</a></li>\n  <li><a href=\"/systems/software.html\">Software</a></li>\n  <li><a href=\"/systems/training.html\">Training</a></li>\n  <li><a href=\"/systems/acknowledgement.html\">Acknowledgement</a></li>\n  <li><a href=\"/systems/contacts.html\">Contacts</a></li>\n</ul>\n","dir":"/systems/","name":"README.md","path":"systems/README.md","url":"/systems/"},{"sort":1,"layout":"default","title":"System","content":"# System\n\nCSD3 stands for the Cambridge Service for Data Driven Discovery by [Research Computing Services](https://www.csd3.cam.ac.uk/), which provies an excellent [documentation](https://docs.hpc.cam.ac.uk/hpc/) besides [other services](https://www.hpc.cam.ac.uk/).\nHere some aspects are highlighted from the perspectives of the [Cardiovascular Epidemiology Unit](https://www.phpc.cam.ac.uk/ceu/) (CEU), where all information about procedures and access requests can be found from **W:\\Administration\\CSD3 Data Users**.\n\nBasic information is available from a CSD3 console\n\n```bash\n# system bit\ngetconf LONG_BIT\n# system information\nuname -a\n# -s kernel name\n# -n node name\n# -r kernel release\n# -v kernel version\n# -p processor\n# -o operating system\n# Linux Standard Base (lsb) and distribution information\nlsb_release -a\n# CPU information\nlscpu\nwatch -n.1 \"cat /proc/cpuinfo | grep \\\"^[c]pu MHz\\\"\"\n```\n\nwhere breakdown of `uname -a` is also given and for instance `lsb_release -a` gives\n\n```\nDistributor ID: Scientific\nDescription:    Scientific Linux release 7.7 (Nitrogen)\nRelease:        7.7\nCodename:       Nitrogen\n```\n\nso the system is Scientific Linux 7 (SL7), see [http://www.scientificlinux.org/](http://www.scientificlinux.org/), also /usr/share/doc/HTML/en-US/index.html,\n","dir":"/systems/","name":"system.md","path":"systems/system.md","url":"/systems/system.html"},{"sort":1,"layout":"default","title":"Front matter","content":"# Front matter\n\nIf you have a Jekyll page that doesn't have a title specified in the YAML Front Matter, but the first non-whitespace line in the page is a Markdown H1/H2/H3, [this plugin](https://rubygems.org/gems/jekyll-titles-from-headings) instructs Jekyll to use that first heading as the page's title.\n\nIf you have a readme file, and your site doesn't otherwise have an index file, [this plugin](https://rubygems.org/gems/jekyll-readme-index) instructs Jekyll to use the readme as the site's index. That's it. No more, no less.\n\n```tip\nAll of the github pages features is supported\n```\n","dir":"/writing/","name":"front-matter.md","path":"writing/front-matter.md","url":"/writing/front-matter.html"},{"sort":1,"permalink":"/writing/test/","layout":"default","title":"Test Documentation","content":"<h1 id=\"test-documentation\">Test Documentation</h1>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>{% include list.liquid all=true %}\n</code></pre> </div></div>\n\n<ul>\n  <li><a href=\"/writing/test/markdown.html\">Markdown Elements</a></li>\n  <li><a href=\"/writing/test/toasts.html\">Toasts Card</a></li>\n  <li><a href=\"/writing/test/codes.html\">Code Blocks</a></li>\n  <li><a href=\"/writing/test/mermaid.html\">Mermaid Test</a></li>\n  <li><a href=\"/writing/test/emoji.html\">Emoji Test</a></li>\n  <li><a href=\"/writing/test/gist.html\">Gist Test</a></li>\n  <li><a href=\"/writing/test/avatar.html\">Avatar Test</a></li>\n  <li><a href=\"/writing/test/mentions.html\">Mentions Test</a></li>\n  <li><a href=\"/writing/test/fonts.html\">Fonts Test</a></li>\n  <li><a href=\"/writing/test/utilities.html\">Primer Utilities Test</a></li>\n</ul>\n","dir":"/writing/test/","name":"README.md","path":"writing/test/README.md","url":"/writing/test/"},{"sort":1,"layout":"default","title":"Markdown Elements","content":"# Markdown Elements\n\nText can be **bold**, _italic_, or ~~strikethrough~~. [Links](https://github.com) should be blue with no underlines (unless hovered over).\n\nThere should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs.\n\nThere should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs.\n\n> There should be no margin above this first sentence.\n>\n> Blockquotes should be a lighter gray with a gray border along the left side.\n>\n> There should be no margin below this final sentence.\n\n# Header 1\n\nThis is a normal paragraph following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\n\n## Header 2\n\n> This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\n\n### Header 3\n\n```\nThis is a code block following a header.\n```\n\n#### Header 4\n\n- This is an unordered list following a header.\n- This is an unordered list following a header.\n- This is an unordered list following a header.\n\n##### Header 5\n\n1. This is an ordered list following a header.\n2. This is an ordered list following a header.\n3. This is an ordered list following a header.\n\n###### Header 6\n\n| What    | Follows  |\n| ------- | -------- |\n| A table | A header |\n| A table | A header |\n| A table | A header |\n\n---\n\nThere's a horizontal rule above and below this.\n\n---\n\nHere is an unordered list:\n\n- Salt-n-Pepa\n- Bel Biv DeVoe\n- Kid 'N Play\n\nAnd an ordered list:\n\n1. Michael Jackson\n2. Michael Bolton\n3. Michael Bublé\n\nAnd an unordered task list:\n\n- [x] Create a sample markdown document\n- [x] Add task lists to it\n- [ ] Take a vacation\n\nAnd a \"mixed\" task list:\n\n- [ ] Steal underpants\n- ?\n- [ ] Profit!\n\nAnd a nested list:\n\n- Jackson 5\n  - Michael\n  - Tito\n  - Jackie\n  - Marlon\n  - Jermaine\n- TMNT\n  - Leonardo\n  - Michelangelo\n  - Donatello\n  - Raphael\n\nDefinition lists can be used with HTML syntax. Definition terms are bold and italic.\n\n<dl>\n    <dt>Name</dt>\n    <dd>Godzilla</dd>\n    <dt>Born</dt>\n    <dd>1952</dd>\n    <dt>Birthplace</dt>\n    <dd>Japan</dd>\n    <dt>Color</dt>\n    <dd>Green</dd>\n</dl>\n\n---\n\nTables should have bold headings and alternating shaded rows.\n\n| Artist          | Album          | Year |\n| --------------- | -------------- | ---- |\n| Michael Jackson | Thriller       | 1982 |\n| Prince          | Purple Rain    | 1984 |\n| Beastie Boys    | License to Ill | 1986 |\n\nIf a table is too wide, it should condense down and/or scroll horizontally.\n\n<!-- prettier-ignore-start -->\n\n| Artist            | Album           | Year | Label       | Awards   | Songs     |\n|-------------------|-----------------|------|-------------|----------|-----------|\n| Michael Jackson   | Thriller        | 1982 | Epic Records | Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R&B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical | Wanna Be Startin' Somethin', Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life |\n| Prince            | Purple Rain     | 1984 | Warner Brothers Records | Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R&B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal | Let's Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I'm a Star, Purple Rain |\n| Beastie Boys      | License to Ill  | 1986 | Mercury Records | noawardsbutthistablecelliswide | Rhymin & Stealin, The New Style, She's Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill |\n\n<!-- prettier-ignore-end -->\n\n---\n\nCode snippets like `var foo = \"bar\";` can be shown inline.\n\nAlso, `this should vertically align` ~~`with this`~~ ~~and this~~.\n\nCode can also be shown in a block element.\n\n```\nvar foo = \"bar\";\n```\n\nCode can also use syntax highlighting.\n\n```javascript\nvar foo = \"bar\";\n```\n\n```\nLong, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.\n```\n\n```javascript\nvar foo =\n  \"The same thing is true for code with syntax highlighting. A single line of code should horizontally scroll if it is really long.\";\n```\n\nInline code inside table cells should still be distinguishable.\n\n| Language   | Code               |\n| ---------- | ------------------ |\n| Javascript | `var foo = \"bar\";` |\n| Ruby       | `foo = \"bar\"`      |\n\n---\n\nSmall images should be shown at their actual size.\n\n![Octocat](https://github.githubassets.com/images/icons/emoji/octocat.png)\n\nLarge images should always scale down and fit in the content container.\n\n![Branching](https://guides.github.com/activities/hello-world/branching.png)\n\n```\nThis is the final element on the page and there should be no margin below this.\n```\n","dir":"/writing/test/","name":"markdown.md","path":"writing/test/markdown.md","url":"/writing/test/markdown.html"},{"sort":2,"layout":"default","title":"AEGIS","content":"<h1 id=\"aegis\">AEGIS</h1>\n\n<p>Web: http://aegis.stanford.edu/</p>\n\n<p>The installation follows the documentation but requires slight change,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># default Python 3.6</span>\nmodule load python\n\nvirtualenv py36\n<span class=\"nb\">source </span>py36/bin/activate\ngit clone https://github.com/junjiezhujason/aegis.git\n<span class=\"nb\">cd </span>aegis\n\n<span class=\"c\"># pip3 install -r requirements.txt</span>\n<span class=\"nb\">cat </span>requirements.txt | <span class=\"se\">\\</span>\n<span class=\"nb\">grep</span> <span class=\"nt\">-v</span> MarkupSafe | <span class=\"se\">\\</span>\nxargs pip3 <span class=\"nb\">install</span>\n\n<span class=\"c\"># setup</span>\n<span class=\"c\">#!/usr/bin/bash</span>\n<span class=\"nb\">export </span><span class=\"nv\">PROJECT_PATH</span><span class=\"o\">=</span>/home/<span class=\"nv\">$USER</span>/aegis/data\n<span class=\"nb\">cd</span> <span class=\"k\">${</span><span class=\"nv\">PROJECT_PATH</span><span class=\"k\">}</span>\n<span class=\"c\"># wget -qO- http://stanford.edu/~jjzhu/fileshare/aegis/local_20180719.tar.gz | tar -xvzf local_20180719.tar.gz</span>\n<span class=\"nb\">cd</span> -\npython3 main.py <span class=\"nt\">--lite</span> <span class=\"nt\">--port</span> 5001 <span class=\"nt\">--folder</span> <span class=\"k\">${</span><span class=\"nv\">PROJECT_PATH</span><span class=\"k\">}</span> &amp;\npython3 main.py <span class=\"nt\">--port</span> 5002 <span class=\"nt\">--folder</span> <span class=\"k\">${</span><span class=\"nv\">PROJECT_PATH</span><span class=\"k\">}</span> &amp;\nfirefox http://localhost:5002 &amp;\n</code></pre> </div></div>\n\n<p>as it appears that MarkupSafe could not be installed according to the predefined order.</p>\n","dir":"/applications/","name":"AEGIS.md","path":"applications/AEGIS.md","url":"/applications/AEGIS.html"},{"sort":2,"layout":"default","title":"R/EasyQC","content":"<h1 id=\"reasyqc\">R/EasyQC</h1>\n\n<p>Web: <a href=\"https://www.uni-regensburg.de/medizin/epidemiologie-praeventivmedizin/genetische-epidemiologie/software/index.html\">https://www.uni-regensburg.de/medizin/epidemiologie-praeventivmedizin/genetische-epidemiologie/software/index.html</a></p>\n\n<h2 id=\"setup\">Setup</h2>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Rscript <span class=\"nt\">-e</span> <span class=\"s2\">\"install.packages('https://homepages.uni-regensburg.de/~wit59712/easyqc/EasyQC_23.8.tar.gz', repos = NULL, type = 'source')\"</span>\n</code></pre> </div></div>\n\n<h2 id=\"documentation-example\">Documentation example</h2>\n\n<p>With installation at ${HPC_WORK} (/rds/user/$USER/hpc-work/R/EasyQC) we could modify the example script</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">sed</span> <span class=\"nt\">-i</span> <span class=\"s2\">\"s|EASY_INSTALL_DIR|/rds/user/</span><span class=\"nv\">$USER</span><span class=\"s2\">/hpc-work/R/EasyQC/extdata/|\"</span> /rds/user/<span class=\"nv\">$USER</span>/hpc-work/R/EasyQC/extdata/example_qc.ecf\nRscript <span class=\"nt\">-e</span> <span class=\"s2\">\"library(EasyQC);installDir=system.file('extdata', package='EasyQC');ecfFileQc=file.path(installDir,'example_qc.ecf');EasyQC(ecfFileQc)\"</span>\n</code></pre> </div></div>\n\n<p>and we have these output available from the current directory:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>CLEANED.studyX_file1.txt\nCLEANED.studyX_file1.txt.10rows\nCLEANED.studyX_file2.txt\nCLEANED.studyX_file2.txt.10rows\nexample_qc.rep\n</code></pre> </div></div>\n\n<p>One can carry on with downloading reference data from the website above as well.</p>\n","dir":"/applications/R/","name":"EasyQC.md","path":"applications/R/EasyQC.md","url":"/applications/R/EasyQC.html"},{"sort":2,"permalink":"/cardio/","layout":"default","title":"cardio","content":"<h1 id=\"cardio\">cardio</h1>\n\n<p>source: <code class=\"language-plaintext highlighter-rouge\">cardio/README.md</code></p>\n\n<p>Cardio is the HPC facility at the CEU.</p>\n\n<h2 id=\"csd3-partition\">CSD3 partition</h2>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>scontrol show part\n<span class=\"c\"># cpu-o-1 for interactive jobs with no limit on cpus per user expecting fair usage</span>\n<span class=\"c\"># cpu-o-[2-5] for long jobs with maximum cpus per user set to 40 and a maximum wall time of 7 days</span>\n<span class=\"c\"># cpu-o-7 for short jobs with a maximum wall time of 24 hours</span>\n\n<span class=\"c\"># Interactive jobs</span>\n\n<span class=\"c\">#SBATCH -A CARDIO-SL0-CPU</span>\n<span class=\"c\">#SBATCH -p cardio_intr</span>\n<span class=\"c\">#SBATCH --qos=cardio_intr</span>\n\n<span class=\"c\"># Long jobs</span>\n\n<span class=\"c\">#SBATCH -A CARDIO-SL0-CPU</span>\n<span class=\"c\">#SBATCH -p cardio</span>\n<span class=\"c\">#SBATCH --qos=cardio</span>\n\n<span class=\"c\"># Short jobs</span>\n\n<span class=\"c\">#SBATCH -A CARDIO-SL0-CPU</span>\n<span class=\"c\">#SBATCH -p cardio_short</span>\n<span class=\"c\">#SBATCH --qos=cardio_short</span>\n</code></pre> </div></div>\n\n<hr />\n\n<h2 id=\"legacy-materials\">Legacy materials</h2>\n\n<h3 id=\"login\">login</h3>\n\n<p>Automatic login can be enabled with <code class=\"language-plaintext highlighter-rouge\">ssh-copy-id cardio-login.hpc.cam.ac.uk</code>.</p>\n\n<h3 id=\"migration\">Migration</h3>\n\n<p>It is possible to fetch <code class=\"language-plaintext highlighter-rouge\">my-file-on-Cardio</code> with</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>scp cardio-login.hpc.cam.ac.uk:/home/<span class=\"nv\">$USER</span>/my-file-on-Cardio <span class=\"nb\">.</span>\n</code></pre> </div></div>\n\n<p>as with <code class=\"language-plaintext highlighter-rouge\">sftp</code>, noting the <code class=\"language-plaintext highlighter-rouge\">-r</code> option for both could be very useful. More generally, it is preferable to use <code class=\"language-plaintext highlighter-rouge\">rsync</code>, <a href=\"https://rsync.samba.org/\">https://rsync.samba.org/</a>.</p>\n\n<p>Old version on Cardio, rsync 3.0.6, gives errors,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>rsync <span class=\"nt\">-av</span> <span class=\"nt\">--partial</span> mydir/ login-cpu.hpc.cam.ac.uk:/rds/user/<span class=\"nv\">$USER</span>/hpc-work/mydir\n\nErrors after successfully sending a number of files :\nrsync: writefd_unbuffered failed to write 4 bytes to socket <span class=\"o\">[</span>sender]: Broken pipe <span class=\"o\">(</span>32<span class=\"o\">)</span>\nrsync: connection unexpectedly closed <span class=\"o\">(</span>604 bytes received so far<span class=\"o\">)</span> <span class=\"o\">[</span>sender]\nrsync error: error <span class=\"k\">in </span>rsync protocol data stream <span class=\"o\">(</span>code 12<span class=\"o\">)</span> at io.c<span class=\"o\">(</span>600<span class=\"o\">)</span> <span class=\"o\">[</span><span class=\"nv\">sender</span><span class=\"o\">=</span>3.0.6]\n</code></pre> </div></div>\n\n<p>Bram Prins (<a href=\"mailto:bp406@medschl.cam.ac.uk\">bp406@medschl.cam.ac.uk</a>) has the latest version (3.1.3) that doesn’t give this error here:</p>\n\n<p>/DO-NOT-MODIFY-SCRATCH/bp406/apps/software/data_manipulation/rsync-3.1.3/rsync</p>\n\n<p>Note <strong>/scratch</strong> at Cardio is now <strong>/DO-NOT-MODIFY-SCRATCH</strong> – an example is <a href=\"/cardio/jp.sh\">jp.sh</a>.</p>\n\n<h3 id=\"r-package-reinstallations\">R package reinstallations</h3>\n\n<p>Package redeployment is illustrated with R below for building R package list from <strong><em>/home/$USER/R</em></strong> at Cardio to be resintalled to <strong><em>/rds/user/$USER/hpc-work/R</em></strong> at CSD3.</p>\n\n<p>We can use screen copy of package list from Cardio since users do not have write permission.</p>\n\n<p><strong>On Cardio</strong></p>\n\n<div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"w\">  </span><span class=\"n\">home</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">Sys.getenv</span><span class=\"p\">(</span><span class=\"s2\">\"HOME\"</span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"n\">from</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">paste0</span><span class=\"p\">(</span><span class=\"n\">home</span><span class=\"p\">,</span><span class=\"s2\">\"/R\"</span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"n\">pkgs</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">unname</span><span class=\"p\">(</span><span class=\"n\">installed.packages</span><span class=\"p\">(</span><span class=\"n\">lib.loc</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">from</span><span class=\"p\">)[,</span><span class=\"w\"> </span><span class=\"s2\">\"Package\"</span><span class=\"p\">])</span><span class=\"w\">\n  </span><span class=\"n\">edit</span><span class=\"p\">(</span><span class=\"n\">pkgs</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre> </div></div>\n\n<p><strong>On CSD3</strong></p>\n\n<div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"w\">  </span><span class=\"n\">pkgs</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"s2\">\"mark and copy the list as given in c() above with Shift+Ins\"</span><span class=\"w\">\n  </span><span class=\"n\">user</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">Sys.getenv</span><span class=\"p\">(</span><span class=\"s2\">\"USER\"</span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">paste0</span><span class=\"p\">(</span><span class=\"s2\">\"/rds/user/\"</span><span class=\"p\">,</span><span class=\"n\">user</span><span class=\"p\">,</span><span class=\"s2\">\"/hpc-work/R\"</span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"n\">install.packages</span><span class=\"p\">(</span><span class=\"n\">pkgs</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">lib</span><span class=\"o\">=</span><span class=\"n\">to</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">repos</span><span class=\"o\">=</span><span class=\"s2\">\"https://cran.r-project.org\"</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre> </div></div>\n\n<p>If a package list is already built one can reinstall them as follows.</p>\n\n<div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">user</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">Sys.getenv</span><span class=\"p\">(</span><span class=\"s2\">\"USER\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">location</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">paste0</span><span class=\"p\">(</span><span class=\"s2\">\"/rds/user/\"</span><span class=\"p\">,</span><span class=\"n\">user</span><span class=\"p\">,</span><span class=\"s2\">\"/hpc-work/R\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">pkgs</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">unname</span><span class=\"p\">(</span><span class=\"n\">installed.packages</span><span class=\"p\">(</span><span class=\"n\">lib.loc</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">location</span><span class=\"p\">)[,</span><span class=\"w\"> </span><span class=\"s2\">\"Package\"</span><span class=\"p\">])</span><span class=\"w\">\n</span><span class=\"n\">install.packages</span><span class=\"p\">(</span><span class=\"n\">pkgs</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">lib</span><span class=\"o\">=</span><span class=\"n\">location</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">repos</span><span class=\"o\">=</span><span class=\"s2\">\"https://cran.r-project.org\"</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre> </div></div>\n\n<p>A version by Scott Ritchie (<a href=\"mailto:sr827@medschl.cam.ac.uk\">sr827@medschl.cam.ac.uk</a>), <a href=\"/cardio/reinstall_r_pkgs.R\">reinstall_r_pkgs.R</a>, also touches upon Bioconductor,\nwhose package installations and updates are described at <a href=\"https://bioconductor.org/install/\">https://bioconductor.org/install/</a>.</p>\n","dir":"/cardio/","name":"README.md","path":"cardio/README.md","url":"/cardio/"},{"sort":2,"layout":"default","title":"Login","content":"# Login\n\nThe CSD3 login address is `login.hpc.cam.ac.uk` with a mapping table\n\n| Collective name             | Node name      | Comments    |\n| --------------------------- | -------------- | ----------- |\n| login-gpu.hpc.cam.ac.uk     | login-e-[1-8]  | GPU         |\n| login-cpu.hpc.cam.ac.uk     | login-e-[9-16] | CPU         |\n| login-icelake.hpc.cam.ac.uk | login-q-[1-4]  | CentOS8[^1] |\n\nOn a GPU, we have from `module list`\n\n```bash\nCurrently Loaded Modulefiles:\n  1) dot                                5) singularity/current                9) openmpi-1.10.7-gcc-5.4.0-jdc7f4f\n  2) slurm                              6) rhel7/global                      10) cmake/latest\n  3) turbovnc/2.0.1                     7) cuda/8.0                          11) rhel7/default-gpu\n  4) vgl/2.5.1/64                       8) gcc-5.4.0-gcc-4.8.5-fis24gg\n```\n\nand can switch to a CPU environment with\n\n```bash\nmodule purge\nmodule load rhel7/default-peta4\n```\n\nOur module list is now\n\n```\n  1) dot                            5) singularity/current            9) intel/impi/2017.4/intel       13) intel/libs/daal/2017.4\n  2) slurm                          6) rhel7/global                  10) intel/libs/idb/2017.4         14) intel/bundles/complib/2017.4\n  3) turbovnc/2.0.1                 7) intel/compilers/2017.4        11) intel/libs/tbb/2017.4         15) cmake/latest\n  4) vgl/2.5.1/64                   8) intel/mkl/2017.4              12) intel/libs/ipp/2017.4         16) rhel7/default-peta4\n```\n\nFinally, with icelake we have\n\n```\nCurrently Loaded Modulefiles:\n 1) dot                   4) rhel8/global             7) intel/mkl/2020.2         10) intel/libs/tbb/2020.2   13) intel/bundles/complib/2020.2\n 2) rhel8/slurm           5) cuda/11.4                8) intel/impi/2020.2/intel  11) intel/libs/ipp/2020.2   14) rhel8/default-icl\n 3) singularity/current   6) intel/compilers/2020.2   9) intel/libs/idb/2020.2    12) intel/libs/daal/2020.2\n```\n\nTo reset Raven password, follow [https://password.csx.cam.ac.uk/](https://password.csx.cam.ac.uk/).\n\nTo establish host keys one resorts to `ssh-keygen`; the easiest is to accept the default.\n\nThe CSD3 hostkeys are described here, [https://docs.hpc.cam.ac.uk/hpc/user-guide/hostkeys.html](https://docs.hpc.cam.ac.uk/hpc/user-guide/hostkeys.html). From 1st February 2020, the following script needs to be run\nfrom a local machine,\n\n```bash\ncp ~/.ssh/known_hosts ~/.ssh/known_hosts.`date +%F`\nsed -i -e '/128\\.232\\.224/d' -e '/.*\\.hpc\\.cam\\.ac\\.uk/d' ~/.ssh/known_hosts\n# to remove the entry, try:\n# ssh-keygen -f \"/home/$USER/.ssh/known_hosts\" -R \"login.hpc.cam.ac.uk\"\n```\n\nAutomatic login[^2] via ssh/sftp can be enabled with\n\n```bash\nssh-copy-id login.hpc.cam.ac.uk\n```\n\nfrom a Bash console after one login. If this is not set up, follow these steps,\n\n```bash\nssh-keygen\n# Enter file in which to save the key (/home/$USER/.ssh/id_rsa): mykey\n# Enter passphrase (empty for no passphrase):\n# Enter same passphrase again:\n# Your identification has been saved in mykey.\n# Your public key has been saved in mykey.pub.\nThe key fingerprint is:\nssh-copy-id -i ~/.ssh/mykey.pub login.hpc.cam.ac.uk\n```\n\nas in [https://www.ssh.com/ssh/copy-id](https://www.ssh.com/ssh/copy-id). At a specific host, try\n\n```bash\nmkdir .ssh\nchmod 700 .ssh\nssh-keygen\ncd .ssh\nchmod 700 mykey*\ntouch authorized_keys\nchmod 700 authorized_keys\nssh-copy-id -i mykey.pub user@remoteserver\n```\n\nEnvironmental variables can be set inside `~/.bashrc`. In particular when some changes have been made, one can enable them with\n\n```bash\nsource ~/.bashrc\n```\n\nwhere the `~` sign is equivalent to ${HOME}.\n\nWhen there is an error `'abrt-cli status' timed out`, one should remove ${HOME}/.cache and relogin/source .bashrc.\n\nIt is useful to note that it is preferable to put an alias\n\n```\nalias ssh='ssh -q -X $@'\n```\n\ninto `${HOME}/.bashrc` in case a remote login is necessary (e.g., faster login to CSD3 or there is poor local network connection).\n\n[^1]: Applications such as R/nloptr package require to be recompiled. In this case, we run `download.packages(\"nloptr\",\".\")` inside `R` on an Internet-enabled node and compile the package with `R CMD INSTALL nloptr_1.2.2.3.tar.gz`, say.\n[^2]: This appears subject to the system setup.\n","dir":"/systems/","name":"login.md","path":"systems/login.md","url":"/systems/login.html"},{"sort":2,"layout":"default","title":"List directory","content":"# List directory\n\nIf you want to list all `pages` in the current directory, add following code to your markdown file!\n\n    {% raw %}\n    {% include list.liquid %}\n    {% endraw %}\n\nThen Generate a list like this:\n{% include list.liquid %}\n\nIf you want to list all `pages and sub directory` in the current directory, add following code to your markdown file!\n\n    {% raw %}\n    {% include list.liquid all=true %}\n    {% endraw %}\n\nThen Generate a list like this [table of contents]({{ site.baseurl }}/docs/)!\n","dir":"/writing/","name":"list.md","path":"writing/list.md","url":"/writing/list.html"},{"sort":2,"layout":"default","title":"Toasts Card","content":"# Toasts Card\n\nTHIS IS TOO LONG, NEED UPDATE! HERE IS SOME IDEAS:\n\n- https://primer.style/css/components/box\n- https://primer.style/css/components/toasts\n\n```note\n## This is a note\n\nMarkdown is supported, Text can be **bold**, _italic_, or ~~strikethrough~~. [Links](https://github.com) should be blue with no underlines\n\n`inline code`\n\n[`inline code inside link`](./)\n```\n\n```note\nThis is note2\n```\n\n```note\nThis is note3\n```\n\n```tip\nIt’s bigger than a bread box.\n```\n\n```tip\nIt’s tip 2\n```\n\n```warning\nStrong prose may provoke extreme mental exertion. Reader discretion is strongly advised.\n```\n\n```danger\nMad scientist at work!\n```\n","dir":"/writing/test/","name":"toasts.md","path":"writing/test/toasts.md","url":"/writing/test/toasts.html"},{"sort":3,"permalink":"/applications/R/","layout":"default","title":"R packages","content":"<h1 id=\"r-packages\">R packages</h1>\n\n<p>source: <code class=\"language-plaintext highlighter-rouge\">applications/R/README.md</code></p>\n\n<p>Here is a list of R packages.</p>\n\n<ul>\n  <li><a href=\"/applications/R/brms.html\">R/brms</a></li>\n  <li><a href=\"/applications/R/EasyQC.html\">R/EasyQC</a></li>\n  <li><a href=\"/applications/R/gnn.html\">R/gnn</a></li>\n  <li><a href=\"/applications/R/gsl.html\">R/gsl</a></li>\n  <li><a href=\"/applications/R/LDlinkR.html\">R/LDlinkR</a></li>\n  <li><a href=\"/applications/R/magick.html\">R/magick</a></li>\n  <li><a href=\"/applications/R/PhenoScanner.html\">R/PhenoScanner</a></li>\n  <li><a href=\"/applications/R/PKI.html\">R/PKI</a></li>\n  <li><a href=\"/applications/R/plumber.html\">R/plumber</a></li>\n  <li><a href=\"/applications/R/protolite.html\">R/protolite</a></li>\n  <li><a href=\"/applications/R/Rfast.html\">R/Rfast</a></li>\n  <li><a href=\"/applications/R/rgdal.html\">R/rgdal</a></li>\n  <li><a href=\"/applications/R/rgeos.html\">R/rgeos</a></li>\n  <li><a href=\"/applications/R/Rhdf5lib.html\">R/Rhdf5lib</a></li>\n  <li><a href=\"/applications/R/rjags.html\">R/rjags</a></li>\n  <li><a href=\"/applications/R/rJava.html\">R/rJava</a></li>\n  <li><a href=\"/applications/R/rstan.html\">R/rstan</a></li>\n  <li><a href=\"/applications/R/SAIGE.html\">R/SAIGE 0.36.6 and 0.39.2</a></li>\n  <li><a href=\"/applications/R/sf.html\">R/sf</a></li>\n  <li><a href=\"/applications/R/snpnet.html\">R/snpnet</a></li>\n  <li><a href=\"/applications/R/sojo.html\">R/sojo</a></li>\n  <li><a href=\"/applications/R/xlsx.html\">R/xlsx</a></li>\n</ul>\n","dir":"/applications/R/","name":"README.md","path":"applications/R/README.md","url":"/applications/R/"},{"sort":3,"layout":"default","title":"R/gnn","content":"# R/gnn\n\nIt requires libgsl. The available version is seen from\n\n```bash\ngsl-config --version\n```\n\nBy default, this gives 1.15. A higher version is furnished with an appropriate module, e.g.,\n\n```bash\nmodule avail gsl\nmodule load gsl/2.4\n```\n","dir":"/applications/R/","name":"gnn.md","path":"applications/R/gnn.md","url":"/applications/R/gnn.html"},{"sort":3,"permalink":"/applications/","layout":"default","title":"Applications","content":"<h1 id=\"applications\">Applications</h1>\n\n<p>source: <code class=\"language-plaintext highlighter-rouge\">applications/README.md</code></p>\n\n<p>Whenever appropriate, it is assumed that the destination of software installation is ${HPC_WORK}, e.g.,\nvia <code class=\"language-plaintext highlighter-rouge\">export HPC_WORK=/rds/user/$USER/hpc-work</code> in .bashrc.</p>\n\n<ul>\n  <li><a href=\"/applications/ABCtoolbox.html\">ABCtoolbox</a></li>\n  <li><a href=\"/applications/AEGIS.html\">AEGIS</a></li>\n  <li><a href=\"/applications/akt.html\">akt</a></li>\n  <li><a href=\"/applications/ANNOVAR.html\">ANNOVAR</a></li>\n  <li><a href=\"/applications/aria2.html\">aria2</a></li>\n  <li><a href=\"/applications/bcftools.html\">bcftools</a></li>\n  <li><a href=\"/applications/bedops.html\">bedops</a></li>\n  <li><a href=\"/applications/bgenix.html\">bgenix</a></li>\n  <li><a href=\"/applications/CaVEMaN.html\">CaVEMaN</a></li>\n  <li><a href=\"/applications/citeproc.html\">citeproc</a></li>\n  <li><a href=\"/applications/Cytoscape.html\">Cytoscape</a></li>\n  <li><a href=\"/applications/DosageConverter.html\">DosageConverter</a></li>\n  <li><a href=\"/applications/gcta.html\">GCTA</a></li>\n  <li><a href=\"/applications/gsutil.html\">gsutil</a></li>\n  <li><a href=\"/applications/gwas2vcf.html\">gwas2vcf</a></li>\n  <li><a href=\"/applications/hail.html\">hail</a></li>\n  <li><a href=\"/applications/HESS.html\">HESS</a></li>\n  <li><a href=\"/applications/KentUtils.html\">KentUtils</a></li>\n  <li><a href=\"/applications/lapack.html\">lapack</a></li>\n  <li><a href=\"/applications/ldsc.html\">ldsc</a></li>\n  <li><a href=\"/applications/LEMMA.html\">LEMMA</a></li>\n  <li><a href=\"/applications/MetaXcan.html\">MetaXcan</a></li>\n  <li><a href=\"/applications/pandoc.html\">pandoc</a></li>\n  <li><a href=\"/applications/pca_projection.html\">PCA projection</a></li>\n  <li><a href=\"/applications/plink.html\">PLINK</a></li>\n  <li><a href=\"/applications/polyphen-2.html\">polyphen-2</a></li>\n  <li><a href=\"/applications/poppler.html\">poppler</a></li>\n  <li><a href=\"/applications/PRSice.html\">PRSice</a></li>\n  <li><a href=\"/applications/PRSoS.html\">PRSoS</a></li>\n  <li><a href=\"/applications/pspp.html\">pspp</a></li>\n  <li><a href=\"/applications/qpdf.html\">qpdf</a></li>\n  <li><a href=\"/applications/regenie.html\">regenie</a></li>\n  <li><a href=\"/applications/R.html\">R</a></li>\n  <li><a href=\"/applications/RStudio.html\">RStudio</a></li>\n  <li><a href=\"/applications/shapeit3.html\">shapeit3</a></li>\n  <li><a href=\"/applications/SMR.html\">SMR</a></li>\n  <li><a href=\"/applications/snakemake.html\">snakemake</a></li>\n  <li><a href=\"/applications/SNPTEST.html\">SNPTEST</a></li>\n  <li><a href=\"/applications/tabix.html\">tabix</a></li>\n  <li><a href=\"/applications/VEP.html\">VEP</a></li>\n  <li><a href=\"/applications/VScode.html\">Visual Studio code</a></li>\n  <li><a href=\"/applications/R/\">R packages</a>\n    <ul>\n      <li><a href=\"/applications/R/brms.html\">R/brms</a></li>\n      <li><a href=\"/applications/R/EasyQC.html\">R/EasyQC</a></li>\n      <li><a href=\"/applications/R/gnn.html\">R/gnn</a></li>\n      <li><a href=\"/applications/R/gsl.html\">R/gsl</a></li>\n      <li><a href=\"/applications/R/LDlinkR.html\">R/LDlinkR</a></li>\n      <li><a href=\"/applications/R/magick.html\">R/magick</a></li>\n      <li><a href=\"/applications/R/PhenoScanner.html\">R/PhenoScanner</a></li>\n      <li><a href=\"/applications/R/PKI.html\">R/PKI</a></li>\n      <li><a href=\"/applications/R/plumber.html\">R/plumber</a></li>\n      <li><a href=\"/applications/R/protolite.html\">R/protolite</a></li>\n      <li><a href=\"/applications/R/Rfast.html\">R/Rfast</a></li>\n      <li><a href=\"/applications/R/rgdal.html\">R/rgdal</a></li>\n      <li><a href=\"/applications/R/rgeos.html\">R/rgeos</a></li>\n      <li><a href=\"/applications/R/Rhdf5lib.html\">R/Rhdf5lib</a></li>\n      <li><a href=\"/applications/R/rjags.html\">R/rjags</a></li>\n      <li><a href=\"/applications/R/rJava.html\">R/rJava</a></li>\n      <li><a href=\"/applications/R/rstan.html\">R/rstan</a></li>\n      <li><a href=\"/applications/R/SAIGE.html\">R/SAIGE 0.36.6 and 0.39.2</a></li>\n      <li><a href=\"/applications/R/sf.html\">R/sf</a></li>\n      <li><a href=\"/applications/R/snpnet.html\">R/snpnet</a></li>\n      <li><a href=\"/applications/R/sojo.html\">R/sojo</a></li>\n      <li><a href=\"/applications/R/xlsx.html\">R/xlsx</a></li>\n    </ul>\n  </li>\n</ul>\n","dir":"/applications/","name":"README.md","path":"applications/README.md","url":"/applications/"},{"sort":3,"layout":"default","title":"akt","content":"<h1 id=\"akt\">akt</h1>\n\n<p>Web: https://github.com/Illumina/akt</p>\n\n<p>This is Illumina’s ancestry and kinship toolkit and the installation is canonical.</p>\n","dir":"/applications/","name":"akt.md","path":"applications/akt.md","url":"/applications/akt.html"},{"sort":3,"layout":"default","title":"Directories","content":"# Directories\n\nThis [section](https://docs.hpc.cam.ac.uk/hpc/user-guide/io_management.html#summary-of-available-filesystems) gives a summary of the file system.\n\nGo to **CSD3 portal**: [https://selfservice.uis.cam.ac.uk/account/](https://selfservice.uis.cam.ac.uk/account/) and accept the terms and conditions. An `rds/` directory should then be created with symbolic links as follows,\n\n```\nhpc-work -> /rds/user/$USER/hpc-work/\ngenetics_resources -> /rds/project/jmmh2/rds-jmmh2-genetics_resources/\nlegacy_projects -> /rds/project/jmmh2/rds-jmmh2-legacy_projects/\npre_qc_data -> /rds/project/jmmh2/rds-jmmh2-pre_qc_data/\nprojects -> /rds/project/jmmh2/rds-jmmh2-projects/\npublic_databases -> /rds/project/jmmh2/rds-jmmh2-public_databases/\nresults -> /rds/project/jmmh2/rds-jmmh2-results\n```\n\nShort shorter (without rds-jmmh2- prefix) names as on Cardio can be created equivalently with\n\n```bash\nif [ ! -d /home/$USER/rds ]; then mkdir /home/$USER/rds; fi\nln -sf /rds/user/$USER/hpc-work /home/$USER/rds/hpc-work\nexport rt=/rds/project/jmmh2\nfor d in $(ls $rt | xargs -l basename | sed 's/rds-jmmh2-//g'); do ln -sf $rt/rds-jmmh2-$d /home/$USER/rds/$d; done\n```\n\nNote to list the directories you need postfix them with '/'.\n","dir":"/systems/","name":"directories.md","path":"systems/directories.md","url":"/systems/directories.html"},{"sort":3,"layout":"default","title":"Mathjax","content":"# Mathjax\n\n$$\n\\begin{aligned}\n  & \\phi(x,y) = \\phi \\left(\\sum_{i=1}^n x_ie_i, \\sum_{j=1}^n y_je_j \\right)\n  = \\sum_{i=1}^n \\sum_{j=1}^n x_i y_j \\phi(e_i, e_j) = \\\\\n  & (x_1, \\ldots, x_n) \\left( \\begin{array}{ccc}\n      \\phi(e_1, e_1) & \\cdots & \\phi(e_1, e_n) \\\\\n      \\vdots & \\ddots & \\vdots \\\\\n      \\phi(e_n, e_1) & \\cdots & \\phi(e_n, e_n)\n    \\end{array} \\right)\n  \\left( \\begin{array}{c}\n      y_1 \\\\\n      \\vdots \\\\\n      y_n\n    \\end{array} \\right)\n\\end{aligned}\n$$\n\n```note\nFor documentation, see: https://kramdown.gettalong.org/syntax.html#math-blocks\n```\n","dir":"/writing/","name":"mathjax.md","path":"writing/mathjax.md","url":"/writing/mathjax.html"},{"sort":3,"layout":"default","title":"Code Blocks","content":"<h1 id=\"code-blocks\">Code Blocks</h1>\n\n<p><code class=\"language-plaintext highlighter-rouge\">inline code</code></p>\n\n<p><a href=\"./\"><code class=\"language-plaintext highlighter-rouge\">inline code inside link</code></a></p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>:root {\n  @for $level from 1 through 12 {\n    @if $level % 4 == 0 {\n      --toc-#{$level}: #{darken($theme-white, 4 * 8.8%)};\n    } @else {\n      --toc-#{$level}: #{darken($theme-white, $level % 4 * 8.8%)};\n    }\n  }\n}\n</code></pre> </div></div>\n\n<p><strong>Highlight:</strong></p>\n\n<div class=\"language-scss highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">:root</span> <span class=\"p\">{</span>\n  <span class=\"k\">@for</span> <span class=\"nv\">$level</span> <span class=\"ow\">from</span> <span class=\"m\">1</span> <span class=\"ow\">through</span> <span class=\"m\">12</span> <span class=\"p\">{</span>\n    <span class=\"k\">@if</span> <span class=\"nv\">$level</span> <span class=\"o\">%</span> <span class=\"m\">4</span> <span class=\"o\">==</span> <span class=\"m\">0</span> <span class=\"p\">{</span>\n      <span class=\"nt\">--toc-</span><span class=\"si\">#{</span><span class=\"nv\">$level</span><span class=\"si\">}</span><span class=\"nd\">:</span> <span class=\"si\">#{</span><span class=\"nf\">darken</span><span class=\"p\">(</span><span class=\"nv\">$theme-white</span><span class=\"o\">,</span> <span class=\"m\">4</span> <span class=\"o\">*</span> <span class=\"m\">8</span><span class=\"mi\">.8%</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"p\">;</span>\n    <span class=\"p\">}</span> <span class=\"k\">@else</span> <span class=\"p\">{</span>\n      <span class=\"nt\">--toc-</span><span class=\"si\">#{</span><span class=\"nv\">$level</span><span class=\"si\">}</span><span class=\"nd\">:</span> <span class=\"si\">#{</span><span class=\"nf\">darken</span><span class=\"p\">(</span><span class=\"nv\">$theme-white</span><span class=\"o\">,</span> <span class=\"nv\">$level</span> <span class=\"o\">%</span> <span class=\"m\">4</span> <span class=\"o\">*</span> <span class=\"m\">8</span><span class=\"mi\">.8%</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"p\">;</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre> </div></div>\n","dir":"/writing/test/","name":"codes.md","path":"writing/test/codes.md","url":"/writing/test/codes.html"},{"sort":4,"layout":"default","title":"ANNOVAR","content":"<h1 id=\"annovar\">ANNOVAR</h1>\n\n<p>Web page: http://annovar.openbioinformatics.org/en/latest/</p>\n\n<p>Registered from http://download.openbioinformatics.org/annovar_download_form.php with the following information,</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>User Name:\nUser Email:\nUser Institution: University of Cambridge\n</code></pre> </div></div>\n\n<p>then copy the link received from email and issue commands from csd3,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">cd</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>\nwget http://www.openbioinformatics.org/annovar/download/.../annovar.latest.tar.gz\n<span class=\"nb\">tar </span>xvfz annovar.latest.tar.gz\n<span class=\"nb\">ls</span> <span class=\"k\">*</span>pl | <span class=\"nb\">sed</span> <span class=\"s1\">'s/*//g'</span> | parallel <span class=\"nt\">-C</span><span class=\"s1\">' '</span> <span class=\"s1\">'ln -sf ${HPC_WORK}/annovar/{} ${HPC_WORK}/bin/{}'</span>\n</code></pre> </div></div>\n\n<p>Additionally, one can download the ENSEMBL genes and whole-genome FASTA files to\nhumandb/hg19_seq for CCDS/GENCODE annotation.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">cd </span>annovar\n<span class=\"c\"># ENSEMBL genes</span>\nannotate_variation.pl <span class=\"nt\">-buildver</span> hg19 <span class=\"nt\">-downdb</span> <span class=\"nt\">-webfrom</span> annovar ensGene\nannotate_variation.pl <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-out</span> ex1 <span class=\"nt\">-dbtype</span> ensGene example/ex1.avinput humandb/\n<span class=\"c\"># reference genome in FASTA</span>\nannotate_variation.pl <span class=\"nt\">-downdb</span> <span class=\"nt\">-build</span> hg19 <span class=\"nb\">seq </span>humandb/hg19_seq/\n<span class=\"c\"># CCDS genes</span>\nannotate_variation.pl <span class=\"nt\">-downdb</span> <span class=\"nt\">-build</span> hg19 ccdsGene humandb\nretrieve_seq_from_fasta.pl humandb/hg19_ccdsGene.txt <span class=\"nt\">-seqdir</span> humandb/hg19_seq <span class=\"nt\">-format</span> refGene <span class=\"se\">\\</span>\n                           <span class=\"nt\">-outfile</span> humandb/hg19_ccdsGeneMrna.fa\nannotate_variation.pl <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-out</span> ex1 <span class=\"nt\">-dbtype</span> ccdsGene example/ex1.avinput humandb/\n<span class=\"c\"># GENCODE genes</span>\nannotate_variation.pl <span class=\"nt\">-downdb</span> wgEncodeGencodeBasicV19 humandb/ <span class=\"nt\">-build</span> hg19\nretrieve_seq_from_fasta.pl <span class=\"nt\">-format</span> genericGene <span class=\"nt\">-seqdir</span> humandb/hg19_seq/ <span class=\"se\">\\</span>\n                           <span class=\"nt\">-outfile</span> humandb/hg19_wgEncodeGencodeBasicV19Mrna.fa humandb/hg19_wgEncodeGencodeBasicV19.txt\nannotate_variation.pl <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-out</span> ex1 <span class=\"nt\">-dbtype</span> wgEncodeGencodeBasicV19 example/ex1.avinput humandb/\n</code></pre> </div></div>\n\n<p>We can have region-based annotation as in http://annovar.openbioinformatics.org/en/latest/user-guide/region/,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># Conserved genomic elements</span>\nannotate_variation.pl <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-downdb</span> phastConsElements46way humandb/\nannotate_variation.pl <span class=\"nt\">-regionanno</span> <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-out</span> ex1 <span class=\"nt\">-dbtype</span> phastConsElements46way example/ex1.avinput humandb/\nannotate_variation.pl <span class=\"nt\">-regionanno</span> <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-out</span> ex1 <span class=\"nt\">-dbtype</span> phastConsElements46way example/ex1.avinput humandb/ <span class=\"nt\">-normscore_threshold</span> 400\n<span class=\"c\"># Transcription factor binding sites</span>\nannotate_variation.pl <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-downdb</span> tfbsConsSites humandb/\nannotate_variation.pl <span class=\"nt\">-regionanno</span> <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-out</span> ex1 <span class=\"nt\">-dbtype</span> tfbsConsSites example/ex1.avinput humandb/\n<span class=\"c\"># Cytogenetic band</span>\nannotate_variation.pl <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-downdb</span> cytoBand humandb/\nannotate_variation.pl <span class=\"nt\">-regionanno</span> <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-out</span> ex1 <span class=\"nt\">-dbtype</span> cytoBand example/ex1.avinput humandb/\n<span class=\"c\"># microRNA, snoRNA</span>\nannotate_variation.pl <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-downdb</span> wgRna humandb/\nannotate_variation.pl <span class=\"nt\">-regionanno</span> <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-out</span> ex1 <span class=\"nt\">-dbtype</span> wgRna example/ex1.avinput humandb/\n<span class=\"c\"># previously reported structural variants</span>\nannotate_variation.pl <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-downdb</span> dgvMerged humandb/\nannotate_variation.pl <span class=\"nt\">-regionanno</span> <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-out</span> ex1 <span class=\"nt\">-dbtype</span> dgvMerged example/ex1.avinput humandb/\n<span class=\"c\"># published GWAS</span>\nannotate_variation.pl <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-downdb</span> gwasCatalog humandb/\nannotate_variation.pl <span class=\"nt\">-regionanno</span> <span class=\"nt\">-build</span> hg19 <span class=\"nt\">-out</span> ex1 <span class=\"nt\">-dbtype</span> gwasCatalog example/ex1.avinput humandb/\n</code></pre> </div></div>\n\n<p>Here is a more sophisticated example contrasting <code class=\"language-plaintext highlighter-rouge\">table_annovar.pl</code> with <code class=\"language-plaintext highlighter-rouge\">annotate_variation.pl</code>,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">if</span> <span class=\"o\">[</span> <span class=\"o\">!</span> <span class=\"nt\">-d</span> <span class=\"nb\">test</span> <span class=\"o\">]</span><span class=\"p\">;</span> <span class=\"k\">then </span><span class=\"nb\">mkdir test</span><span class=\"p\">;</span> <span class=\"k\">fi\n</span>annotate_variation.pl <span class=\"nt\">-buildver</span> hg19 <span class=\"nt\">-downdb</span> <span class=\"nt\">-webfrom</span> annovar refGene <span class=\"nb\">test</span>/\nannotate_variation.pl <span class=\"nt\">-buildver</span> hg19 <span class=\"nt\">-downdb</span> <span class=\"nt\">-webfrom</span> annovar exac03 <span class=\"nb\">test</span>/\nannotate_variation.pl <span class=\"nt\">-buildver</span> hg19 <span class=\"nt\">-downdb</span> <span class=\"nt\">-webfrom</span> annovar avsnp147 <span class=\"nb\">test</span>/\nannotate_variation.pl <span class=\"nt\">-buildver</span> hg19 <span class=\"nt\">-downdb</span> <span class=\"nt\">-webfrom</span> annovar dbnsfp30a <span class=\"nb\">test</span>/\n\nannotate_variation.pl <span class=\"nt\">-geneanno</span> <span class=\"nt\">-dbtype</span> refGene <span class=\"nt\">-buildver</span> hg19 example/ex1.avinput <span class=\"nb\">test</span>/\nannotate_variation.pl <span class=\"nt\">-filter</span> <span class=\"nt\">-dbtype</span> exac03 <span class=\"nt\">-buildver</span> hg19 example/ex1.avinput <span class=\"nb\">test</span>/\n\ntable_annovar.pl example/ex1.avinput <span class=\"nb\">test</span>/ <span class=\"nt\">-buildver</span> hg19 <span class=\"nt\">-out</span> myanno <span class=\"se\">\\</span>\n     <span class=\"nt\">-remove</span> <span class=\"nt\">-protocol</span> refGene,cytoBand,exac03,avsnp147,dbnsfp30a,gwasCatalog <span class=\"nt\">-operation</span> g,r,f,f,f,r <span class=\"se\">\\</span>\n     <span class=\"nt\">-nastring</span> <span class=\"nb\">.</span> <span class=\"nt\">-csvout</span> <span class=\"nt\">-polish</span> <span class=\"nt\">-xref</span> example/gene_xref.txt\ntable_annovar.pl example/ex1.avinput <span class=\"nb\">test</span>/ <span class=\"nt\">-buildver</span> hg19 <span class=\"nt\">--outfile</span> ex1 <span class=\"se\">\\</span>\n     <span class=\"nt\">-protocol</span> ensGene,refGene,ccdsGene,wgEncodeGencodeBasicV19,cytoBand,exac03,avsnp147,dbnsfp30a,gwasCatalog <span class=\"se\">\\</span>\n     <span class=\"nt\">-operation</span> g,g,g,g,r,f,f,f,r <span class=\"se\">\\</span>\n     <span class=\"nt\">-csvout</span> <span class=\"nt\">-polish</span> <span class=\"nt\">-remove</span> <span class=\"nt\">-nastring</span> <span class=\"nb\">.</span>\n</code></pre> </div></div>\n\n<p>The second <code class=\"language-plaintext highlighter-rouge\">table_annovar.pl</code> above works after symbolic links to relevant databases at humandb/ were created at test/.</p>\n\n<p>It is handy to create a VCF file to be used by VEP (see below),</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>convert2annovar.pl <span class=\"nt\">-format</span> annovar2vcf example/ex1.avinput <span class=\"o\">&gt;</span> ex1.vcf\nvep <span class=\"nt\">-i</span> ex1.vcf <span class=\"nt\">-o</span> ex1.vcfoutput <span class=\"nt\">--offline</span> <span class=\"nt\">--force_overwrite</span>\n</code></pre> </div></div>\n","dir":"/applications/","name":"ANNOVAR.md","path":"applications/ANNOVAR.md","url":"/applications/ANNOVAR.html"},{"sort":4,"layout":"default","title":"R/gsl","content":"# R/gsl\n\nWhen there is message,\n\n```\nconfigure: error: Need GSL version >= 1.16\nERROR: configuration failed for package ‘gsl’\n* removing ‘/rds/user/jhz22/hpc-work/R/gsl’\n* restoring previous ‘/rds/user/jhz22/hpc-work/R/gsl’\n\nThe downloaded source packages are in\n        ‘/tmp/Rtmp5Qygwr/downloaded_packages’\nWarning message:\nIn install.packages(\"gsl\") :\n  installation of package ‘gsl’ had non-zero exit status\n```\n\nOur first attempt is to load modules as for `R/gnn`\n\n```bash\nmodule avail gsl\nmodule load gsl/2.4\n```\n\nUnfortunately, the same error message remains.\n\nOur second attempt is to install from source\n\n```bash\nRscript -e 'download.packages(\"gsl\",\".\")'\ntar xvfz gsl_2.1-7.tar.gz\ncd gsl\nmv configure configure.sav\ncd -\ngsl-config --cflags\ngsl-config --libs\n```\n\nwhere we mask the default `configure` and the latter two commands give\n\n```\n-I/usr/local/Cluster-Apps/gsl/2.4/include\n-L/usr/local/Cluster-Apps/gsl/2.4/lib -lgsl -lgslcblas -lm\n```\n\nand we use template `Markvars.in` from gsl/src/ and create `Makevars` with the following contents,\n\n```\n# Kindly supplied by Dirk Eddelbuettel\n# set by configure\nGSL_CFLAGS = -I/usr/local/Cluster-Apps/gsl/2.4/include\nGSL_LIBS   = -L/usr/local/Cluster-Apps/gsl/2.4/lib -lgsl -lgslcblas -lm\n\n# combine to standard arguments for R\nPKG_CPPFLAGS =  $(GSL_CFLAGS) -I.\nPKG_LIBS = $(GSL_LIBS)\n```\n\nNow our installation is succesful with\n\n```bash\nR CMD INSTALL gsl\n```\n\nNote that it was proposed on the web to use an equivalence of `CFLAGS=$(gsl-config --cflags) LDFLAGS=$(gsl-config --cflags) R` and try `install.packages(\"gsl\")` but that does not work, either.\n\n## gsl_2.1-7.1\n\nThe requires more recent version of GNU gsl and we illustrate with 2.7 below.\n\n```bash\nwget -qO- https://mirror.ibcp.fr/pub/gnu/gsl/gsl-latest.tar.gz | \\\ntar xfz -\ncd gsl-2.7/\nconfigure --prefix=${HPC_WORK}\nmake\nmake install\nRscript -e 'install.packages(\"gsl\")'\n```\n","dir":"/applications/R/","name":"gsl.md","path":"applications/R/gsl.md","url":"/applications/R/gsl.html"},{"sort":4,"layout":"default","title":"Policies","content":"# Policies\n\nSee [https://docs.hpc.cam.ac.uk/hpc/user-guide/policies.html#acknowledging-csd3](https://docs.hpc.cam.ac.uk/hpc/user-guide/policies.html#acknowledging-csd3).\n","dir":"/systems/","name":"policies.md","path":"systems/policies.md","url":"/systems/policies.html"},{"sort":4,"layout":"default","title":"Admonition card","content":"<h1 id=\"admonition-card\">Admonition card</h1>\n\n<h2 id=\"note\">note</h2>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>```note\n### This is a note\n\nMarkdown is supported, Text can be **bold**, _italic_, or ~~strikethrough~~. [Links](https://github.com) should be blue with no underlines\n\n`inline code`\n\n[`inline code inside link`](#)\n```\n</code></pre> </div></div>\n\n<pre><code class=\"language-note\">### This is a note\n\nMarkdown is supported, Text can be **bold**, _italic_, or ~~strikethrough~~. [Links](https://github.com) should be blue with no underlines\n\n`inline code`\n\n[`inline code inside link`](#)\n</code></pre> \n\n<h2 id=\"tip\">tip</h2>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>```tip\nIt’s bigger than a bread box.\n```\n</code></pre> </div></div>\n\n<pre><code class=\"language-tip\">It’s bigger than a bread box.\n</code></pre> \n\n<h2 id=\"warning\">warning</h2>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>```warning\nStrong prose may provoke extreme mental exertion. Reader discretion is strongly advised.\n```\n</code></pre> </div></div>\n\n<pre><code class=\"language-warning\">Strong prose may provoke extreme mental exertion. Reader discretion is strongly advised.\n</code></pre> \n\n<h2 id=\"danger\">danger</h2>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>```danger\nMad scientist at work!\n```\n</code></pre> </div></div>\n\n<pre><code class=\"language-danger\">Mad scientist at work!\n</code></pre> \n","dir":"/writing/","name":"admonitions.md","path":"writing/admonitions.md","url":"/writing/admonitions.html"},{"sort":4,"layout":"default","title":"Mermaid Test","content":"# Mermaid Test\n\n    ```mermaid\n    graph TB\n        c1-->a2\n        subgraph one\n        a1-->a2\n        end\n        subgraph two\n        b1-->b2\n        end\n        subgraph three\n        c1-->c2\n        end\n    ```\n\n```mermaid\ngraph TB\n    c1-->a2\n    subgraph one\n    a1-->a2\n    end\n    subgraph two\n    b1-->b2\n    end\n    subgraph three\n    c1-->c2\n    end\n```\n\n```mermaid\ngraph TD;\n    A-->B;\n    A-->C;\n    B-->D;\n    C-->D;\n```\n\n```mermaid\nclassDiagram\nclassA <|-- classB\nclassC *-- classD\nclassE o-- classF\nclassG <-- classH\nclassI -- classJ\nclassK <.. classL\nclassM <|.. classN\nclassO .. classP\n```\n\n```mermaid\nerDiagram\n    CUSTOMER ||--o{ ORDER : places\n    ORDER ||--|{ LINE-ITEM : contains\n    CUSTOMER }|..|{ DELIVERY-ADDRESS : uses\n```\n","dir":"/writing/test/","name":"mermaid.md","path":"writing/test/mermaid.md","url":"/writing/test/mermaid.html"},{"sort":5,"layout":"default","title":"R/LDlinkR","content":"<h1 id=\"rldlinkr\">R/LDlinkR</h1>\n\n<p>Issue <code class=\"language-plaintext highlighter-rouge\">install.packages(\"LDlinkR\")</code> from R but requires registration at <a href=\"https://ldlink.nci.nih.gov/?tab=apiaccess\">https://ldlink.nci.nih.gov/?tab=apiaccess</a>.</p>\n","dir":"/applications/R/","name":"LDlinkR.md","path":"applications/R/LDlinkR.md","url":"/applications/R/LDlinkR.html"},{"sort":5,"layout":"default","title":"aria2","content":"<h1 id=\"aria2\">aria2</h1>\n\n<p>Web: <a href=\"https://aria2.github.io/\">https://aria2.github.io/</a>.</p>\n\n<p>On CSD3, one can use the prerequiste <code class=\"language-plaintext highlighter-rouge\">module load aria2-1.33.1-gcc-5.4.0-r36jubs</code>.</p>\n\n<h2 id=\"installation\">Installation</h2>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">export </span><span class=\"nv\">HPC_WORK</span><span class=\"o\">=</span>/rds/user/<span class=\"k\">${</span><span class=\"nv\">USER</span><span class=\"k\">}</span>/hpc-work\n<span class=\"nb\">cd</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>\nwget <span class=\"nt\">-qO-</span> https://github.com/aria2/aria2/releases/download/release-1.36.0/aria2-1.36.0.tar.gz | <span class=\"se\">\\</span>\n<span class=\"nb\">tar </span>xvfz -\n<span class=\"nb\">cd </span>aria2-1.36.0/\nmodule load gettext-0.19.8.1-gcc-5.4.0-zaldouz\n./configure <span class=\"nt\">--prefix</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span> <span class=\"nt\">--enable-static</span>\nmake\nmake <span class=\"nb\">install</span>\n</code></pre> </div></div>\n\n<h2 id=\"examples\">Examples</h2>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>aria2c <span class=\"nt\">-h</span>\naria2c <span class=\"nt\">-i</span> urls.txt\n<span class=\"nb\">awk</span> <span class=\"s1\">'NR==10'</span> urls.txt | <span class=\"se\">\\</span>\naria2c <span class=\"nt\">-i</span> -\n</code></pre> </div></div>\n\n<p>The commands give usage information, download files in <code class=\"language-plaintext highlighter-rouge\">urls.txt</code> and the tenth file only, respectively.</p>\n","dir":"/applications/","name":"aria2.md","path":"applications/aria2.md","url":"/applications/aria2.html"},{"sort":5,"layout":"default","title":"email","content":"# email\n\nThis can be done with `mutt` from the console.\n","dir":"/systems/","name":"email.md","path":"systems/email.md","url":"/systems/email.html"},{"sort":5,"permalink":"/writing/","layout":"default","title":"Writing Related","content":"<h1 id=\"writing-related\">Writing Related</h1>\n\n<ul>\n  <li><a href=\"/writing/front-matter.html\">Front matter</a></li>\n  <li><a href=\"/writing/list.html\">List directory</a></li>\n  <li><a href=\"/writing/mathjax.html\">Mathjax</a></li>\n  <li><a href=\"/writing/admonitions.html\">Admonition card</a></li>\n  <li><a href=\"/writing/test/\">Test Documentation</a>\n    <ul>\n      <li><a href=\"/writing/test/markdown.html\">Markdown Elements</a></li>\n      <li><a href=\"/writing/test/toasts.html\">Toasts Card</a></li>\n      <li><a href=\"/writing/test/codes.html\">Code Blocks</a></li>\n      <li><a href=\"/writing/test/mermaid.html\">Mermaid Test</a></li>\n      <li><a href=\"/writing/test/emoji.html\">Emoji Test</a></li>\n      <li><a href=\"/writing/test/gist.html\">Gist Test</a></li>\n      <li><a href=\"/writing/test/avatar.html\">Avatar Test</a></li>\n      <li><a href=\"/writing/test/mentions.html\">Mentions Test</a></li>\n      <li><a href=\"/writing/test/fonts.html\">Fonts Test</a></li>\n      <li><a href=\"/writing/test/utilities.html\">Primer Utilities Test</a></li>\n    </ul>\n  </li>\n</ul>\n","dir":"/writing/","name":"README.md","path":"writing/README.md","url":"/writing/"},{"sort":5,"layout":"default","title":"Emoji Test","content":"# Emoji Test\n\n```\nI give this theme two :+1:!\n```\n\nI give this theme two :+1:!\n\n```tip\nSet config `plugins: [jemoji]`, Emoji searcher, see: [https://emoji.muan.co/](https://emoji.muan.co/)\n```\n","dir":"/writing/test/","name":"emoji.md","path":"writing/test/emoji.md","url":"/writing/test/emoji.html"},{"sort":6,"layout":"default","title":"R/magick","content":"# R/magick\n\n> The magick package provide a modern and simple toolkit for image processing in R. It wraps the ImageMagick STL which is the most comprehensive open-source image processing library available today.\n\nWhen installing the package we have error message,\n\n```\n> install.packages(\"magick\")\nInstalling package into ‘/rds/user/jhz22/hpc-work/R’\n(as ‘lib’ is unspecified)\ntrying URL 'https://cran.r-project.org/src/contrib/magick_2.7.1.tar.gz'\nContent type 'application/x-gzip' length 4813079 bytes (4.6 MB)\n==================================================\ndownloaded 4.6 MB\n\n* installing *source* package ‘magick’ ...\n** package ‘magick’ successfully unpacked and MD5 sums checked\n** using staged installation\nPackage Magick++ was not found in the pkg-config search path.\nPerhaps you should add the directory containing `Magick++.pc'\nto the PKG_CONFIG_PATH environment variable\nNo package 'Magick++' found\nUsing PKG_CFLAGS=\nUsing PKG_LIBS=-lMagick++-6.Q16\n--------------------------- [ANTICONF] --------------------------------\nConfiguration failed to find the Magick++ library. Try installing:\n - deb: libmagick++-dev (Debian, Ubuntu)\n - rpm: ImageMagick-c++-devel (Fedora, CentOS, RHEL)\n - csw: imagemagick_dev (Solaris)\n - brew imagemagick@6 (MacOS)\nFor Ubuntu versions Trusty (14.04) and Xenial (16.04) use our PPA:\n   sudo add-apt-repository -y ppa:cran/imagemagick\n   sudo apt-get update\n   sudo apt-get install -y libmagick++-dev\nIf Magick++ is already installed, check that 'pkg-config' is in your\nPATH and PKG_CONFIG_PATH contains a Magick++.pc file. If pkg-config\nis unavailable you can set INCLUDE_DIR and LIB_DIR manually via:\nR CMD INSTALL --configure-vars='INCLUDE_DIR=... LIB_DIR=...'\n-------------------------- [ERROR MESSAGE] ---------------------------\n<stdin>:1:22: fatal error: Magick++.h: No such file or directory\ncompilation terminated.\n--------------------------------------------------------------------\nERROR: configuration failed for package ‘magick’\n* removing ‘/rds/user/jhz22/hpc-work/R/magick’\n\nThe downloaded source packages are in\n        ‘/tmp/RtmpI9BUec/downloaded_packages’\nWarning message:\nIn install.packages(\"magick\") :\n  installation of package ‘magick’ had non-zero exit status\n```\n\nAlthough `which convert` and `which display` both point to `/usr/bin`,\nit turned out that the required ImageMick needs to be loaded as follows,\n\n```bash\nmodule load image-magick-7.0.5-9-gcc-5.4.0-d4lemcc\n```\n","dir":"/applications/R/","name":"magick.md","path":"applications/R/magick.md","url":"/applications/R/magick.html"},{"sort":6,"layout":"default","title":"bcftools","content":"<h1 id=\"bcftools\">bcftools</h1>\n\n<p>Web: <a href=\"http://www.htslib.org/download/\">http://www.htslib.org/download/</a></p>\n\n<p>On CSD3, <code class=\"language-plaintext highlighter-rouge\">module list bcftools</code> gives a list of versions but none can query data from the web.</p>\n\n<p>However, the installation is straightforward.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget <span class=\"nt\">-qO-</span> https://github.com/samtools/bcftools/releases/download/1.12/bcftools-1.12.tar.bz2 | <span class=\"se\">\\</span>\n<span class=\"nb\">tar </span>xjf -\n<span class=\"nb\">cd </span>bcftools-1.12/\n./configure <span class=\"nt\">--prefix</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>\nmake\nmake <span class=\"nb\">install\n</span>bcftools <span class=\"nt\">--version</span>\n</code></pre> </div></div>\n\n<p>The data query example as in <code class=\"language-plaintext highlighter-rouge\">tabix</code> is quoted here.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>bcftools query <span class=\"nt\">-f</span> <span class=\"s1\">'%ID\\t%ALT\\t%REF\\t%AF\\t[%ES]\\t[%SE]\\t[%LP]\\t[%SS]\\t%CHROM\\t%POS\\n'</span> <span class=\"nt\">-r</span> 1:1-1000000 <span class=\"se\">\\</span>\n               https://gwas.mrcieu.ac.uk/files/ebi-a-GCST010776/ebi-a-GCST010776.vcf.gz\n</code></pre> </div></div>\n\n<p>A number of other options can be used together with -r or -R.</p>\n\n<p>We can also obtain data with the header (-H)</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">export </span><span class=\"nv\">f</span><span class=\"o\">=</span>ebi-a-GCST010776\nbcftools query <span class=\"nt\">-f</span> <span class=\"s1\">'%ID\\t%ALT\\t%REF\\t%AF\\t[%ES]\\t[%SE]\\t[%LP]\\t[%SS]\\t%CHROM\\t%POS\\n'</span> <span class=\"se\">\\</span>\n               <span class=\"nt\">-H</span> <span class=\"se\">\\</span>\n               <span class=\"nt\">-r</span> 1:1-1000000 <span class=\"se\">\\</span>\n               https://gwas.mrcieu.ac.uk/files/<span class=\"k\">${</span><span class=\"nv\">f</span><span class=\"k\">}</span>/<span class=\"k\">${</span><span class=\"nv\">f</span><span class=\"k\">}</span>.vcf.gz | <span class=\"se\">\\</span>\n               <span class=\"nb\">sed</span> <span class=\"s2\">\"s/</span><span class=\"se\">\\[</span><span class=\"s2\">[0-9]*</span><span class=\"se\">\\]</span><span class=\"s2\">//g;s/^[</span><span class=\"se\">\\#</span><span class=\"s2\">] //;s/</span><span class=\"k\">${</span><span class=\"nv\">f</span><span class=\"k\">}</span><span class=\"s2\">://g\"</span> <span class=\"o\">&gt;</span> chr1:1-1000000.dat\n</code></pre> </div></div>\n","dir":"/applications/","name":"bcftools.md","path":"applications/bcftools.md","url":"/applications/bcftools.html"},{"sort":6,"layout":"default","title":"Software","content":"# Software\n\nThe CEU software repository is here, /usr/local/Cluster-Apps/ceuadmin/. As of June 2021, the list is\n\n```\nbgenix/                       JAGS/                        plink_linux_x86_64_beta2a/    raremetal_4.14.0/                   snptest_2.5.4_beta3/\nbiobank/                      LDstore/                     plink_linux_x86_64_beta3.32/  raremetal_4.14.1/                   snptest_new/\nboltlmm/                      locuszoom/                   plinkseq-0.08-x86_64/         raremetal_BPGen/                    source/\nboltlmm_2.2/                  magma/                       plinkseq-0.10/                Raremetal_linux_executables/        stata/\ncrossmap/                     MAGMA_Celltyping/            qctool_v1.4-linux-x86_64/     Raremetal_linux_executables.tgz     tabix/\nexomeplus/                    metabolomics/                R/                            raremetal.log                       temp/\ngcta/                         metal/                       raremetal_4.13/               regenie/                            vcftools/\ngtool_v0.7.5_x86_64/          metal_updated/               raremetal_4.13.3/             samtools-1.10.tar.bz2               vcftools_ps629/\nhpg/                          plink/                       raremetal_4.13.4/             samtools_1.2/\nhtslib/                       plink_1.90_beta/             raremetal_4.13.5/             shapeit.v2.r790.RHELS_5.4.dynamic/\nimpute_v2.3.2_x86_64_static/  plink_bgi_Dev/               raremetal_4.13.7/             snptest/\ninterval/                     plink-bgi_linux_x86_64_may/  raremetal_4.13.8/             snptest_2.5.2/\n```\n\nThese can be loaded with `module load ceuadmin/<module name>`.\n\n#### gcc\n\nIt is one of the critical software to use, e.g.,\n\n```bash\nmodule avail gcc\ngcc --version\n```\n\n#### gfortran\n\n```bash\ngfortran --version\n```\n\n#### git\n\nTo have the latest git, e.g.,\n\n```bash\nwget -qO- https://github.com/git/git/archive/v2.30.0.tar.gz | tar xfz -\ncd git-2.30.0\nmake NO_GETTEXT=YesPlease install\n```\n\nand the executables will be put to ~/bin.\n\n#### JAVA\n\n```bash\nmodule avail openjdk\njava -version\n```\n\n#### libraOffice\n\nThe executables (`oocalc`, `ooffice`, `ooimpress`, `oomath`, `ooviewdoc`, `oowriter`) are in the `/usr/bin` directory and can be conveniently called from the console, e.g.,\n\n```bash\noowriter README.docx\n```\n\nto load the Word document.\n\n#### matlab\n\nOfficial website: [https://www.mathworks.com/products/matlab.html](https://www.mathworks.com/products/matlab.html).\n\n```\nmodule avail matlab\nmodule load matlab/r2019b\n```\n\nfollowed by `matlab`.\n\n#### MySQL\n\nOne could access databases elsewhere, e.g., at UCSC -- see examples on VEP.\n\n> There isn't any MySQL cluster running as a general service on CSD3. Do you believe your group has something running on a VM hosted on our network possibly? If you need a database for your work, running it in your own department and then allowing access to it from CSD3. Databases are not suitable candidates to run on a HPC cluster, the resource requirements are different and by definition they need to be running continuously whilst access is required, so wouldn't be run via slurm for example.\n\n#### Python\n\nOfficial website: [https://www.python.org/](https://www.python.org/).\n\nThis can be invoked from a CSD3 console via `python` and `python3`. Libraries can be installed via `pip` and `pip3` (or equivalently `python -m pip install` and `python3 -m pip install`), e.g., the script\n\n```bash\npip install mygene --user\npip install tensorflow --user\npip install keras --user\npip install jupyter --user\n```\n\ninstalls libraries at `$HOME/.local`.\n\nIt is advised to use virual environments, i.e.,\n\n```bash\n# inherit system-wide packages as well\nmodule load python/3.5\nvirtualenv --system-site-packages py35\nsource py35/bin/activate\n# pip new packages\ndeactivate\n```\n\nNote that when this is set up, one only needs to restart from the `source` command. The `pip` is appropriate for installing small number of package; otherwise Anaconda ([https://www.anaconda.com/](https://www.anaconda.com/)) and Jupyter notebook ([https://jupyter.org/](https://jupyter.org/)) are useful.\n\nWe first load Anaconda and create virtual environments,\n\n```bash\nmodule avail miniconda\nmodule load miniconda/2\nconda create -n py27 python=2.7 ipykernel\nsource activate py27\n```\n\nfor Python 2.7 at `/home/$USER/.conda/envs/py27`, where envs could be replaced with the `--prefix` option. These are only required once.\n\nWe can also load Anaconda and activate Python 3.5,\n\n```bash\nmodule load miniconda/3\nconda create -n py35 python=3.5 ipykernel\nsource activate py35\n```\n\nand follow [Autoencoder in Keras tutorial](https://www.datacamp.com/community/tutorials/autoencoder-keras-tutorial) on\ndata from [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)\n\nThe Jupyter notebook can be started as follows,\n\n```bash\nhostname\n$HOME/.local/bin/jupyter notebook --ip=127.0.0.1 --no-browser --port 8081\n```\n\nIf it fails to assign the port number, let the system choose (by dropping the `--port` option). The process which use the port can be shown with `lsof -i:8081` or stopped by `lsof -ti:8081 | xargs kill -9`. The command `hostname` gives the name of the node. Once the port number is assigned, it is\nused by another ssh session _elsewhere_ and the URL generated openable from a browser, e.g.,\n\n```bash\nssh -4 -L 8081:127.0.0.1:8081 -fN <hostname>.hpc.cam.ac.uk\nfirefox <URL> &\n```\n\npaying attention to the port number as it may change.\n\nAn `hello world` example is [hello.ipynb](/systems/files/hello.ipynb) from which [hello.html](/systems/files/hello.html) and [hello.pdf](/systems/files/hello.pdf) were generated with `jupyter nbconvert --to html|pdf hello.ipynb`.\n\nSee also https://docs.hpc.cam.ac.uk/hpc/software-tools/python.html#using-anaconda-python and https://docs.hpc.cam.ac.uk/hpc/software-packages/jupyter.html.\n\nSee [HPC docuementation](https://docs.hpc.cam.ac.uk/hpc/) for additional information on PyTorch, Tensorflow and GPU.\n\n#### R\n\nOfficial website: [https://www.r-project.org/](https://www.r-project.org/) and also [https://bioconductor.org/](https://bioconductor.org/).\n\nUnder HPC, the default version is 3.3.3 with /usr/bin/R; alternatively choose the desired version of R from\n\n```bash\nmodule avail R\nmodule avail r\n# if you would also like to use RStudio\nmodule avail rstudio\n```\n\ne.g., `module load r-3.6.0-gcc-5.4.0-bzuuksv rstudio/1.1.383`.\n\nFor information about Bioconductor installation, see [https://bioconductor.org/install/](https://bioconductor.org/install/).\n\nThe following code installs package for weighted correlation network analysis ([WGCNA](https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/)).\n\n```r\n# from CRAN\ndependencies <- c(\"matrixStats\", \"Hmisc\", \"splines\", \"foreach\", \"doParallel\",\n                  \"fastcluster\", \"dynamicTreeCut\", \"survival\")\ninstall.packages(dependencies)\n# from Bioconductor\nbiocPackages <- c(\"GO.db\", \"preprocessCore\", \"impute\", \"AnnotationDbi\")\n# R < 3.5.0\nsource(\"http://bioconductor.org/biocLite.R\")\nbiocLite(biocPackages)\n# R >= 3.5.0\ninstall.packages(\"BiocManager\")\nBiocManager::install(biocPackages)\ninstall.packages(\"WGCNA\")\n```\n\nA good alternative is to use `remotes` or `devtools` package, e.g.,\n\n```r\nremotes::install_bioc(\"snpStats\")\n```\n\nA separate example is from r-forge, e.g.,\n\n```r\nrforge <- \"http://r-forge.r-project.org\"\ninstall.packages(\"estimate\", repos=rforge, dependencies=TRUE)\n```\n\nIn case of difficulty it is still useful to install directly, e.g.,\n\n```bash\nwget http://master.bioconductor.org/packages//2.10/bioc/src/contrib/ontoCAT_1.8.0.tar.gz\nR CMD INSTALL ontoCAT_1.8.0.tar.gz\n# Alternatively,\nR -e \"install.packages('ontoCAT_1.8.0.tar.gz',repos=NULL)\"\n```\n\nThe package installation directory can be spefied explicitly with R_LIBS, i.e.,\n\n```bash\nexport R_LIBS=/rds/user/$USER/hpc-work/R:/rds/user/$USER/hpc-work/R-3.6.1/library\n```\n\n#### SLURM\n\nOfficial website: [https://slurm.schedmd.com/](https://slurm.schedmd.com/).\n\nLocation at csd3: `/usr/local/Cluster-Docs/SLURM/`.\n\n##### Account details\n\n```bash\nmybalance\n```\n\n##### Partition\n\n```bash\nscontrol show partition\n```\n\n##### An interacive job\n\n```bash\nsintr -A MYPROJECT -p skylake -N2 -n2 -t 1:0:0 --qos=INTR\n```\n\nand also\n\n```bash\nsrun -N1 -n1 -c4 -p skylake-himem -t 12:0:0 --pty bash -i\n```\n\nthen check with `squeue -u $USER`, `qstat -u $USER` and `sacct`. The directory `/usr/local/software/slurm/current/bin/` contains all the executables while sample scripts are in `/usr/local/Cluster-Docs/SLURM`, e.g., [template for Skylake](/systems/files/slurm_submit.peta4-skylake).\n\n**NOTE** the skylakes are approaching end of life, see [https://docs.hpc.cam.ac.uk/hpc/user-guide/cclake.html](https://docs.hpc.cam.ac.uk/hpc/user-guide/cclake.html) and [https://docs.hpc.cam.ac.uk/hpc/user-guide/icelake.html](https://docs.hpc.cam.ac.uk/hpc/user-guide/icelake.html). For Ampere GPG, see [https://docs.hpc.cam.ac.uk/hpc/user-guide/a100.html](https://docs.hpc.cam.ac.uk/hpc/user-guide/a100.html).\n\n##### Use of modules\n\nThe following is part of a real implementation.\n\n```bash\n. /etc/profile.d/modules.sh\nmodule purge\nmodule load rhel7/default-peta4\nmodule load gcc/6\nmodule load aria2-1.33.1-gcc-5.4.0-r36jubs\n```\n\n##### An example\n\nTo convert a large number of PDF files (INTERVAL.\\*.manhattn.pdf) to PNG with smaller file sizes. To start, we build a file list, and pipe into ``parallel`.\n\n```bash\nls *pdf | \\\nsed 's/INTERVAL.//g;s/.manhattan.pdf//g' | \\\nparallel -j8 -C' ' '\n  echo {}\n  pdftopng -r 300 INTERVAL.{}.manhattan.pdf\n  mv {}-000001.png INTERVAL.{}.png\n'\n```\n\nwhich is equivalent to SLURM implementation using array jobs (https://slurm.schedmd.com/job_array.html).\n\n```bash\n#!/usr/bin/bash\n\n#SBATCH --ntasks=1\n#SBATCH --job-name=pdftopng\n#SBATCH --time=6:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --partition=skylake\n#SBATCH --array=1-50%10\n#SBATCH --output=pdftopng_%A_%a.out\n#SBATCH --error=pdftopng_%A_%a.err\n#SBATCH --export ALL\n\nexport p=$(awk 'NR==ENVIRON[\"SLURM_ARRAY_TASK_ID\"]' INTERVAL.list)\nexport TMPDIR=/rds/user/$USER/hpc-work/\n\necho ${p}\npdftopng -r 300 INTERVAL.${p}.manhattan.pdf ${p}\nmv ${p}-000001.png INTERVAL.${p}.png\n```\n\ninvoked by `sbatch`. As with Cardio, it is helpful to set a temporary directory, i.e.,\n\n```bash\nexport TMPDIR=/rds/user/$USER/hpc-work/\n```\n\n##### Neither `parallel` nor SLURM\n\n```bash\nexport url=https://zenodo.org/record/2615265/files/\nif [ ! -d ~/rds/results/public/proteomics/scallop-cvd1 ]; then mkdir ~/rds/results/public/proteomics/scallop-cvd1; fi\ncat cvd1.txt | xargs -I {} bash -c \"wget ${url}/{}.txt.gz -O ~/rds/results/public/proteomics/scallop-cvd1/{}.txt.gz\"\n#  ln -s ~/rds/results/public/proteomics/scallop-cvd1\n```\n\nwhich downloads the SCALLOP-cvd1 sumstats for proteins listed in `cvd1.txt`.\n\n##### Trouble shooting\n\nWith error message\n\n```\nsqueue: error: _parse_next_key: Parsing error at unrecognized key:\nInteractiveStepOptions\nsqueue: error: Parse error in file\n/usr/local/software/slurm/slurm-20.11.4/etc/slurm.conf line 22:\n\"InteractiveStepOptions=\"--pty --preserve-env --mpi=none $SHELL\"\"\nsqueue: fatal: Unable to process configuration file\n```\n\nthen either log out and login again, or\n\n```bash\nunset SLURM_CONF\n```\n\n#### Stata\n\nOfficial website: [https://www.stata.com/](https://www.stata.com/).\n\nAs a CEU member the following is possible,\n\n```\nmodule load ceuadmin/stata/14\n```\n\nas with `ceuadmin/stata/15`. The meta-analysis (metan) and Mendelian Randomisation (mrrobust) packages can be installed as follows,\n\n```stata\nssc install metan\nnet install mrrobust, from(\"https://raw.github.com/remlapmot/mrrobust/master/\") replace\n```\n","dir":"/systems/","name":"software.md","path":"systems/software.md","url":"/systems/software.html"},{"sort":6,"layout":"default","title":"Gist Test","content":"# Gist Test\n\n```\n{% raw %}{% gist c08ee0f2726fd0e3909d %}{% endraw %}\n```\n\n{% gist c08ee0f2726fd0e3909d %}\n","dir":"/writing/test/","name":"gist.md","path":"writing/test/gist.md","url":"/writing/test/gist.html"},{"sort":7,"layout":"default","title":"R/PhenoScanner","content":"<h1 id=\"rphenoscanner\">R/PhenoScanner</h1>\n\n<h2 id=\"r-package-setup\">R package setup</h2>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>install.packages<span class=\"o\">(</span><span class=\"s2\">\"devtools\"</span><span class=\"o\">)</span>\nlibrary<span class=\"o\">(</span>devtools<span class=\"o\">)</span>\ninstall_github<span class=\"o\">(</span><span class=\"s2\">\"phenoscanner/phenoscanner\"</span><span class=\"o\">)</span>\nlibrary<span class=\"o\">(</span>phenoscanner<span class=\"o\">)</span>\nexample<span class=\"o\">(</span>phenoscanner<span class=\"o\">)</span>\n</code></pre> </div></div>\n\n<h2 id=\"long-query\">Long query</h2>\n\n<p>The call is made by chunks, e.g.,</p>\n\n<div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">options</span><span class=\"p\">(</span><span class=\"n\">width</span><span class=\"o\">=</span><span class=\"m\">500</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">require</span><span class=\"p\">(</span><span class=\"n\">phenoscanner</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">rsid</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">scan</span><span class=\"p\">(</span><span class=\"s2\">\"swath-ms-invn.snp\"</span><span class=\"p\">,</span><span class=\"n\">what</span><span class=\"o\">=</span><span class=\"s2\">\"\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">batches</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">rsid</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nf\">ceiling</span><span class=\"p\">(</span><span class=\"nf\">seq_along</span><span class=\"p\">(</span><span class=\"n\">rsid</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"m\">100</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"n\">s</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">t</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">list</span><span class=\"p\">()</span><span class=\"w\">\n</span><span class=\"k\">for</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">:</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">batches</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"n\">q</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">phenoscanner</span><span class=\"p\">(</span><span class=\"n\">snpquery</span><span class=\"o\">=</span><span class=\"n\">batches</span><span class=\"p\">[[</span><span class=\"n\">i</span><span class=\"p\">]],</span><span class=\"w\"> </span><span class=\"n\">catalogue</span><span class=\"o\">=</span><span class=\"s2\">\"pQTL\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">proxies</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"EUR\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">pvalue</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">1e-07</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">r2</span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">0.6</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">build</span><span class=\"o\">=</span><span class=\"m\">37</span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"n\">s</span><span class=\"p\">[[</span><span class=\"n\">i</span><span class=\"p\">]]</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">with</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span><span class=\"n\">snps</span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"n\">t</span><span class=\"p\">[[</span><span class=\"n\">i</span><span class=\"p\">]]</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">with</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span><span class=\"n\">results</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"n\">snps</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">do.call</span><span class=\"p\">(</span><span class=\"n\">rbind</span><span class=\"p\">,</span><span class=\"n\">s</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">results</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">do.call</span><span class=\"p\">(</span><span class=\"n\">rbind</span><span class=\"p\">,</span><span class=\"n\">t</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">r</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">list</span><span class=\"p\">(</span><span class=\"n\">snps</span><span class=\"o\">=</span><span class=\"n\">snps</span><span class=\"p\">,</span><span class=\"n\">results</span><span class=\"o\">=</span><span class=\"n\">results</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre> </div></div>\n\n<p>i.e., each chunk has 100 SNPs and chunks are combined manually.</p>\n\n<h2 id=\"command-line-interface-cli\">Command-line interface (CLI)</h2>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>module load ceuadmin/phenoscanner\nphenoscanner <span class=\"nt\">--help</span>\nphenoscanner <span class=\"nt\">--snp</span><span class=\"o\">=</span>rs704 <span class=\"nt\">-c</span> All <span class=\"nt\">-x</span> EUR <span class=\"nt\">-r</span> 0.8\nphenoscanner <span class=\"nt\">-s</span> T <span class=\"nt\">-c</span> All <span class=\"nt\">-x</span> EUR <span class=\"nt\">-p</span> 0.0000001 <span class=\"nt\">--r2</span> 0.6 <span class=\"nt\">-i</span> INF1.merge.snp <span class=\"nt\">-o</span> INF1\n</code></pre> </div></div>\n\n<p>Note that <code class=\"language-plaintext highlighter-rouge\">module load phenoscanner</code> is enabled from ~/.bashrc:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">export </span><span class=\"nv\">MODULEPATH</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">MODULEPATH</span><span class=\"k\">}</span>:/usr/local/Cluster-Config/modulefiles/ceuadmin/\n</code></pre> </div></div>\n\n<p>via <code class=\"language-plaintext highlighter-rouge\">source ~/.bashrc</code> or a new login.</p>\n\n<h2 id=\"r-4xx\">R 4.x.x</h2>\n\n<p>Section above would fail under R 4.x.x; to get around, make a copy of phenoscanner according to</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>module load ceuadmin/phenoscanner\nwhich phenoscanner\n<span class=\"c\"># /rds/project/jmmh2/rds-jmmh2-projects/phenoscanner/mrcatalogue/mrcatalogue/phenoscanner_v2/phenoscanner</span>\n</code></pre> </div></div>\n\n<p>and edit the header to call packages at the default R_LIBS location,</p>\n\n<div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">#!/rds/user/jhz22/hpc-work/bin/Rscript</span><span class=\"w\">\n</span><span class=\"n\">suppressPackageStartupMessages</span><span class=\"p\">(</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">getopt</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"n\">suppressPackageStartupMessages</span><span class=\"p\">(</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">optparse</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"n\">suppressPackageStartupMessages</span><span class=\"p\">(</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">DBI</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"n\">suppressPackageStartupMessages</span><span class=\"p\">(</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">RMySQL</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"n\">suppressPackageStartupMessages</span><span class=\"p\">(</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">reshape2</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"n\">suppressPackageStartupMessages</span><span class=\"p\">(</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">plyr</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"n\">suppressPackageStartupMessages</span><span class=\"p\">(</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">stringi</span><span class=\"p\">))</span><span class=\"w\">\n</span></code></pre> </div></div>\n\n<p>then deposit this to a directory on the search path and invoke,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>module load gcc/6\nphenoscanner <span class=\"nt\">-h</span>\nphenoscanner <span class=\"nt\">-s</span> chr5:29439275\n</code></pre> </div></div>\n\n<p>and we have <code class=\"language-plaintext highlighter-rouge\">chr5:29439275_PhenoScanner_SNP_Info.tsv</code> and <code class=\"language-plaintext highlighter-rouge\">chr5:29439275_PhenoScanner_GWAS.tsv</code> for variant annotation and GWAS lookup, respectively; one can add <code class=\"language-plaintext highlighter-rouge\">-c None</code> to the last command and get the SNP annotation only.</p>\n","dir":"/applications/R/","name":"PhenoScanner.md","path":"applications/R/PhenoScanner.md","url":"/applications/R/PhenoScanner.html"},{"sort":7,"layout":"default","title":"bedops","content":"<h1 id=\"bedops\">bedops</h1>\n\n<p>Web, https://bedops.readthedocs.io/en/latest/</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget <span class=\"nt\">-qO-</span> https://github.com/bedops/bedops/releases/download/v2.4.39/bedops_linux_x86_64-v2.4.39.tar.bz2 | <span class=\"se\">\\</span>\n<span class=\"nb\">tar </span>xfj -\n</code></pre> </div></div>\n\n<p>would extract the executables into the bin/ directory.</p>\n","dir":"/applications/","name":"bedops.md","path":"applications/bedops.md","url":"/applications/bedops.html"},{"sort":7,"layout":"default","title":"Training","content":"# Training\n\n- **First training**: Wednesday, 10th July 2019 from 10am – 12pm.\n- **Second training**: Wednesday, 31st July 2019 from 9:30am – 4pm.\n- **UCS**: [https://training.cam.ac.uk/ucs/](https://training.cam.ac.uk/ucs/).\n- **Handouts**: [June](https://www.hpc.cam.ac.uk/files/introduction_to_hpc-jun2019-handout_0.pdf) and [November](https://www.hpc.cam.ac.uk/files/introduction_to_hpc-nov2019.pdf), 2019.\n- **Presentation**: [Google document](https://tinyurl.com/y3l6jssg) by Praveen Surendran (<ps629@medschl.cam.ac.uk>).\n","dir":"/systems/","name":"training.md","path":"systems/training.md","url":"/systems/training.html"},{"sort":7,"layout":"default","title":"Avatar Test","content":"<h1 id=\"avatar-test\">Avatar Test</h1>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>{% avatar saowang %}\n</code></pre> </div></div>\n\n<p><img class=\"avatar avatar-small\" alt=\"saowang\" width=\"40\" height=\"40\" data-proofer-ignore=\"true\" src=\"https://avatars1.githubusercontent.com/saowang?v=3&amp;s=40\" srcset=\"https://avatars1.githubusercontent.com/saowang?v=3&amp;s=40 1x, https://avatars1.githubusercontent.com/saowang?v=3&amp;s=80 2x, https://avatars1.githubusercontent.com/saowang?v=3&amp;s=120 3x, https://avatars1.githubusercontent.com/saowang?v=3&amp;s=160 4x\" /></p>\n\n<pre><code class=\"language-tip\">Set config `plugins: [jekyll-avatar]`\n\nFor documentation, see: [https://github.com/benbalter/jekyll-avatar](https://github.com/benbalter/jekyll-avatar)\n</code></pre> \n","dir":"/writing/test/","name":"avatar.md","path":"writing/test/avatar.md","url":"/writing/test/avatar.html"},{"sort":8,"layout":"default","title":"R/PKI","content":"<h1 id=\"rpki\">R/PKI</h1>\n\n<p>When installing/updating the package, the following error message appears,</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Error: package or namespace load failed for ‘PKI’ in dyn.load(file, DLLpath = DLLpath, ...):\n unable to load shared object '/rds/user/jhz22/hpc-work/R/00LOCK-PKI/00new/PKI/libs/PKI.so':\n  /rds/user/jhz22/hpc-work/R/00LOCK-PKI/00new/PKI/libs/PKI.so: undefined symbol: EVP_CIPHER_CTX_reset\nError: loading failed\nExecution halted\nERROR: loading failed\n</code></pre> </div></div>\n\n<p>and one can get around with a twist of openssl, e.g.,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Rscript <span class=\"nt\">-e</span> <span class=\"s2\">\"download.packages('PKI','.')\"</span>\n<span class=\"nb\">tar </span>xvfz PKI_0.1-8.tar.gz\nmodule load openssl-1.1.0e-gcc-5.4.0-a4xxzqm\n<span class=\"nb\">cd </span>PKI\n./configure <span class=\"nv\">PKG_CPPFLAGS</span><span class=\"o\">=</span><span class=\"s2\">\"-I/usr/local/software/spack/spack-0.11.2/opt/spack/linux-rhel7-x86_64/gcc-5.4.0/openssl-1.1.0e-a4xxzqmcsb3o2o7yctmpxef3cp36qk33/include/openssl/\"</span> <span class=\"se\">\\</span>\n            <span class=\"nv\">PKG_LIBS</span><span class=\"o\">=</span><span class=\"s2\">\"-L/usr/local/software/spack/spack-0.11.2/opt/spack/linux-rhel7-x86_64/gcc-5.4.0/openssl-1.1.0e-a4xxzqmcsb3o2o7yctmpxef3cp36qk33/lib\"</span>\n<span class=\"c\"># with openssl-1.1.1h installed locally</span>\n<span class=\"c\"># ./configure PKG_CPPFLAGS=\"-I/rds-d4/user/jhz22/hpc-work/openssl-1.1.1h/include/openssl\" PKG_LIBS=\"-L/rds-d4/user/jhz22/hpc-work/openssl-1.1.1h\"</span>\n<span class=\"nb\">mv </span>configure configure.sav\n<span class=\"nb\">cd</span> -\nR CMD INSTALL PKI\n</code></pre> </div></div>\n","dir":"/applications/R/","name":"PKI.md","path":"applications/R/PKI.md","url":"/applications/R/PKI.html"},{"sort":8,"layout":"default","title":"bgenix","content":"<h1 id=\"bgenix\">bgenix</h1>\n\n<p>Web: <a href=\"https://enkre.net/cgi-bin/code/bgen/dir?ci=trunk\">https://enkre.net/cgi-bin/code/bgen/dir?ci=trunk</a></p>\n\n<p>It is now possible to compile 1.1.7 under Python 3.x.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/usr/bin/bash</span>\n<span class=\"c\"># get it</span>\nwget <span class=\"nt\">-qO-</span> http://code.enkre.net/bgen/tarball/release/bgen.tgz | <span class=\"nb\">tar </span>xvfz -\n<span class=\"nb\">mv </span>bgen.tgz bgenix\n<span class=\"nb\">cd </span>bgenix\n<span class=\"c\"># compile it</span>\n./waf configure\n./waf\n<span class=\"c\"># test it</span>\n./build/test/unit/test_bgen\n./build/apps/bgenix <span class=\"nt\">-g</span> example/example.16bits.bgen <span class=\"nt\">-list</span>\n</code></pre> </div></div>\n\n<p>and we could set up appropriate symbolic links.</p>\n","dir":"/applications/","name":"bgenix.md","path":"applications/bgenix.md","url":"/applications/bgenix.html"},{"sort":8,"layout":"default","title":"Acknowledgement","content":"<h1 id=\"acknowledgement\">Acknowledgement</h1>\n\n<p>The repository was created as a result of the cambridge-ceu organisation; information regarding system login, Jupyter, polyphen, Stata and VEP received inputs from HPC supporint team.</p>\n","dir":"/systems/","name":"acknowledgement.md","path":"systems/acknowledgement.md","url":"/systems/acknowledgement.html"},{"sort":8,"layout":"default","title":"Contacts","content":"<h1 id=\"contacts\">Contacts</h1>\n\n<ul>\n  <li><strong>CSD3 account</strong>: <a href=\"https://www.hpc.cam.ac.uk/rcs-application\">https://www.hpc.cam.ac.uk/rcs-application</a> as in <a href=\"https://www.hpc.cam.ac.uk/applications-access-research-computing-services\">Applications for Access to Research Computing Services</a>.</li>\n  <li><strong>HPC support</strong>: <a href=\"mailto:support@hpc.cam.ac.uk\">support@hpc.cam.ac.uk</a> with the title “cardio migration”.</li>\n  <li><strong>CEU contacts</strong>: Ank Michielsen (<a href=\"mailto:am2710@medschl.cam.ac.uk\">am2710@medschl.cam.ac.uk</a>) and Niko Ovenden (<a href=\"mailto:nao26@medschl.cam.ac.uk\">nao26@medschl.cam.ac.uk</a>).</li>\n  <li><strong>Request for software</strong>: Savita Karthikeyan (<a href=\"mailto:sk752@medschl.cam.ac.uk\">sk752@medschl.cam.ac.uk</a>).</li>\n  <li><strong>Contacts for data access</strong>. CSD3datamanager@medschl.cam.ac.uk.</li>\n</ul>\n","dir":"/systems/","name":"contacts.md","path":"systems/contacts.md","url":"/systems/contacts.html"},{"sort":8,"layout":"default","title":"Mentions Test","content":"# Mentions Test\n\n```\nHey @saowang, what do you think of this?\n```\n\nHey @saowang, what do you think of this?\n\n```tip\nSet config `plugins: [jekyll-mentions]`\n\nFor documentation, see: [https://github.com/jekyll/jekyll-mentions](https://github.com/jekyll/jekyll-mentions)\n```\n","dir":"/writing/test/","name":"mentions.md","path":"writing/test/mentions.md","url":"/writing/test/mentions.html"},{"sort":9,"layout":"default","title":"CaVEMaN","content":"<h1 id=\"caveman\">CaVEMaN</h1>\n\n<p>Web, https://github.com/funpopgen/CaVEMaN/</p>\n\n<p>Causal Variant Evidence Mapping with Non-parametric resampling (CaVEMaN) is one of the three fine-mapping software which provided data for GTEx v8.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># binary distribution</span>\nhttps://github.com/funpopgen/CaVEMaN/releases/download/v1.01/CaVEMaN\n<span class=\"c\"># build from source</span>\ngit clone https://github.com/funpopgen/CaVEMaN <span class=\"o\">&amp;&amp;</span> <span class=\"nb\">cd </span>CaVEMaN\nmodule load xz\nwget <span class=\"nt\">-qO-</span> https://github.com/ldc-developers/ldc/releases/download/v1.24.0/ldc2-1.24.0-linux-x86_64.tar.xz | <span class=\"se\">\\</span>\n<span class=\"nb\">tar </span>Jxf -\nmodule load gsl/2.1\n<span class=\"c\"># modify makefile</span>\nLDC :<span class=\"o\">=</span> <span class=\"k\">${</span><span class=\"nv\">PWD</span><span class=\"k\">}</span>/ldc2-1.24.0-linux-x86_64/bin/ldc2\nDMD :<span class=\"o\">=</span> <span class=\"k\">${</span><span class=\"nv\">PWD</span><span class=\"k\">}</span>/ldc2-1.24.0-linux-x86_64/bin/dmd\nGSL :<span class=\"o\">=</span> /usr/local/Cluster-Apps/gsl/2.1\nmake\n</code></pre> </div></div>\n\n<p>Note that the D compiler setup actually requires gsl/2.1.</p>\n","dir":"/applications/","name":"CaVEMaN.md","path":"applications/CaVEMaN.md","url":"/applications/CaVEMaN.html"},{"sort":9,"layout":"default","title":"R/plumber","content":"# R/plumber\n\n## Installation\n\nIt requires R/sodium which in turn requires `libsodium`, [https://doc.libsodium.org/](https://doc.libsodium.org/).\n\n```bash\nwget -qO- https://download.libsodium.org/libsodium/releases/libsodium-1.0.18.tar.gz | \\\ntar xvfz -\ncd libsodium-1.0.18\nconfigure --prefix=${HPC_WORK}\nmake\nmake install\n```\n\nFollowing this, we could issue `Rscript -e \"install.packages('plumber')\"`.\n\n## Example\n\nThis is adapted from [https://www.rplumber.io/](https://www.rplumber.io/).\n\nWe first create a file named [`plumber.R`](files/plumber.R) as follows,\n\n```r\n# plumber.R\n\n#* Echo back the input\n#* @param msg The message to echo\n#* @get /echo\nfunction(msg=\"\") {\n  list(msg = paste0(\"The message is: '\", msg, \"'\"))\n}\n\n#* Plot a histogram\n#* @serializer png\n#* @get /plot\nfunction() {\n  rand <- rnorm(100)\n  hist(rand)\n}\n\n#* Return the sum of two numbers\n#* @param a The first number to add\n#* @param b The second number to add\n#* @post /sum\nfunction(a, b) {\n  as.numeric(a) + as.numeric(b)\n}\n```\n\nWe could then start\n\n```r\nRscript -e \"library(plumber);pr('plumber.R') %>% pr_run(port=8000)\"\n```\n\nand GET and POST as follows,\n\n```bash\ncurl \"http://localhost:8000/echo\"\ncurl \"http://localhost:8000/echo?msg=hello\"\ncurl \"http://localhost:8000/plot\" | display # -o plot.png\ncurl --data \"a=4&b=3\" \"http://localhost:8000/sum\"\ncurl -H \"Content-Type: application/json\" --data '{\"a\":4, \"b\":5}' http://localhost:8000/sum\n```\n","dir":"/applications/R/","name":"plumber.md","path":"applications/R/plumber.md","url":"/applications/R/plumber.html"},{"sort":9,"text":"ABCDEFGHIJKLMNOPQRSTUVWXYZ\nabcdefghijklmnopqrstuvwxyz\n1234567890\n一二三四五六七八九十百千萬上中下左右大小春夏秋冬東南西北金木水火土\n‘?’“!”(%)[#]{@}/&\\<-+÷×=>®©$€£¥¢:;,.*\n","layout":"default","title":"Fonts Test","content":"# Fonts Test\n\n`{:.font-mono}`\n\n```\n{{ page.text -}}\n```\n\n`{:.font-body}`\n\n{:.font-body}\n{{ page.text }}\n\n`{:.font-head}`\n\n{:.font-head}\n{{ page.text }}\n\n## font-awesome\n\n```html\n<i class=\"fa fa-check-circle text-green\">checked</i>\n<i class=\"fa fa-battery-quarter text-red\">battery</i>\n```\n\n<i class=\"fa fa-check-circle text-green\">checked</i>\n<i class=\"fa fa-battery-quarter text-red\">battery</i>\n","dir":"/writing/test/","name":"fonts.md","path":"writing/test/fonts.md","url":"/writing/test/fonts.html"},{"sort":10,"layout":"default","title":"R/protolite","content":"# R/protolite\n\nWhen we issued `update.packages(ask=FALSE, checkBuilt=TRUE)`, we saw the following error message\n\n```\nError: package or namespace load failed for ‘protolite’ in dyn.load(file, DLLpath = DLLpath, ...):\n unable to load shared object '/rds/user/jhz22/hpc-work/R/00LOCK-protolite/00new/protolite/libs/protolite.so':\n  /rds/user/jhz22/hpc-work/R/00LOCK-protolite/00new/protolite/libs/protolite.so: undefined symbol: _ZNK6google8protobuf11MessageLite25InitializationErrorStringB5cxx11Ev\nError: loading failed\nExecution halted\nERROR: loading failed\n* removing ‘/rds/user/jhz22/hpc-work/R/protolite’\n* restoring previous ‘/rds/user/jhz22/hpc-work/R/protolite’\n```\n\nWe can get away with this,\n\n```bash\nmodule load  protobuf-3.4.0-gcc-5.4.0-zkpendv\nRscript -e \"install.packages('protolite')\"\n```\n","dir":"/applications/R/","name":"protolite.md","path":"applications/R/protolite.md","url":"/applications/R/protolite.html"},{"sort":10,"layout":"default","title":"citeproc","content":"<h1 id=\"citeproc\">citeproc</h1>\n\n<p>Web: <a href=\"https://hackage.haskell.org/package/citeproc\">https://hackage.haskell.org/package/citeproc</a> (<a href=\"https://github.com/jgm/citeproc\">GitHub</a>)</p>\n\n<p>We can set up the latest version as follows,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">cd</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>/\nwget <span class=\"nt\">-qO-</span> https://hackage.haskell.org/package/citeproc-0.4.0.1/citeproc-0.4.0.1.tar.gz | <span class=\"se\">\\</span>\n<span class=\"nb\">tar </span>xvfz -\n<span class=\"nb\">cd </span>citeproc-0.4.0.1\nmodule load cabal/3.0.0.0 gcc/6\ncabal <span class=\"nb\">install</span> <span class=\"nt\">--installdir</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>/bin exe:citeproc\n</code></pre> </div></div>\n\n<p>We have error messages as follows,</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>[1 of 1] Compiling Main             ( app/Main.hs, dist/build/citeproc/citeproc-tmp/Main.o )\n\napp/Main.hs:35:36: error:\n    • Variable not in scope: (&lt;&gt;) :: IO () -&gt; [Char] -&gt; IO a0\n    • Perhaps you meant one of these:\n        ‘&lt;|&gt;’ (imported from Control.Applicative),\n        ‘&lt;$&gt;’ (imported from Prelude), ‘*&gt;’ (imported from Prelude)\n   |\n35 |     putStrLn $ \"citeproc version \" &lt;&gt; VERSION_citeproc\n   |                                    ^^\ncabal: Failed to build exe:citeproc from citeproc-0.4.0.1. See the build log\nabove for details.\n</code></pre> </div></div>\n\n<p>and we change <code class=\"language-plaintext highlighter-rouge\">&lt;&gt;</code> to <code class=\"language-plaintext highlighter-rouge\">&lt;|&gt;</code> at line 35 of <code class=\"language-plaintext highlighter-rouge\">app/Main.hs</code> and repeat the last command with success.</p>\n\n<p>An example use with <code class=\"language-plaintext highlighter-rouge\">pandoc</code>:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>pandoc README.md <span class=\"nt\">--citeproc</span> <span class=\"nt\">--mathjax</span> <span class=\"nt\">-s</span> <span class=\"nt\">-o</span> index.html\n</code></pre> </div></div>\n\n<p>where <code class=\"language-plaintext highlighter-rouge\">README.md</code> contains lines,</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>---\ntitle: Shiny for Genetic Analysis Package (gap) Designs\noutput:\n  html_document:\n    mathjax:  default\n    fig_caption:  true\n    toc: true\n    section_numbering: true\nbibliography: shinygap.bib\nvignette: &gt;\n  %\\VignetteEngine{knitr::rmarkdown}\n  %\\VignetteIndexEntry{Shiny for Genetic Analysis Package (gap) Designs}\n  %\\VignetteEncoding{UTF-8}\n---\n\n# Appendix: Theory for gap designs\n\n## Family-based and population-based designs\n\nSee the R/gap package vignette jss or @zhao07.\n\n## Case-cohort design\n\n### Power\n\nFollowing @cai04, we have\n$$\\Phi\\left(Z_\\alpha+\\tilde{n}^\\frac{1}{2}\\theta\\sqrt{\\frac{p_1p_2p_D}{q+(1-q)p_D}}\\right)$$\n\nwhere $\\alpha$ is the significance level, $\\theta$ is the log-hazard ratio for\ntwo groups, $p_j, j = 1, 2$, are the proportion of the two groups\nin the population ($p_1 + p_2 = 1$), $\\tilde{n}$ is the total number of subjects in the subcohort, $p_D$ is the proportion of the failures in\nthe full cohort, and $q$ is the sampling fraction of the subcohort.\n\n### Sample size\n\n$$\\tilde{n}=\\frac{nBp_D}{n-B(1-p_D)}$$ where $B=\\frac{Z_{1-\\alpha}+Z_\\beta}{\\theta^2p_1p_2p_D}$ and $n$ is the whole cohort size.\n\n# References\n\n</code></pre> </div></div>\n\n<p>and <code class=\"language-plaintext highlighter-rouge\">shinygap.bib</code> contains two entries,</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>@article{cai04,\n   author = {Cai, J. and Zeng, D.},\n   title = {Sample size/power calculation for case–cohort studies},\n   journal = {Biometrics},\n   volume = {60},\n   number = {4},\n   pages = {1015-1024},\n   ISSN = {1471-0056},\n   DOI = {10.1111/j.0006-341X.2004.00257.x},\n   year = {2004},\n   pmid = {15606422},\n   type = {Journal Article}\n}\n\n@article{zhao07,\n   Author = {Zhao, J. H.},\n   Title = {gap: genetic analysis package},\n   Journal = {Journal of Statistical Software},\n   Volume = {23},\n   Number = {8},\n   Pages = {1-18},\n   Year = {2007}\n}\n</code></pre> </div></div>\n\n<p>We would like to use available styles as well from <a href=\"https://github.com/citation-style-language/styles\">https://github.com/citation-style-language/styles</a>, after which we add the following after the command <code class=\"language-plaintext highlighter-rouge\">bibliography</code>.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>csl: biomed-central.csl\n</code></pre> </div></div>\n","dir":"/applications/","name":"citeproc.md","path":"applications/citeproc.md","url":"/applications/citeproc.html"},{"sort":10,"layout":"default","title":"Primer Utilities Test","content":"# Primer Utilities Test\n\nText can be **bold**, _italic_, or ~~strikethrough~~. [Links](https://github.com) should be blue with no underlines (unless hovered over).\n\n{:.text-red}\nText can be **bold**, _italic_, or ~~strikethrough~~. [Links](https://github.com) should be blue with no underlines (unless hovered over).\n\n{:.bg-yellow-dark}\nText can be **bold**, _italic_, or ~~strikethrough~~. [Links](https://github.com) should be blue with no underlines (unless hovered over).\n\n{:.bg-yellow-dark.text-white}\nText can be **bold**, _italic_, or ~~strikethrough~~. [Links](https://github.com) should be blue with no underlines (unless hovered over).\n\n{:.bg-yellow-dark.text-white.m-5}\nText can be **bold**, _italic_, or ~~strikethrough~~. [Links](https://github.com) should be blue with no underlines (unless hovered over).\n\n{:.bg-yellow-dark.text-white.p-5.mb-6}\nText can be **bold**, _italic_, or ~~strikethrough~~. [Links](https://github.com) should be blue with no underlines (unless hovered over).\n\n{:.bg-yellow-dark.text-white.p-5.mb-6}\nText can be **bold**{:.h1}, _italic_, or ~~strikethrough~~. [Links](https://github.com) should be blue with no underlines (unless hovered over).\n\n{:.bg-yellow-dark.text-white.p-2.box-shadow-large}\nText can be **bold**{:.h1}, _italic_, or ~~strikethrough~~. [Links](https://github.com) should be blue with no underlines (unless hovered over).\n\n{:.bg-yellow-dark.text-white.p-5.box-shadow-large.anim-pulse}\nText can be **bold**{:.h1}, _italic_, or ~~strikethrough~~. [Links](https://github.com) should be blue with no underlines (unless hovered over).\n\n```tip\nEdit this page to see how to add this to your docs, theme can use [@primer/css utilities](https://primer.style/css/utilities)\n```\n","dir":"/writing/test/","name":"utilities.md","path":"writing/test/utilities.md","url":"/writing/test/utilities.html"},{"sort":11,"layout":"default","title":"Cytoscape","content":"<h1 id=\"cytoscape\">Cytoscape</h1>\n\n<p>Web: <a href=\"https://cytoscape.org/\">https://cytoscape.org/</a>.</p>\n\n<p>GitHub: <a href=\"https://github.com/cytoscape/cytoscape\">https://github.com/cytoscape/cytoscape</a>.</p>\n\n<h2 id=\"installation\">Installation</h2>\n\n<p>To start, we experiment with the latest version.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">export </span><span class=\"nv\">HPC_WORK</span><span class=\"o\">=</span>/rds/user/<span class=\"k\">${</span><span class=\"nv\">USER</span><span class=\"k\">}</span>/hpc-work\n<span class=\"nb\">cd</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>\nwget https://github.com/cytoscape/cytoscape/releases/download/3.9.0/Cytoscape_3_9_0_unix.sh\nmodule load openjdk-11.0.2-gcc-5.4.0-3dxltae\nbash Cytoscape_3_9_0_unix.sh\n<span class=\"nb\">cd </span>bin\n<span class=\"nb\">ln</span> <span class=\"nt\">-s</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>/Cytoscape_v3.9.0/cytoscape.sh\ncytoscape.sh\n</code></pre> </div></div>\n\n<p>To press <code class=\"language-plaintext highlighter-rouge\">&lt;tab&gt;</code> for a list of commands and press <code class=\"language-plaintext highlighter-rouge\">&lt;ctrl-d&gt;</code> or <code class=\"language-plaintext highlighter-rouge\">osgi:shutdown</code> to quit the session.</p>\n\n<p>There might be message such as <code class=\"language-plaintext highlighter-rouge\">karaf: There is a Root instance already running with name Cytoscape 3.9.0 and pid 78872</code>, then simply,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">export </span><span class=\"nv\">CHECK_ROOT_INSTANCE_RUNNING</span><span class=\"o\">=</span><span class=\"nb\">false\n</span>cytoscape.sh\n</code></pre> </div></div>\n\n<h2 id=\"rcy3\">RCy3</h2>\n\n<p>Web: <a href=\"https://cytoscape.org/RCy3/\">https://cytoscape.org/RCy3/</a>.</p>\n\n<p>Bioconductor: <a href=\"https://bioconductor.org/packages/release/bioc/html/RCy3.html\">https://bioconductor.org/packages/release/bioc/html/RCy3.html</a>.</p>\n\n<p>We can first start <code class=\"language-plaintext highlighter-rouge\">RStudio</code> and then <code class=\"language-plaintext highlighter-rouge\">Cytoscape</code>, so that the R session detects the Cytoscape session. In the following script, the <code class=\"language-plaintext highlighter-rouge\">enhancedGraphics</code> and <code class=\"language-plaintext highlighter-rouge\">STRINGapp</code> apps are also installed.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">if</span> <span class=\"o\">(!</span><span class=\"s2\">\"RCy3\"</span> %in% installed.packages<span class=\"o\">())</span>\n<span class=\"o\">{</span>\n  install.packages<span class=\"o\">(</span><span class=\"s2\">\"BiocManager\"</span><span class=\"o\">)</span>\n  BiocManager::install<span class=\"o\">(</span><span class=\"s2\">\"RCy3\"</span><span class=\"o\">)</span>\n<span class=\"o\">}</span>\nlibrary<span class=\"o\">(</span>RCy3<span class=\"o\">)</span>\ncytoscapePing<span class=\"o\">()</span>\n<span class=\"c\"># You are connected to Cytoscape!</span>\ncytoscapeVersionInfo<span class=\"o\">()</span>\n<span class=\"c\">#     apiVersion cytoscapeVersion</span>\n<span class=\"c\">#           \"v1\"          \"3.9.0\"</span>\nbrowseVignettes<span class=\"o\">(</span><span class=\"s2\">\"RCy3\"</span><span class=\"o\">)</span>\n<span class=\"c\"># from the first vignette</span>\nnodes &lt;- data.frame<span class=\"o\">(</span><span class=\"nb\">id</span><span class=\"o\">=</span>c<span class=\"o\">(</span><span class=\"s2\">\"node 0\"</span>,<span class=\"s2\">\"node 1\"</span>,<span class=\"s2\">\"node 2\"</span>,<span class=\"s2\">\"node 3\"</span><span class=\"o\">)</span>,\n           <span class=\"nv\">group</span><span class=\"o\">=</span>c<span class=\"o\">(</span><span class=\"s2\">\"A\"</span>,<span class=\"s2\">\"A\"</span>,<span class=\"s2\">\"B\"</span>,<span class=\"s2\">\"B\"</span><span class=\"o\">)</span>, <span class=\"c\"># categorical strings</span>\n           <span class=\"nv\">score</span><span class=\"o\">=</span>as.integer<span class=\"o\">(</span>c<span class=\"o\">(</span>20,10,15,5<span class=\"o\">))</span>, <span class=\"c\"># integers</span>\n           <span class=\"nv\">stringsAsFactors</span><span class=\"o\">=</span>FALSE<span class=\"o\">)</span>\nedges &lt;- data.frame<span class=\"o\">(</span><span class=\"nb\">source</span><span class=\"o\">=</span>c<span class=\"o\">(</span><span class=\"s2\">\"node 0\"</span>,<span class=\"s2\">\"node 0\"</span>,<span class=\"s2\">\"node 0\"</span>,<span class=\"s2\">\"node 2\"</span><span class=\"o\">)</span>,\n           <span class=\"nv\">target</span><span class=\"o\">=</span>c<span class=\"o\">(</span><span class=\"s2\">\"node 1\"</span>,<span class=\"s2\">\"node 2\"</span>,<span class=\"s2\">\"node 3\"</span>,<span class=\"s2\">\"node 3\"</span><span class=\"o\">)</span>,\n           <span class=\"nv\">interaction</span><span class=\"o\">=</span>c<span class=\"o\">(</span><span class=\"s2\">\"inhibits\"</span>,<span class=\"s2\">\"interacts\"</span>,<span class=\"s2\">\"activates\"</span>,<span class=\"s2\">\"interacts\"</span><span class=\"o\">)</span>,  <span class=\"c\"># optional</span>\n           <span class=\"nv\">weight</span><span class=\"o\">=</span>c<span class=\"o\">(</span>5.1,3.0,5.2,9.9<span class=\"o\">)</span>, <span class=\"c\"># numeric</span>\n           <span class=\"nv\">stringsAsFactors</span><span class=\"o\">=</span>FALSE<span class=\"o\">)</span>\ncreateNetworkFromDataFrames<span class=\"o\">(</span>nodes,edges, <span class=\"nv\">title</span><span class=\"o\">=</span><span class=\"s2\">\"my first network\"</span>, <span class=\"nv\">collection</span><span class=\"o\">=</span><span class=\"s2\">\"DataFrame Example\"</span><span class=\"o\">)</span>\ninstallApp<span class=\"o\">(</span><span class=\"s2\">\"enhancedGraphics\"</span><span class=\"o\">)</span>\ninstallApp<span class=\"o\">(</span><span class=\"s1\">'STRINGapp'</span><span class=\"o\">)</span>\n</code></pre> </div></div>\n\n<p>It is also possible to first start <code class=\"language-plaintext highlighter-rouge\">cytoscape.sh</code> and then <code class=\"language-plaintext highlighter-rouge\">R</code> command-line interface which is less resource-demanding compared to <code class=\"language-plaintext highlighter-rouge\">RStudio</code>.</p>\n\n<h3 id=\"references\">References</h3>\n\n<p>Gustavsen, J.A., Pai, S., Isserlin, R., Demchak, B. &amp; Pico, A.R. RCy3: Network biology using Cytoscape from within R. F1000Research 8, 1774:1-21 (2019).</p>\n\n<p>Ideker, T. et al. Integrated genomic and proteomic analyses of a systematically perturbed metabolic network. Science 292, 929-34 (2001).</p>\n","dir":"/applications/","name":"Cytoscape.md","path":"applications/Cytoscape.md","url":"/applications/Cytoscape.html"},{"sort":11,"layout":"default","title":"R/Rfast","content":"<h1 id=\"rrfast\">R/Rfast</h1>\n\n<p>For 2.0.4, we received the following error,</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Error: C++17 standard requested but CXX17 is not defined\n</code></pre> </div></div>\n\n<p>To proceed, we modify ~/.R/Makevars with the following line</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>CXX17 = g++ -std=gnu++17 -fPIC\n</code></pre> </div></div>\n\n<p>We can get away with this,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>module load gcc/7\nRscript <span class=\"nt\">-e</span> <span class=\"s2\">\"install.packages('Rfast')\"</span>\n</code></pre> </div></div>\n","dir":"/applications/R/","name":"Rfast.md","path":"applications/R/Rfast.md","url":"/applications/R/Rfast.html"},{"sort":12,"layout":"default","title":"DosageConverter","content":"<h1 id=\"dosageconverter\">DosageConverter</h1>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">## assuming you use hpc-work/ with a subdirectory called bin/</span>\n<span class=\"nb\">cd</span> /rds/user/<span class=\"nv\">$USER</span>/hpc-work/\ngit clone https://github.com/Santy-8128/DosageConvertor\n<span class=\"nb\">cd </span>DosageConverter\npip <span class=\"nb\">install </span>cget <span class=\"nt\">--user</span>\nmodule load cmake-3.8.1-gcc-4.8.5-zz55m7x\n./install.sh\n<span class=\"nb\">cd</span> /rds/user/<span class=\"nv\">$USER</span>/hpc-work/bin/\n<span class=\"nb\">ln</span> <span class=\"nt\">-s</span> /rds/user/<span class=\"nv\">$USER</span>/hpc-work/DosageConvertor/release-build/DosageConvertor\n<span class=\"c\">## testing</span>\nDosageConvertor  <span class=\"nt\">--vcfDose</span>  <span class=\"nb\">test</span>/TestDataImputedVCF.dose.vcf.gz <span class=\"se\">\\</span>\n                 <span class=\"nt\">--info</span>     <span class=\"nb\">test</span>/TestDataImputedVCF.info <span class=\"se\">\\</span>\n                 <span class=\"nt\">--prefix</span>   <span class=\"nb\">test</span> <span class=\"se\">\\</span>\n                 <span class=\"nt\">--type</span>     mach\n\n<span class=\"nb\">gunzip</span> <span class=\"nt\">-c</span> test.mach.dose.gz | <span class=\"nb\">wc</span> <span class=\"nt\">-l</span>\n\nDosageConvertor  <span class=\"nt\">--vcfDose</span>  <span class=\"nb\">test</span>/TestDataImputedVCF.dose.vcf.gz <span class=\"se\">\\</span>\n                 <span class=\"nt\">--info</span>     <span class=\"nb\">test</span>/TestDataImputedVCF.info <span class=\"se\">\\</span>\n                 <span class=\"nt\">--prefix</span>   <span class=\"nb\">test</span> <span class=\"se\">\\</span>\n                 <span class=\"nt\">--type</span>     plink\n\n<span class=\"nb\">gunzip</span> <span class=\"nt\">-c</span> test.plink.dosage.gz | <span class=\"nb\">wc</span> <span class=\"nt\">-l</span>\n</code></pre> </div></div>\n\n<p>so the MaCH dosage file is individual x genotype whereas PLINK dosage file is genotype x individual.</p>\n","dir":"/applications/","name":"DosageConverter.md","path":"applications/DosageConverter.md","url":"/applications/DosageConverter.html"},{"sort":12,"layout":"default","title":"R/rgdal","content":"# R/rgdal\n\nWe have .R/Makevars as follows,\n\n```bash\nCC=gcc\nCXX=g++ -std=gnu++11\nPKG_CXXFLAGS= -std=c++11\nCFLAGS = -std=c99 -I/usr/include -g -O2 -Wall -pedantic -mtune=native -Wno-ignored-attributes -Wno-deprecated-declarations -Wno-parentheses -Wimplicit-function-declaration\nCXXFLAGS = -std=c++11\n```\n\nthe proceed with\n\n```bash\nmodule load gcc/5\nmodule load geos-3.6.2-gcc-5.4.0-vejexvy\nmodule load gdal-2.3.1-gcc-5.4.0-m7j7nw6\nmodule load proj-5.0.1-gcc-5.4.0-cpqxtzr\nRscript -e \"install.packages('sf')\"\n# R 3.6.3 also requires\nmodule load json-c-0.13.1-gcc-5.4.0-ffamohj\nmodule load libgeotiff-1.4.2-gcc-5.4.0-2emzhxh\nmodule load libpng-1.6.29-gcc-5.4.0-3qwhidp\nmodule load cfitsio-3.450-gcc-5.4.0-colpo6h\nmodule load zlib/1.2.11\nmodule load mpich-3.2-gcc-5.4.0-idlluti\nRscript -e \"install.packages('rgdal')\"\n```\n\nUnder R 3.6.3, there are complaints about `-std=c++11` when installing `sf` but can be dealt with it first (as described in its own section).\n\nFinally, gdal could also be installed with proj 6 available,\n\n```bash\nmodule load geos-3.6.2-gcc-5.4.0-vejexvy\n./configure --with-proj=$HPC_WORK --without-sqlite3 --prefix=$HPC_WORK\n```\n\nThen `proj_api.h` should have a statement\n\n```c\n#define ACCEPT_USE_OF_DEPRECATED_PROJ_API_H 1\n```\n","dir":"/applications/R/","name":"rgdal.md","path":"applications/R/rgdal.md","url":"/applications/R/rgdal.html"},{"sort":13,"layout":"default","title":"R/rgeos","content":"# R/rgeos\n\nThis requires geos to be loaded,\n\n```bash\nmodule load geos-3.6.2-gcc-5.4.0-vejexvy\n```\n","dir":"/applications/R/","name":"rgeos.md","path":"applications/R/rgeos.md","url":"/applications/R/rgeos.html"},{"sort":13,"layout":"default","title":"GCTA","content":"# GCTA\n\nThree components will be covered: direct download of the executable file, compiling it from source and running the documentation example.\n\n## Executable\n\nWeb: [https://yanglab.westlake.edu.cn/software/gcta](https://yanglab.westlake.edu.cn/software/gcta)\n\nThe setup is quite straitforward.\n\n```bash\ncd ${HPC_WORK}\nwget https://yanglab.westlake.edu.cn/software/gcta/bin/gcta_1.93.3beta2.zip\nunzip gcta_1.93.3beta2.zip\ncd gcta_1.93.3beta2/\nln -sf ${HPC_WORK}/gcta_1.93.3beta2/gcta64 ${HPC_WORK}/bin/gcta-1.9\n```\n\n## Source\n\nDistribution with the paper: [https://zenodo.org/record/5226943/files/jianyangqt/gcta-v1.93.3beta2.zip](https://zenodo.org/record/5226943/files/jianyangqt/gcta-v1.93.3beta2.zip).\n\nGitHub: [https://github.com/jianyangqt/gcta](https://github.com/jianyangqt/gcta).\n\nNote that they work with a specific version of plink-ng from [https://github.com/zhilizheng/plink-ng](https://github.com/zhilizheng/plink-ng). With GitHub, we could track any change(s) we made with `git diff`.\n\nWe proceed as follows,\n\n```bash\nmodule load eigen/3.3.7\nmodule load intel/mkl/2017.8\nmodule load spectra/0.8.1\nexport EIGEN3_INCLUDE_DIR=/usr/local/software/master/eigen/latest/include\nexport BOOST_LIB=/usr/local/Cluster-Apps/boost/1.65.1/python3.5.1-gcc5.3.0/\nexport SPECTRA_LIB=/usr/local/Cluster-Apps/spectra/0.8.1/spectra-0.8.1\ngit clone https://github.com/jianyangqt/gcta\ncd gcta\ncd submods\ngit clone https://github.com/zhilizheng/plink-ng\ncd ..\nmkdir build && cd build\ncmake ..\nmake\n```\n\nIt requires specification of `/usr/local/Cluster-Apps/spectra/0.8.1/include/Spectra/` in `FastFAM.cpp` and `Geno.cpp`. We also get complaints about -lzstd and but could get around with adding -L${HPC_WORK}/lib to `CMakeFiles//gcta64.dir/link.txt` and then `bash CMakeFiles//gcta64.dir/link.txt` which gives the much-desired `gcta64`.\n\n## Documentation example\n\nWe use the documentation example to illutrate a linear mixed model (LMM).\n\n```bash\ngcta-1.9 --bfile test --make-grm --out test\ngcta-1.9 --grm test --reml --pheno test.phen --out test\n```\n\nWhere the first statement generates the genomic relationship matrix (GRM) followed by the second statement for the LMM via restricted maximum likelihood (REML). The screen outputs are given here as well,\n\n```\n*******************************************************************\n* Genome-wide Complex Trait Analysis (GCTA)\n* version 1.93.3 beta Linux\n* (C) 2010-present, Jian Yang, The University of Queensland\n* Please report bugs to Jian Yang <jian.yang.qt@gmail.com>\n*******************************************************************\nAnalysis started at 10:53:18 GMT on Wed Nov 17 2021.\nHostname: login-e-16\n\nOptions:\n\n--bfile test\n--make-grm\n--out test\n\nThe program will be running on up to 1 threads.\nNote: GRM is computed using the SNPs on the autosome.\nReading PLINK FAM file from [test.fam]...\n3925 individuals to be included from FAM file.\n3925 individuals to be included. 1643 males, 2282 females, 0 unknown.\nReading PLINK BIM file from [test.bim]...\n1000 SNPs to be included from BIM file(s).\nComputing the genetic relationship matrix (GRM) v2 ...\nSubset 1/1, no. subject 1-3925\n  3925 samples, 1000 markers, 7704775 GRM elements\nIDs for the GRM file has been saved in the file [test.grm.id]\nComputing GRM...\n  100% finished in 0.6 sec\n1000 SNPs have been processed.\n  Used 1000 valid SNPs.\nThe GRM computation is completed.\nSaving GRM...\nGRM has been saved in the file [test.grm.bin]\nNumber of SNPs in each pair of individuals has been saved in the file [test.grm.N.bin]\n```\n\nand\n\n```\n*******************************************************************\n* Genome-wide Complex Trait Analysis (GCTA)\n* version 1.93.3 beta Linux\n* (C) 2010-present, Jian Yang, The University of Queensland\n* Please report bugs to Jian Yang <jian.yang.qt@gmail.com>\n*******************************************************************\nAnalysis started at 10:55:35 GMT on Wed Nov 17 2021.\nHostname: login-e-16\n\nAccepted options:\n--grm test\n--reml\n--pheno test.phen\n--out test\n\nNote: This is a multi-thread program. You could specify the number of threads by the --thread-num option to speed up the computation if there are multiple processors in your machine.\n\nReading IDs of the GRM from [test.grm.id].\n3925 IDs read from [test.grm.id].\nReading the GRM from [test.grm.bin].\nGRM for 3925 individuals are included from [test.grm.bin].\nReading phenotypes from [test.phen].\nNon-missing phenotypes of 3925 individuals are included from [test.phen].\n\n3925 individuals are in common in these files.\n\nPerforming  REML analysis ... (Note: may take hours depending on sample size).\n3925 observations, 1 fixed effect(s), and 2 variance component(s)(including residual variance).\nCalculating prior values of variance components by EM-REML ...\nUpdated prior values: 0.462455 0.889944\nlogL: -2529.14\nRunning AI-REML algorithm ...\nIter.   logL    V(G)    V(e)\n1       -2074.82        0.02389 0.91928\n2       -1945.84        0.02343 0.93419\n3       -1944.49        0.02309 0.94480\n4       -1943.84        0.02283 0.95228\n5       -1943.53        0.02223 0.96878\n6       -1943.25        0.02216 0.96911\n7       -1943.24        0.02215 0.96912\n8       -1943.24        0.02215 0.96912\nLog-likelihood ratio converged.\n\nCalculating the logLikelihood for the reduced model ...\n(variance component 1 is dropped from the model)\nCalculating prior values of variance components by EM-REML ...\nUpdated prior values: 0.99065\nlogL: -1947.71500\nRunning AI-REML algorithm ...\nIter.   logL    V(e)\n1       -1947.71        0.99065\nLog-likelihood ratio converged.\n\nSummary result of REML analysis:\nSource  Variance        SE\nV(G)    0.022152        0.008751\nV(e)    0.969117        0.022896\nVp      0.991269        0.022467\nV(G)/Vp 0.022347        0.008769\n\nSampling variance/covariance of the estimates of variance components:\n7.657779e-05    -4.800941e-05\n-4.800941e-05   5.242193e-04\n\nSummary result of REML analysis has been saved in the file [test.hsq].\n```\n\nTherefore the documentation example provides a heritability estimate of 0.223 (0.00877).\n\nIt offers alternative to regenie on this site according to the associate paper below.\n\n## Reference\n\nJiang, L., Zheng, Z., Fang, H. & Yang, J. A generalized linear mixed model association tool for biobank-scale data. Nature Genetics 53, 1616-1621 (2021).\n","dir":"/applications/","name":"gcta.md","path":"applications/gcta.md","url":"/applications/gcta.html"},{"sort":14,"layout":"default","title":"R/Rhdf5lib","content":"<h1 id=\"rrhdf5lib\">R/Rhdf5lib</h1>\n\n<p>This is useful for hdf5 file handling, but BiocManager::install() gives error <code class=\"language-plaintext highlighter-rouge\">cp: cannot stat ‘hdf5/c++/src/.libs/libhdf5_cpp.a’: No such file or directory </code> so we proceed manually,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>module load gcc/6\n<span class=\"nb\">cd</span> <span class=\"nv\">$HOME</span>\nwget https://bioconductor.org/packages/3.11/bioc/src/contrib/Rhdf5lib_1.10.1.tar.gz\n<span class=\"nb\">tar </span>xfz Rhdf5lib_1.10.1.tar.gz\n<span class=\"nb\">cd </span>Rhdf5lib\n./configure\n<span class=\"nb\">mv </span>configure configure.sav\n<span class=\"nb\">cd </span>src\n<span class=\"nb\">tar </span>xfz hdf5small_cxx_hl_1.10.6.tar.gz\n<span class=\"nb\">cd </span>hdf5\n./configure <span class=\"nt\">--prefix</span><span class=\"o\">=</span><span class=\"nv\">$HPC_WORK</span> <span class=\"nt\">--enable-build-all</span> <span class=\"nt\">--enable-cxx</span> <span class=\"nv\">CFLAGS</span><span class=\"o\">=</span><span class=\"nt\">-fPIC</span>\nmake\nmake <span class=\"nb\">install\nmv </span>configure configure.sav\n<span class=\"nb\">cd</span> <span class=\"nv\">$HOME</span>\nR CMD INSTALL Rhdf5lib\n</code></pre> </div></div>\n","dir":"/applications/R/","name":"Rhdf5lib.md","path":"applications/R/Rhdf5lib.md","url":"/applications/R/Rhdf5lib.html"},{"sort":14,"layout":"default","title":"gsutil","content":"# gsutil\n\n## Installation\n\nWeb site: [https://cloud.google.com/storage/docs/gsutil_install#linux](https://cloud.google.com/storage/docs/gsutil_install#linux)\n\nthe authentification is achieved via Google SDK (e.g. [301.0.0](https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-301.0.0-linux-x86_64.tar.gz))\n\n```bash\nwget https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-301.0.0-linux-x86_64.tar.gz\ntar xfz google-cloud-sdk-301.0.0-linux-x86_64.tar.gz\ncd google-cloud-sdk\n./install.sh\n# to gain a bit flexibility by not using the awkward browser (Konqueror) at CSD3\ngcloud auto login --no-launch-browser\n# to set PROJECT_ID=divine-aegis-278909\ngcloud config set divine-aegis-278909\n```\n\nfollowed by `gsutil config` where `gsutil` is installed via `virtualenv` as follows,\n\n```bash\nmodule load python/3.7\nvirtualenv py37\nsource py37/bin/activate\npip install gsutil==4.50\ncd ${dir}/py37/bin\ngsutil ls\n# gsutil cp test gs::/covid19-hg-upload-bugbank\ngsutil cp test gs://covid19-hg-upload-uk--blood-donors-cohort\n```\n\nLess useful is the usual way to install.\n\n```bash\nwget https://storage.googleapis.com/pub/gsutil.tar.gz\ntar xvfz gsutil.tar.gz -C ..\ncd ../gsutil\npip install pyasn1==0.4.8  --user\npython setup.py install --prefix=$HPC_WORK\n```\n\n## Example\n\nThe GTEx v8 QTL resources,\n\n```bash\ngsutil -m cp -r \\\n  \"gs://gtex-resources/GTEx_Analysis_v8_QTLs/\" \\\n  .\n```\n\nor Europeans individually,\n\n```bash\ngsutil -m cp -r \\\n  \"gs://gtex-resources/GTEx_Analysis_v8_QTLs/GTEx_Analysis_v8_EUR_eQTL_all_associations/\" \\\n  \"gs://gtex-resources/GTEx_Analysis_v8_QTLs/GTEx_Analysis_v8_EUR_sQTL/\" \\\n  \"gs://gtex-resources/GTEx_Analysis_v8_QTLs/GTEx_Analysis_v8_EUR_sQTL_all_associations/\" \\\n  \"gs://gtex-resources/GTEx_Analysis_v8_QTLs/GTEx_Analysis_v8_eQTL_all_associations/\" \\\n  \"gs://gtex-resources/GTEx_Analysis_v8_QTLs/GTEx_Analysis_v8_sQTL/\" \\\n  \"gs://gtex-resources/GTEx_Analysis_v8_QTLs/GTEx_Analysis_v8_sQTL_all_associations/\" \\\n  \"gs://gtex-resources/GTEx_Analysis_v8_QTLs/GTEx_Analysis_v8_sQTL_leafcutter_counts.tar\" \\\n  .\n```\n\nas described here, [https://cloud.google.com/storage/docs/downloading-objects](https://cloud.google.com/storage/docs/downloading-objects).\n","dir":"/applications/","name":"gsutil.md","path":"applications/gsutil.md","url":"/applications/gsutil.html"},{"sort":15,"layout":"default","title":"R/rjags","content":"# R/rjags\n\nWeb page: [https://sourceforge.net/projects/mcmc-jags/files/rjags/](https://sourceforge.net/projects/mcmc-jags/files/rjags/).\n\nIt is known for sometime for its difficulty to install; here is what was done\n\n```bash\n\n# Cardio\nexport PKG_CONFIG_PATH=/scratch/$USER/lib/pkgconfig\n\nR CMD INSTALL rjags_4-6.tar.gz --configure-args='CPPFLAGS=\"-fPIC\" LDFLAGS=\"-L/scratch/$USER/lib -ljags\"\n--with-jags-prefix=/scratch/$USER\n--with-jags-libdir=/scratch/$USER/lib\n--with-jags-includedir=/scratch/$USER/include'\n\n# csd3\nexport hpcwork=/rds-d4/user/$USER/hpc-work\nexport PKG_CONFIG_PATH=${hpcwork}/lib/pkgconfig\n\nwget https://cran.r-project.org/src/contrib/rjags_4-10.tar.gz\nR CMD INSTALL rjags_4-10.tar.gz --configure-args='CPPFLAGS=\"-fPIC\" LDFLAGS=\"-L${hpcwork}/lib -ljags\"\n--with-jags-prefix=${hpcwork}\n--with-jags-libdir=${hpcwork}/lib\n--with-jags-includedir=${hpcwork}/include'\n```\n\nAs is with the module `jags-4.3.0-gcc-5.4.0-4z5shby`, we would see this error message from R 4.1.0,\n\n```\n** testing if installed package can be loaded from temporary location\nError: package or namespace load failed for ‘rjags’:\n .onLoad failed in loadNamespace() for 'rjags', details:\n  call: dyn.load(file, DLLpath = DLLpath, ...)\n  error: unable to load shared object '/rds/user/jhz22/hpc-work/R/00LOCK-rjags/00new/rjags/libs/rjags.so':\n  /rds/user/jhz22/hpc-work/R/00LOCK-rjags/00new/rjags/libs/rjags.so: undefined symbol: _ZN4jags7Console10setRNGnameERKNSt7__cxx1112basic_stringIcSt11char_t$\nError: loading failed\nExecution halted\nERROR: loading failed\n```\n\nthen this is due to different versions of compilers were used to build JAGS and rjags, so the former needs to be rebuilt.\n","dir":"/applications/R/","name":"rjags.md","path":"applications/R/rjags.md","url":"/applications/R/rjags.html"},{"sort":15,"layout":"default","title":"gwas2vcf","content":"# gwas2vcf\n\nWeb: https://github.com/MRCIEU/gwas2vcf\n\nThe required setup script is as follows,\n\n```bash\nmodule load jdk-8u141-b15-gcc-5.4.0-p4aaopt\nmodule load gatk\nmodule load python/3.7\ngit clone https://github.com/MRCIEU/gwas2vcf\ncd gwas2vcf\npython -m venv env\nsource env/bin/activate\npip install -r requirements.txt\npip install git+git://github.com/bioinformed/vgraph@v1.4.0#egg=vgraph\nln -s ~/rds/public_databases/GRCh37_reference_fasta/human_g1k_v37.fasta\nln -s ~/rds/public_databases/GRCh37_reference_fasta/human_g1k_v37.fasta.fai\npython -m pytest -v test\npython main.py -h\n\n## these are not necessarily required and GRC37 is here, ~/rds/public_databases/GRCh37_reference_fasta/\n# GRCh36/hg18/b36\nwget http://fileserve.mrcieu.ac.uk/ref/2.8/b36/human_b36_both.fasta\nwget http://fileserve.mrcieu.ac.uk/ref/2.8/b36/human_b36_both.fasta.fai\n\n# GRCh37/hg19/b37\nwget http://fileserve.mrcieu.ac.uk/ref/2.8/b37/human_g1k_v37.fasta\nwget http://fileserve.mrcieu.ac.uk/ref/2.8/b37/human_g1k_v37.fasta.fai\n\n# GRCh38/hg38/b38\nwget https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\nwget https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta.fai\n```\n","dir":"/applications/","name":"gwas2vcf.md","path":"applications/gwas2vcf.md","url":"/applications/gwas2vcf.html"},{"sort":16,"layout":"default","title":"R/rJava","content":"# R/rJava\n\nOne may see the messages\n\n```\n...\nchecking whether JNI programs can be compiled... configure: error: Cannot compile a simple JNI program. See config.log for details.\n...\nERROR: configuration failed for package ‘rJava’\n```\n\nso quit R and run\n\n```bash\nR CMD javareconf\nRscript -e 'install.packages(\"rJava\")'\n```\n","dir":"/applications/R/","name":"rJava.md","path":"applications/R/rJava.md","url":"/applications/R/rJava.html"},{"sort":16,"layout":"default","title":"hail","content":"# hail\n\nWeb: [https://hail.is/](https://hail.is/) as with [hail on cloud](https://github.com/danking/hail-cloud-docs/blob/master/how-to-cloud.md).\n\n## Preparations\n\nWe will need to set up an virtual environment as follows,\n\n```bash\nmodule load python/3.7 hadoop/2.7.7\nvirtualenv py37\nsource py37/bin/activate\n```\n\nLater, only two of the commands are necessary, i.e.,\n\n```bash\nmodule load python/3.7 hadoop/2.7.7\nsource py37/bin/activate\n```\n\n## Installation\n\nWe proceed with,\n\n```bash\npip install hail\npip install gnomAD\npip install pyensembl\npip install varcode\npyensembl install --release 99 --species human\n```\n\nwhere `gnomAD`, `pyensembl` ([https://github.com/openvax/pyensembl](https://github.com/openvax/pyensembl)) and `varcode` ([https://github.com/openvax/varcode](https://github.com/openvax/varcode)) are optional, and we could invoke Python as follows,\n\n```python\npython\n>>> import hail as hl\n>>> mt = hl.balding_nichols_model(n_populations=3, n_samples=50, n_variants=100)\n>>> mt.describe()\n>>> mt.summarize()\n>>> mt.count()\n>>> mt.show()\n>>> mt.write(\"hail\")\n>>> at = hl.import_vcf(\"INTERVAL.vcf.bgz\")\n>>> hl.export_vcf(at,\"at.vcf.bgz\")\n```\n\nAlternatively, we could also start a session using `ipython`.\n\n## Tutorials\n\nWe can get the tutorials at the current directory,\n\n```bash\nwget -qO- https://hail.is/docs/0.2/tutorials.tar.gz | tar xf -\n```\n\nand cut/paste code from the hail website to our command-line session. A better quality can be achieved with a browser, which unfortunately does not function so well on csd3 and requires some tweak from elsewhere,\n\nFirst, from a csd3 session we issue commands,\n\n```bash\nhostname\njupyter notebook tutorials/ --ip=127.0.0.1 --no-browser --port 8081\n```\n\nand for this instance we have `hostname` as `login-e-12` and additional information as follows,\n\n```\n[I 20:21:59.258 NotebookApp] Serving notebooks from local directory: /rds/project/jmmh2/rds-jmmh2-results/public/gwas/ukb_exomes/tutorials\n[I 20:21:59.258 NotebookApp] The Jupyter Notebook is running at:\n[I 20:21:59.258 NotebookApp] http://127.0.0.1:8081/?token=2d4ba02d3d0d782ad1dbafc73f96a3fc3d0bdff4573899c9\n[I 20:21:59.258 NotebookApp]  or http://127.0.0.1:8081/?token=2d4ba02d3d0d782ad1dbafc73f96a3fc3d0bdff4573899c9\n[I 20:21:59.258 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 20:21:59.300 NotebookApp]\n\n    To access the notebook, open this file in a browser:\n        file:///home/jhz22/.local/share/jupyter/runtime/nbserver-235186-open.html\n    Or copy and paste one of these URLs:\n        http://127.0.0.1:8081/?token=2d4ba02d3d0d782ad1dbafc73f96a3fc3d0bdff4573899c9\n     or http://127.0.0.1:8081/?token=2d4ba02d3d0d782ad1dbafc73f96a3fc3d0bdff4573899c9\n```\n\nfrom a console other than a csd3 session (e.g., from a `srcf` or `ds` or Windows `virtualbox` virtual machine) we access the csd3 session above.\n\n```bash\nssh -4 -L 8081:127.0.0.1:8081 -fN login-e-12.hpc.cam.ac.uk\nfirefox http://127.0.0.1:8081/?token=2d4ba02d3d0d782ad1dbafc73f96a3fc3d0bdff4573899c9\n```\n\nand run the tutorial from firefox. Similar process is also described at the system/software section on Python.\n\nFor convenience, we list the code from the tutorials on 1000Genomes data, see https://notebook.community/danking/hail/notebook/images/hail/resources/Hail-Workshop-Notebook.\n\n````python\nimport hail as hl\nimport os\ndir=os.environ['HPC_WORK']+\"/lib64/\"\nfiles=dir+\"libblas.so:\"+dir+\"libcblas.so:\"+dir+\"liblapack.so\"\nprint(files)\n# os.environ['PYENSEMBL_CACHE_DIR'] = '/custom/cache/dir'\nhl.init(spark_conf={\"spark.executor.extraClassPath\": files})\n# Gene information\ngene_ht = hl.import_table('tutorials/data/ensembl_gene_annotations.txt', impute=True)\ngene_ht.show()\ngene_ht.count()\ngene_ht = gene_ht.transmute(interval = hl.locus_interval(gene_ht['Chromosome'],\n                                                         gene_ht['Gene start'],\n                                                         gene_ht['Gene end'],\n                                                         reference_genome='GRCh37'))\ngene_ht = gene_ht.key_by('interval')\n# 1000Genomes data from the tutorials\nsa = hl.import_table('tutorials/data/1kg_annotations.txt', impute=True, key='Sample')\nsa.describe()\nsa.show()\nmt = hl.read_matrix_table('tutorials/data/1kg.mt')\nmt = mt.annotate_rows(gene_info = gene_ht[mt.locus])\nmt.s.show(5)\nmt.locus.show(5)\nhl.summarize_variants(mt)\nmt = mt.annotate_cols(pheno = sa[mt.s])\nmt = hl.sample_qc(mt)\nmt.sample_qc.describe()\np = hl.plot.scatter(x=mt.sample_qc.r_het_hom_var,\n                    y=mt.sample_qc.call_rate)\nshow(p)\nmt = mt.filter_cols(mt.sample_qc.dp_stats.mean >= 4)\nmt = mt.filter_cols(mt.sample_qc.call_rate >= 0.97)\nmt.aggregate_entries(hl.agg.fraction(hl.is_defined(mt.GT)))\nab = mt.AD[1] / hl.sum(mt.AD)\nfilter_condition_ab = (\n    hl.case()\n    .when(mt.GT.is_hom_ref(), ab <= 0.1)\n    .when(mt.GT.is_het(), (ab >= 0.25) & (ab <= 0.75))\n    .default(ab >= 0.9) # hom-var\n)\nmt = mt.filter_entries(filter_condition_ab)\nmt.aggregate_entries(hl.agg.fraction(hl.is_defined(mt.GT)))\nmt = hl.variant_qc(mt)\nmt.variant_qc.describe()\nmt.variant_qc.AF.show()\nmt = mt.filter_rows(hl.min(mt.variant_qc.AF) > 1e-6)\nmt = mt.filter_rows(mt.variant_qc.p_value_hwe > 0.005)\nmt = mt.annotate_rows(gene_info = gene_ht[mt.locus])\nmt.gene_info.show()\nburden_mt = (\n    mt\n    .group_rows_by(gene = mt.gene_info['Gene name'])\n    .aggregate(n_variants = hl.agg.count_where(mt.GT.n_alt_alleles() > 0))\n)\nburden_mt.describe()\npca_eigenvalues, pca_scores, pca_loadings = hl.hwe_normalized_pca(mt.GT, compute_loadings=True)\npca_eigenvalues\npca_scores.describe()\npca_scores.scores[0].show()\npca_loadings.describe()\nmt = mt.annotate_cols(pca = pca_scores[mt.s])\np = hl.plot.scatter(mt.pca.scores[0],\n                    mt.pca.scores[1],\n                    label=mt.pheno.SuperPopulation)\nshow(p)\n# errors from here though hl.init() above\n# https://discuss.hail.is/t/undefined-symbol-cblas-dgemm/1488\ngwas = hl.linear_regression_rows(y=mt.pheno.CaffeineConsumption,\n                                 x=mt.GT.n_alt_alleles(),\n                                 covariates=[1.0])\ngwas.describe()\np = hl.plot.manhattan(gwas.p_value)\nshow(p)\np = hl.plot.qq(gwas.p_value)\nshow(p)\ngwas = hl.linear_regression_rows(\n    y=mt.pheno.CaffeineConsumption,\n    x=mt.GT.n_alt_alleles(),\n    covariates=[1.0, mt.pheno.isFemale, mt.pca.scores[0], mt.pca.scores[1], mt.pca.scores[2]])```\n````\n\n## Resources\n\nSome files can be made available with `gsutil` installed, e.g.,\n\n- The HGDP release 3.1, `gs://gcp-public-data--gnomad/release/3.1`.\n  - See also, `gs://gnomad-public/release`.\n- UK Biobank results, `gs://hail-datasets/ukbb_imputed_v3_gwas_results_both_sexes.GRCh37.mt`.\n- Pan-ancestry genetic analysis of the UK Biobank, [https://pan.ukbb.broadinstitute.org/docs/hail-format](https://pan.ukbb.broadinstitute.org/docs/hail-format).\n- Exome-based association statistics, see below.\n\n## genebass\n\nWeb: [https://genebass.org/](https://genebass.org/)\n\n### Download\n\n```bash\n# genebass and csd3 locations\nexport src=gs://ukbb-exome-public/300k/results\nexport dest=~/rds/results/public/gwas/ukb_exomes\nmkdir -p ${dest}\ncd ${dest}\n# Gene burden results\ngsutil -m cp -r ${src}/results.mt .\n# Single variant association results\ngsutil -m cp -r ${src}/variant_results.mt .\n```\n\n### Access\n\nWe could run the script directly as follows,\n\n```bash\ncd ~/rds/results/public/gwas/ukb_exomes\npython <<END\n\nimport hail as hl\n\n# Gene (burden) results\ngr = hl.read_matrix_table('results.mt')\n\n## Summaries\ngr.count()\ngr.describe()\ngr.show()\ngr.cols().show()\ngr.rows().show()\ngr.entry.take(5)\ngr.summarize()\n\n## filtering\nFGR = gr.filter_rows(gr.gene_symbol==\"FGR\")\nFGR1065 = FGR.filter_cols(FGR.coding == '1065')\nFGR1065burden = FGR1065.select_entries(FGR1065.BETA_Burden)\n\n## col operations\ntrait_gr = gr.cols().collect_by_key()\ntrait_gr.group_by('trait_type').aggregate(n_phenos=hl.agg.count()).show()\ngr_cols = gr.cols()\ngr_cols.count()\ngr_cols.show(truncate=40, width=85)\ngr_cols.phenocode.show()\ngr_cols.filter(gb_cols.phenocode == '1777').show()\nfields_to_drop = ['category', 'coding_description', 'description_more', 'inv_normalized',\n                  'n_cases_both_sexes', 'n_cases_females', 'n_cases_males', 'saige_version']\ngr_cols_reduced=gb_cols.drop(*fields_to_drop)\n\n# Variant results\nvr = hl.read_matrix_table('variant_results.mt')\nhl.summarize_variants(vr)\nvr.count()\nvr.describe()\nvr.select_cols().show(1)\nvr.select_rows().show(1)\nvr.locus.show(1)\nco1777=vr.filter_cols(vr.coding==\"1777\").show()\nco1777.entry\nco1777.entry.take(5)\n\n# IL12B\nEND\n```\n\n## Reference\n\nKarczewski, K.J. et al. Systematic single-variant and gene-based association testing of 3,700 phenotypes in 281,850 UK Biobank exomes. medRxiv, 2021.06.19.21259117 (2021).\n","dir":"/applications/","name":"hail.md","path":"applications/hail.md","url":"/applications/hail.html"},{"sort":17,"layout":"default","title":"HESS","content":"<h1 id=\"hess\">HESS</h1>\n\n<p>This section is extracted from https://github.com/jinghuazhao/software-notes.</p>\n\n<p>HESS (Heritability Estimation from Summary Statistics) is now available from https://github.com/huwenboshi/hess and has a web page at</p>\n\n<p>https://huwenboshi.github.io/hess-0.5/#hess</p>\n\n<p>To prepare for the software, one can proceeds with</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>python <span class=\"nt\">-m</span> pip <span class=\"nb\">install </span>pysnptools <span class=\"nt\">--user</span>\n</code></pre> </div></div>\n\n<p>Now we set up for analysis of height</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n\n<span class=\"nb\">export </span><span class=\"nv\">HEIGHT</span><span class=\"o\">=</span>https://portals.broadinstitute.org/collaboration/giant/images/0/01/GIANT_HEIGHT_Wood_et_al_2014_publicrelease_HapMapCeuFreq.txt.gz\n\nwget <span class=\"nt\">-qO-</span> <span class=\"nv\">$HEIGHT</span> | <span class=\"se\">\\</span>\n<span class=\"nb\">awk</span> <span class=\"s1\">'NR&gt;1'</span> | <span class=\"se\">\\</span>\n<span class=\"nb\">sort</span> <span class=\"nt\">-k1</span>,1 | <span class=\"se\">\\</span>\n<span class=\"nb\">join</span> <span class=\"nt\">-13</span> <span class=\"nt\">-21</span> snp150.txt - | <span class=\"se\">\\</span>\n<span class=\"nb\">awk</span> <span class=\"s1\">'($9!=\"X\" &amp;&amp; $9!=\"Y\" &amp;&amp; $9!=\"Un\"){if(NR==1) print \"SNP CHR BP A1 A2 Z N\"; else print $1,$2,$3,$4,$5,$7/$8,$10}'</span> <span class=\"o\">&gt;</span> height.tsv.gz\n\n<span class=\"c\">#  SNP - rs ID of the SNP (e.g. rs62442).</span>\n<span class=\"c\">#  CHR - Chromosome number of the SNP. This should be a number between 1 and 22.</span>\n<span class=\"c\">#  BP - Base pair position of the SNP.</span>\n<span class=\"c\">#  A1 - Effect allele of the SNP. The sign of the Z-score is with respect to this allele.</span>\n<span class=\"c\">#  A2 - The other allele of the SNP.</span>\n<span class=\"c\">#  Z - The Z-score of the SNP.</span>\n<span class=\"c\">#  N - Sample size of the SNP.</span>\n</code></pre> </div></div>\n\n<p>where snp150.txt from UCSC is described at the SUMSTATS repository, <a href=\"https://github.com/jinghuazhao/SUMSTATS\">https://github.com/jinghuazhao/SUMSTATS</a>.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">for </span>chrom <span class=\"k\">in</span> <span class=\"si\">$(</span><span class=\"nb\">seq </span>22<span class=\"si\">)</span>\n<span class=\"k\">do\n    </span>python hess.py <span class=\"se\">\\</span>\n        <span class=\"nt\">--local-hsqg</span> height <span class=\"se\">\\</span>\n        <span class=\"nt\">--chrom</span> <span class=\"nv\">$chrom</span> <span class=\"se\">\\</span>\n        <span class=\"nt\">--bfile</span> 1kg_eur_1pct/1kg_eur_1pct_chr<span class=\"k\">${</span><span class=\"nv\">chrom</span><span class=\"k\">}</span> <span class=\"se\">\\</span>\n        <span class=\"nt\">--partition</span> nygcresearch-ldetect-data-ac125e47bf7f/EUR/fourier_ls-chr<span class=\"k\">${</span><span class=\"nv\">chrom</span><span class=\"k\">}</span>.bed <span class=\"se\">\\</span>\n        <span class=\"nt\">--out</span> step1\n<span class=\"k\">done\n</span>python hess.py <span class=\"nt\">--prefix</span> step1 <span class=\"nt\">--reinflate-lambda-gc</span> 1 <span class=\"nt\">--tot-hsqg</span> 0.8 0.2 <span class=\"nt\">--out</span> step2\n</code></pre> </div></div>\n\n<p>It is preferable to use <code class=\"language-plaintext highlighter-rouge\">miniconda</code> since it associates with faster libraries.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>module load miniconda2-4.3.14-gcc-5.4.0-xjtq53h\nconda <span class=\"nb\">install </span>pandas\n</code></pre> </div></div>\n","dir":"/applications/","name":"HESS.md","path":"applications/HESS.md","url":"/applications/HESS.html"},{"sort":17,"layout":"default","title":"R/rstan","content":"# R/rstan\n\nOfficial page: [https://mc-stan.org/users/interfaces/rstan](https://mc-stan.org/users/interfaces/rstan) and also [https://cran.r-project.org/package=rstan](https://cran.r-project.org/package=rstan).\n\nIt is necessary to have `¬/.R/Makevars` the following lines,\n\n```\nCXX14 = g++ -fPIC -std=gnu++11 -fext-numeric-literals\n```\n\nto deal with `error: unable to find numeric literal operator 'operator\"\"Q'` but\n\n```\nCXX14 = g++ -std=c++1y -fPIC\n```\n\nto do away with the error message ``C++14 standard requested but CXX14 is not defined`.\n\nIn case `ggplot2` installed with `gcc 5.2.0` it is also necessary to preceed with `module load gcc/5`.\n\nFor the developmental version, try `remotes::install_github(\"stan-dev/rstan\", ref = \"develop\", subdir = \"rstan/rstan\")`.\n","dir":"/applications/R/","name":"rstan.md","path":"applications/R/rstan.md","url":"/applications/R/rstan.html"},{"sort":18,"layout":"default","title":"KentUtils","content":"<h1 id=\"kentutils\">KentUtils</h1>\n\n<p>This is part of the Kent utilities in module <code class=\"language-plaintext highlighter-rouge\">kentutils-302.1-gcc-5.4.0-kbiujaa</code> nevertheless without the appropriate chain file.</p>\n\n<p>To download the latest utilitiess, try <code class=\"language-plaintext highlighter-rouge\">rsync -aP rsync://hgdownload.soe.ucsc.edu/genome/admin/exe/linux.x86_64/ ./</code>.</p>\n\n<p>The most notable is liftOver from UCSC here, <a href=\"https://genome.ucsc.edu/cgi-bin/hgLiftOver\">https://genome.ucsc.edu/cgi-bin/hgLiftOver</a>.</p>\n\n<p>Suppose we have <code class=\"language-plaintext highlighter-rouge\">r6-b38-A2.bed</code>,</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>#CHROM\tstart\tend\tSNP\nchr1\t841851\t841852\t1:841852:C:T\nchr1\t856472\t856473\t1:856473:G:A\nchr1\t858951\t858952\t1:858952:G:A\nchr1\t859841\t859842\t1:859842:C:G\nchr1\t863420\t863421\t1:863421:G:A\nchr1\t863578\t863579\t1:863579:G:A\nchr1\t864118\t864119\t1:864119:T:C\nchr1\t866155\t866156\t1:866156:T:G\nchr1\t866280\t866281\t1:866281:C:T\n</code></pre> </div></div>\n\n<p>To convert back to b37, our syntax is as follows,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>liftOver r6-b38-A2.bed hg38ToHg19.over.chain.gz r6-b37-A2.bed r6-b37-A2.unlifted.bed\n</code></pre> </div></div>\n\n<p>with <code class=\"language-plaintext highlighter-rouge\">r6-b37-A2.bed</code> containing these lines,</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>chr1\t777231\t777232\t1:841852:C:T\nchr1\t791852\t791853\t1:856473:G:A\nchr1\t794331\t794332\t1:858952:G:A\nchr1\t795221\t795222\t1:859842:C:G\nchr1\t798800\t798801\t1:863421:G:A\nchr1\t798958\t798959\t1:863579:G:A\nchr1\t799498\t799499\t1:864119:T:C\nchr1\t801535\t801536\t1:866156:T:G\nchr1\t801660\t801661\t1:866281:C:T\n</code></pre> </div></div>\n","dir":"/applications/","name":"KentUtils.md","path":"applications/KentUtils.md","url":"/applications/KentUtils.html"},{"sort":18,"layout":"default","title":"R/SAIGE 0.36.6 and 0.39.2","content":"<h1 id=\"rsaige-0366-and-0392\">R/SAIGE 0.36.6 and 0.39.2</h1>\n\n<p>Full name: Scalable and Accurate Implementation of GEneralized mixed model (SAIGE)</p>\n\n<p>GitHub page: <a href=\"https://github.com/weizhouUMICH/SAIGE\">https://github.com/weizhouUMICH/SAIGE</a>.</p>\n\n<p>The following is based on source from GitHub (so with the possibility to git pull),</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>module load cmake/3.9 gcc/5\nmodule load python/2.7\nvirtualenv py27\n<span class=\"nb\">source </span>py27/bin/activate\npip <span class=\"nb\">install </span>cget\ngit clone https://github.com/weizhouUMICH/SAIGE\nR CMD INSTALL SAIGE\n</code></pre> </div></div>\n\n<p>Now we see <code class=\"language-plaintext highlighter-rouge\">.../SAIGE.so: undefined symbol: sgecon_</code>. One can get away with it by renaming <code class=\"language-plaintext highlighter-rouge\">configure</code> to <code class=\"language-plaintext highlighter-rouge\">configure.sav</code> (so avoid repeated downloads) and amend the last <code class=\"language-plaintext highlighter-rouge\">g++ ... -o SAIGE.so</code> with <code class=\"language-plaintext highlighter-rouge\">-L$HPC_WORK/lib64 -llapack</code> and then rerun <code class=\"language-plaintext highlighter-rouge\">R CMD INSTALL SAIGE</code>. After successful installation, we can try <code class=\"language-plaintext highlighter-rouge\">cd SAIGE/extdata; bash cmd.sh</code>.</p>\n\n<p>One of the third party software is <code class=\"language-plaintext highlighter-rouge\">bgenix</code> (BE careful with a buggy <code class=\"language-plaintext highlighter-rouge\">cat-bgen</code>!), whose <code class=\"language-plaintext highlighter-rouge\">wscript</code> uses Python 2 syntax so it is necessary to stick to python/2.7 explicitly since gcc/5 automatically loads python 3.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>cd SAIGE\ncd thirdParty\ncd bgen\n./waf configure --prefix=$HPC_WORK\n./waf\n./waf install\nbuild/test/unit/test_bgen\nbuild/apps/bgenix -g example/example.16bits.bgen -list\ncd ../../..\n</code></pre> </div></div>\n\n<p>See <a href=\"https://github.com/weizhouUMICH/SAIGE/issues/98\">https://github.com/weizhouUMICH/SAIGE/issues/98</a>.</p>\n\n<p>For the latest version 0.39.2 which deals with the chromosome X ploidy, the following steps are necessary</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>R <span class=\"nt\">-e</span> <span class=\"s2\">\"devtools::install_github('leeshawn/MetaSKAT')\"</span>\nR <span class=\"nt\">-e</span> <span class=\"s2\">\"devtools::install_github('leeshawn/SPAtest')\"</span>\ngit clone <span class=\"nt\">--depth</span> 1 <span class=\"nt\">-b</span> 0.39.2 https://github.com/weizhouUMICH/SAIGE\nR CMD INSTALL SAIGE\n</code></pre> </div></div>\n\n<p>which first installs MetaSKAT 0.80 also at CRAN but SPAtest 3.1.2 instead of 3.0.2 from CRAN.</p>\n","dir":"/applications/R/","name":"SAIGE.md","path":"applications/R/SAIGE.md","url":"/applications/R/SAIGE.html"},{"sort":19,"layout":"default","title":"R/sf","content":"# R/sf\n\nA number of packages use it as dependency and there might be error such as this,\n\n```\nError in dyn.load(file, DLLpath = DLLpath, ...) :\n  unable to load shared object '/rds/user/jhz22/hpc-work/R/sf/libs/sf.so':\n  /rds/user/jhz22/hpc-work/R/sf/libs/sf.so: undefined symbol: GEOSSTRtree_nearest_generic_r\nCalls: <Anonymous> ... asNamespace -> loadNamespace -> library.dynam -> dyn.load\nExecution halted\n```\n\nWe can get away with this,\n\n```bash\nmodule load geos-3.6.2-gcc-5.4.0-vejexvy\nmodule load gcc/6\nRscript -e \"install.packages('sf')\"\n```\n","dir":"/applications/R/","name":"sf.md","path":"applications/R/sf.md","url":"/applications/R/sf.html"},{"sort":19,"layout":"default","title":"lapack","content":"# lapack\n\nWeb: [http://www.netlib.org/lapack/](http://www.netlib.org/lapack/)\n\nThe sequence is as follows,\n\n```bash\nwget http://www.netlib.org/lapack/lapack-3.9.0.tar.gz\ntar xvfz lapack-3.9.0.tar.gz\ncd lapack-3.9.0\nmkdir build\ncd build\n## ccmake .\ncmake ..\nmake\nmake install\n```\n\nNote `ccmake` is commented as it does not work properly on csd3. By default these build static libraries into build/lib/. It is also possible to build the static libraries in lapack-3.9.0/ by making a copy of `make.inc.example` as `make.inc` and compile `libblas.a`,`liblapack.a` as well as `libcblas.a` and `liblapacke.a` from BLAS/ and LAPACKE/ directories.\n\nTo enable the shared libraries, proceed with\n\n```bash\ncmake -DCMAKE_INSTALL_PREFIX=${HPC_WORK} -DCMAKE_BUILD_TYPE=RELEASE -DBUILD_SHARED_LIBS=ON -DLAPACKE=ON -DCBLAS=ON ..\n```\n","dir":"/applications/","name":"lapack.md","path":"applications/lapack.md","url":"/applications/lapack.html"},{"sort":20,"layout":"default","title":"R/snpnet","content":"# R/snpnet\n\nGitHub page: [https://github.com/rivas-lab/snpnet](https://github.com/rivas-lab/snpnet).\n\nA number of software needs to be set up with the current version.\n\n```bash\nmodule load lz4-1.8.1.2-intel-17.0.4-celw56p\nwget https://github.com/facebook/zstd/releases/download/v1.4.4/zstd-1.4.4.tar.gz\ntar xfz zstd-1.4.4.tar.gz\ncd zstd-1.4.4\nmake\nmake install prefix=$HPC_WORK\n# gcc/6 is required for pgenlibr\nmodule load gcc/6\n```\n\nFile `dotCall64/Makevars` needs to be modified, but can be difficult (e.g., reinstallation of gettext, https://ftp.gnu.org/gnu/gettext/gettext-0.20.tar.gz), then\n\n```\nPKG_CFLAGS = $(SHLIB_OPENMP_CFLAGS) -I../inst/include/ -DDOTCAL64_PRIVATE -I$HPC_WORK/include\nPKG_LIBS = $(SHLIB_OPENMP_CFLAGS) -L$HPC_WORK/lib -lintl\n```\n\nFinally, we can proceed\n\n```r\ndevtools::install_github(\"junyangq/glmnetPlus\")\ndevtools::install_github(\"chrchang/plink-ng\", subdir=\"/2.0/cindex\")\ndevtools::install_github(\"chrchang/plink-ng\", subdir=\"/2.0/pgenlibr\")\ndevtools::install_github(\"rivas-lab/snpnet\")\n```\n","dir":"/applications/R/","name":"snpnet.md","path":"applications/R/snpnet.md","url":"/applications/R/snpnet.html"},{"sort":20,"layout":"default","title":"ldsc","content":"# ldsc\n\nWeb: [https://github.com/bulik/ldsc](https://github.com/bulik/ldsc) ([Google group](https://groups.google.com/g/ldsc_users/)).\n\n## Installation\n\nWe proceed as follows for installation into HPC_WORK=/rds/user/$USER/hpc-work,\n\n```bash\nmodule load python/2.7\nvirtualenv py27\nsource py27/bin/activate\ncd ${HPC_WORK}\ngit clone https://github.com/bulik/ldsc\ncd ldsc\npip install -r requirements.txt\nwget https://data.broadinstitute.org/alkesgroup/LDSCORE/w_hm3.snplist.bz2 | \\\nbzip2 -d\n```\n\nThe last two commands also gets the HapMap 3 SNP list. It is worthwhile to note that\n\n- The ldsc documentation suggests Anaconda and on CSD3 we could use the miniconda/2 module, i.e, [https://cambridge-ceu.github.io/csd3/systems/software.html#python](https://cambridge-ceu.github.io/csd3/systems/software.html#python), but it is considerably more involved.\n- We only need `module load python/2.7` and `source ${HOME}/py27/bin/activate` later on.\n\n## Testing\n\nWe use the GIANT BMI data,\n\n```bash\nwget -qO- http://portals.broadinstitute.org/collaboration/giant/images/c/c8/Meta-analysis_Locke_et_al%2BUKBiobank_2018_UPDATED.txt.gz | \\\ngunzip -c > BMI.txt\npython munge_sumstats.py --sumstats BMI.txt --a1 Tested_Allele --a2 Other_Allele --merge-alleles w_hm3.snplist --out BMI --a1-inc\n```\n\nNote the munging procedure requests large resources and will be terminated by CSD3, so we better test with a SLURM job instead.\n\n## Analysis\n\n### Heritability partition\n\nWe now complete the download on frequencies, baseline model LD scores, and regression weights and furnish this.\n\n```bash\nwget -qO- https://storage.googleapis.com/broad-alkesgroup-public/LDSCORE/1000G_Phase1_frq.tgz | tar xvfz -\nwget -qO- https://storage.googleapis.com/broad-alkesgroup-public/LDSCORE/1000G_Phase1_baseline_ldscores.tgz | tar xvfz -\nwget -qO- https://storage.googleapis.com/broad-alkesgroup-public/LDSCORE/weights_hm3_no_hla.tgz | tar xvfz -\n```\n\nOur batch file is as follows,\n\n```\n#!/usr/bin/bash\n\n#SBATCH --job-name=BMI\n#SBATCH --account CARDIO-SL0-CPU\n#SBATCH --partition cardio\n#SBATCH --qos=cardio\n#SBATCH --mem=28800\n#SBATCH --time=5-00:00:00\n#SBATCH --output=/rds/user/jhz22/hpc-work/work/_BMI_%A_%a.out\n#SBATCH --error=/rds/user/jhz22/hpc-work/work/_BMI_%A_%a.err\n#SBATCH --export ALL\n\nexport TMPDIR=/rds/user/$USER/hpc-work/work\n\nmodule load python/2.7\nsource ${HOME}/py27/bin/activate\ncd ${HPC_WORK}/ldsc\npython munge_sumstats.py --sumstats BMI.txt --a1 Tested_Allele --a2 Other_Allele --merge-alleles w_hm3.snplist --out BMI --a1-inc\npython ldsc.py\\\n\t--h2 BMI.sumstats.gz\\\n\t--ref-ld-chr baseline/baseline.\\\n\t--w-ld-chr weights_hm3_no_hla/weights.\\\n\t--overlap-annot\\\n\t--frqfile-chr 1000G_frq/1000G.mac5eur.\\\n\t--out BMI_baseline\n```\n\nand our results are contained in the tab-delimited file named `BMI_baseline.result` -- note in particular the CNS enrichment P=8.30e-24.\n\n### Cell type analysis\n\nWe carry on with the download,\n\n```bash\nwget -qO- https://storage.googleapis.com/broad-alkesgroup-public/LDSCORE/1000G_Phase1_cell_type_groups.tgz | \\\ntar xvfz -\n```\n\nand for CNS, we have\n\n```bash\npython ldsc.py\\\n        --h2 BMI.sumstats.gz\\\n        --w-ld-chr weights_hm3_no_hla/weights.\\\n        --ref-ld-chr cell_type_groups/CNS.,baseline/baseline.\\\n        --overlap-annot\\\n        --frqfile-chr 1000G_frq/1000G.mac5eur.\\\n        --out BMI_CNS\\\n        --print-coefficients\n```\n\nand our results are now contained in the tab-delimited file named `BMI_CNS.results`.\n\nWe next use the `--h2-cts` option with the Cahoy data analysis in a nutshell,\n\n```bash\nexport cts_name=Cahoy\nwget -qO- https://storage.googleapis.com/broad-alkesgroup-public/LDSCORE/LDSC_SEG_ldscores/${cts_name}_1000Gv3_ldscores.tgz | \\\ntar xfvz -\nwget -qO- https://storage.googleapis.com/broad-alkesgroup-public/LDSCORE/1000G_Phase3_baseline_ldscores.tgz | \\\ntar xvfz -\nldsc.py\\\n    --h2-cts BMI.sumstats.gz \\\n    --ref-ld-chr 1000G_EUR_Phase3_baseline/baseline. \\\n    --out BMI_${cts_name} \\\n    --ref-ld-chr-cts $cts_name.ldcts \\\n    --w-ld-chr weights_hm3_no_hla/weights.\n```\n\nThe output `BMI_Cahoy.cell_type_results.txt` is sufficiently small to include here,\n\n| Name            | Coefficient            | Coefficient_std_error  | Coefficient_P_value  |\n| --------------- | ---------------------- | ---------------------- | -------------------- |\n| Neuron          | 4.4874060288359995e-09 | 2.48025909733557e-09   | 0.035206172355899706 |\n| Oligodendrocyte | 8.067689953393081e-10  | 2.569340962599481e-09  | 0.376761120478732    |\n| Astrocyte       | -4.036699628095808e-09 | 2.0886996416620756e-09 | 0.9733595763245972   |\n\nIn line with the finding above, we have a P=0.035 for neurons.\n\n### Genetic correlation\n\nWe carry on to calculate the genetic correlation (rg) between BMI and height. First, we obtain the LD scores,\n\n```bash\nwget -qO- https://data.broadinstitute.org/alkesgroup/LDSCORE/eur_w_ld_chr.tar.bz2 | \\\ntar xjf -\n```\n\nfollowed by downloading and munging height GWAS summary statistics\n\n```bash\nwget -qO- http://portals.broadinstitute.org/collaboration/giant/images/0/01/GIANT_HEIGHT_Wood_et_al_2014_publicrelease_HapMapCeuFreq.txt.gz | \\\ngunzip -c > height.txt\npython munge_sumstats.py --sumstats height.txt \\\n                         --snp MarkerName --a1 Allele1 --a2 Allele2 --merge-alleles w_hm3.snplist --p p --out height --a1-inc\n```\n\nbut again it will be killed and we need a SLURM job as above. On CSD3, it took a staggering 14hr.\n\nOur analysis then proceeds with\n\n```bash\npython ldsc.py \\\n      --rg BMI.sumstats.gz,height.sumstats.gz \\\n      --ref-ld-chr eur_w_ld_chr/ \\\n      --w-ld-chr eur_w_ld_chr/ \\\n      --out BMI_height\n```\n\nIt took just under 16s and `BMI_height.log` contains the desired output quoted below\n\n```\n*********************************************************************\n* LD Score Regression (LDSC)\n* Version 1.0.1\n* (C) 2014-2019 Brendan Bulik-Sullivan and Hilary Finucane\n* Broad Institute of MIT and Harvard / MIT Department of Mathematics\n* GNU General Public License v3\n*********************************************************************\nCall:\n./ldsc.py \\\n--ref-ld-chr eur_w_ld_chr/ \\\n--out BMI_height \\\n--rg BMI.sumstats.gz,height.sumstats.gz \\\n--w-ld-chr eur_w_ld_chr/\n\nBeginning analysis at Wed Aug  4 07:11:02 2021\nReading summary statistics from BMI.sumstats.gz ...\nRead summary statistics for 1019865 SNPs.\nReading reference panel LD Score from eur_w_ld_chr/[1-22] ... (ldscore_fromlist)\nRead reference panel LD Scores for 1290028 SNPs.\nRemoving partitioned LD Scores with zero variance.\nReading regression weight LD Score from eur_w_ld_chr/[1-22] ... (ldscore_fromlist)\nRead regression weight LD Scores for 1290028 SNPs.\nAfter merging with reference panel LD, 1014995 SNPs remain.\nAfter merging with regression SNP LD, 1014995 SNPs remain.\nComputing rg for phenotype 2/2\nReading summary statistics from height.sumstats.gz ...\nRead summary statistics for 1217311 SNPs.\nAfter merging with summary statistics, 1014995 SNPs remain.\n993172 SNPs with valid alleles.\n\nHeritability of phenotype 1\n---------------------------\nTotal Observed scale h2: 0.2104 (0.0066)\nLambda GC: 2.7872\nMean Chi^2: 3.9573\nIntercept: 1.0292 (0.0298)\nRatio: 0.0099 (0.0101)\n\nHeritability of phenotype 2/2\n-----------------------------\nTotal Observed scale h2: 0.342 (0.0176)\nLambda GC: 2.0007\nMean Chi^2: 2.9726\nIntercept: 1.2239 (0.033)\nRatio: 0.1135 (0.0167)\n\nGenetic Covariance\n------------------\nTotal Observed scale gencov: 0.1517 (0.0047)\nMean z1*z2: 2.0178\nIntercept: 0.738 (0.0148)\n\nGenetic Correlation\n-------------------\nGenetic Correlation: 0.5656 (0.0081)\nZ-score: 70.1339\nP: 0.\n\n\nSummary of Genetic Correlation Results\np1                  p2      rg      se        z    p  h2_obs  h2_obs_se  h2_int  h2_int_se  gcov_int  gcov_int_se\nBMI.sumstats.gz  height.sumstats.gz  0.5656  0.0081  70.1339  0.0   0.342     0.0176  1.2239      0.033     0.738       0.0148\n\nAnalysis finished at Wed Aug  4 07:11:18 2021\nTotal time elapsed: 15.56s\n```\n\nand rg=0.57.\n","dir":"/applications/","name":"ldsc.md","path":"applications/ldsc.md","url":"/applications/ldsc.html"},{"sort":21,"layout":"default","title":"LEMMA","content":"<h1 id=\"lemma\">LEMMA</h1>\n\n<p>A by-product of SAIGE installation is LEMMA,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget <span class=\"nt\">-qO-</span> https://github.com/mkerin/LEMMA/archive/v1.0.2.tar.gz | <span class=\"se\">\\</span>\n<span class=\"nb\">tar </span>xvfz | <span class=\"se\">\\</span>\n<span class=\"nb\">cd </span>LEMMA-1.0.2\ncmake <span class=\"nt\">-S</span> <span class=\"nb\">.</span> <span class=\"nt\">-B</span> build <span class=\"se\">\\</span>\n      <span class=\"nt\">-DBGEN_ROOT</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">HOME</span><span class=\"k\">}</span>/SAIGE/thirdParty/bgen <span class=\"se\">\\</span>\n      <span class=\"nt\">-DBOOST_ROOT</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">HOME</span><span class=\"k\">}</span>/SAIGE/thirdParty/bgen/3rd_party/boost_1_55_0\n<span class=\"nb\">cd </span>build\nmake\n</code></pre> </div></div>\n","dir":"/applications/","name":"LEMMA.md","path":"applications/LEMMA.md","url":"/applications/LEMMA.html"},{"sort":21,"layout":"default","title":"R/sojo","content":"# R/sojo\n\nGitHub page: [https://github.com/zhenin/sojo](https://github.com/zhenin/sojo).\n\n```r\ninstall.packages(\"sojo\", repos = \"http://R-Forge.R-project.org\")\n# Swedish twin registry\ndownload.file(\"https://www.dropbox.com/s/ty1udfhx5ohauh8/LD_chr22.rda?raw=1\",\ndestfile = paste0(find.package('sojo'), \"/LD_chr22.rda\"))\nload(file = paste0(find.package('sojo'), \"/LD_chr22.rda\"))\n# 1000Genomes\ndownload.file(\"https://data.broadinstitute.org/alkesgroup/LDSCORE/1000G_Phase3_plinkfiles.tgz\",\ndestfile = paste0(find.package('sojo'), \"/1000G_Phase3_plinkfiles.tgz\"))\nuntar(paste0(find.package('sojo'), \"/1000G_Phase3_plinkfiles.tgz\"),exdir=find.package('sojo'))\nrequire(sojo)\ndata(sum.stat.discovery)\nhpc_work <- Sys.getenv(\"HPC_WORK\")\npath.plink <- paste0(hpc_work,\"/bin/plink\")\npath.1kG <- paste0(find.package('sojo'),\"/1000G_EUR_Phase3_plink\")\nsnps <- sum.stat.discovery$SNP\nwrite.table(snps, file = paste0(snps[1],\"_snp_list.txt\"), quote = F, row.names = F, col.names = F)\nchr <- 22\nsystem(paste0(path.plink,\" -bfile \", path.1kG,\"/1000G.EUR.QC.\",chr,\" --r square --extract \", snps[1], \"_snp_list.txt --out \", snps[1], \" --noweb\"))\nsystem(paste0(path.plink,\" -bfile \", path.1kG,\"/1000G.EUR.QC.\",chr,\" --freq --extract \", snps[1], \"_snp_list.txt --out \", snps[1], \" --noweb\"))\nLD_1kG <- as.matrix(read.table(paste0(snps[1], \".ld\")))\nmaf_1kG <- read.table(paste0(snps[1], \".frq\"), header = T)\nsnp_ref_1kG <- maf_1kG[,\"A2\"]\nnames(snp_ref_1kG) <- maf_1kG[,\"SNP\"]\ncolnames(LD_1kG) <- rownames(LD_1kG) <- maf_1kG$SNP\nres <- sojo(sum.stat.discovery, LD_ref = LD_mat, snp_ref = snp_ref, nvar = 20)\nmatplot(log(res$lambda.v), t(as.matrix(res$beta.mat)), lty = 1, type = \"l\",\n    xlab = expression(paste(log, \" \", lambda)), ylab = \"Coefficients\", main = \"Summary-level LASSO\")\ndata(sum.stat.validation)\nres.valid <- sojo(sum.stat.discovery, sum.stat.validation, LD_ref = LD_mat, snp_ref = snp_ref, nvar = 20)\n```\n","dir":"/applications/R/","name":"sojo.md","path":"applications/R/sojo.md","url":"/applications/R/sojo.html"},{"sort":22,"layout":"default","title":"MetaXcan","content":"<h1 id=\"metaxcan\">MetaXcan</h1>\n\n<p>Web: https://github.com/hakyimlab/MetaXcan</p>\n\n<p>Installation and a test run,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>git clone https://github.com/hakyimlab/MetaXcan\n<span class=\"nb\">cd </span>MetaXcan/software\n<span class=\"c\"># download sample_data.tar.gz</span>\n<span class=\"nb\">tar</span> <span class=\"nt\">-xzvpf</span> sample_data.tar.gz\nmodule load miniconda3/4.5.1\nconda <span class=\"nb\">env </span>create <span class=\"nt\">-f</span> <span class=\"nv\">$HPC_WORK</span>/MetaXcan/software/conda_env.yaml\nconda activate imlabtools\nconda <span class=\"nb\">install </span>pandas\npython setup.py <span class=\"nb\">install</span> <span class=\"nt\">--user</span>\n\n./SPrediXcan.py <span class=\"se\">\\</span>\n<span class=\"nt\">--model_db_path</span> data/DGN-WB_0.5.db <span class=\"se\">\\</span>\n<span class=\"nt\">--covariance</span> data/covariance.DGN-WB_0.5.txt.gz <span class=\"se\">\\</span>\n<span class=\"nt\">--gwas_folder</span> data/GWAS <span class=\"se\">\\</span>\n<span class=\"nt\">--gwas_file_pattern</span> <span class=\"s2\">\".*gz\"</span> <span class=\"se\">\\</span>\n<span class=\"nt\">--snp_column</span> SNP <span class=\"se\">\\</span>\n<span class=\"nt\">--effect_allele_column</span> A1 <span class=\"se\">\\</span>\n<span class=\"nt\">--non_effect_allele_column</span> A2 <span class=\"se\">\\</span>\n<span class=\"nt\">--beta_column</span> BETA <span class=\"se\">\\</span>\n<span class=\"nt\">--pvalue_column</span> P <span class=\"se\">\\</span>\n<span class=\"nt\">--output_file</span> results/test.csv\n</code></pre> </div></div>\n","dir":"/applications/","name":"MetaXcan.md","path":"applications/MetaXcan.md","url":"/applications/MetaXcan.html"},{"sort":22,"layout":"default","title":"R/xlsx","content":"# R/xlsx\n\nWhen there is message,\n\n```\nError: .onLoad failed in loadNamespace() for 'rJava', details:\n  call: dyn.load(file, DLLpath = DLLpath, ...)\n  error: unable to load shared object '/rds/user/jhz22/hpc-work/R/rJava/libs/rJava.so':\n  libjvm.so: cannot open shared object file: No such file or directory\nExecution halted\nERROR: lazy loading failed for package ‘xlsx’\n* removing ‘/rds/user/jhz22/hpc-work/R/xlsx’\n* restoring previous ‘/rds/user/jhz22/hpc-work/R/xlsx’\n\n```\n\nthen see description on `rJava` earlier.\n","dir":"/applications/R/","name":"xlsx.md","path":"applications/R/xlsx.md","url":"/applications/R/xlsx.html"},{"sort":23,"layout":"default","title":"pandoc","content":"# pandoc\n\nWeb: [https://pandoc.org/](https://pandoc.org/)\n\nThe latest release: [https://github.com/jgm/pandoc/releases/tag/2.13](https://github.com/jgm/pandoc/releases/tag/2.13)\n\nSome applications requires the more recent version than those from CSD3; it is straightforward to install.\n\n```bash\ncd ${HPC_WORK}\nwget -qO- https://github.com/jgm/pandoc/releases/download/2.13/pandoc-2.13-linux-amd64.tar.gz | tar xvfz - --strip-components=1\n```\n\nso that the executable file and documentation are made available from bin/ and share/ directories, respectively.\n\nSee citeproc for handling bibliography.\n","dir":"/applications/","name":"pandoc.md","path":"applications/pandoc.md","url":"/applications/pandoc.html"},{"sort":24,"layout":"default","title":"PCA projection","content":"# PCA projection\n\nOfficial page: [https://github.com/covid19-hg/pca_projection](https://github.com/covid19-hg/pca_projection).\n\n```bash\ngit clone https://github.com/covid19-hg/pca_projection\ncd pca_projection\n```\n\n## setup\n\nThe required steps according to the documentation can be summarised as follows,\n\n```bash\n# Pre-computed PCA loadings, reference allele frequencies and scores\nwget https://storage.googleapis.com/covid19-hg-public/pca_projection/hgdp_tgp_pca_covid19hgi_snps_loadings.GRCh37.plink.tsv\nwget https://storage.googleapis.com/covid19-hg-public/pca_projection/hgdp_tgp_pca_covid19hgi_snps_loadings.GRCh37.plink.afreq\nwget https://storage.googleapis.com/covid19-hg-public/pca_projection/hgdp_tgp_pca_covid19hgi_snps_loadings.GRCh38.plink.tsv\nwget https://storage.googleapis.com/covid19-hg-public/pca_projection/hgdp_tgp_pca_covid19hgi_snps_loadings.GRCh38.plink.afreq\nwget https://storage.googleapis.com/covid19-hg-public/pca_projection/hgdp_tgp_pca_covid19hgi_snps_loadings.rsid.plink.tsv\nwget https://storage.googleapis.com/covid19-hg-public/pca_projection/hgdp_tgp_pca_covid19hgi_snps_loadings.rsid.plink.afreq\nwget https://storage.googleapis.com/covid19-hg-public/pca_projection/hgdp_tgp_pca_covid19hgi_snps_scores.txt.gz\n\n# imputed PLINK2 dosage files\n\ncut -f1 [path to the pre-computed loadings file] | tail -n +2 > variants.extract\n\n## .pgen/.pvar/.psam\nplink2 \\\n  --pfile [path to your per-chromosome pfile] \\\n  --extract variants.extract \\\n  --make-pfile \\\n  --out [per-chromosome output name]\nls [the previous per-chromosome output prefix].*.pgen | sed -e ‘s/.pgen//’ > merge-list.txt\nplink2 --pmerge-list merge-list.txt --out [all-chromosome output name]\n\n## .bed/.bim/.fam\n\nplink \\\n  --bfile [path to your per-chromosome pfile] \\\n  --extract variants.extract \\\n  --make-bed \\\n  --out [per-chromosome output name]\nls [outname].*.bed | sed -e ‘s/.bed//’ > merge-list.txt\nplink --merge-list merge-list.txt --out [all-chromosome output name]\n\n## .bgen/.sample (in doubt)\n\nbgenix \\\n  -g [path to your per-chromosome bgen] \\\n  -incl-rsids variant.extract \\\n  > [per-chromosome output name].bgen\ncat-bgen \\\n-g [path to your per-chromosome bgen 1] \\\n   [path to your per-chromosome bgen 2] \\\n...\n   [path to your per-chromosome bgen 22] \\\n-og [all-chromosome outname]\nplink2 \\\n  --bgen [path to all-chromosome bgen] [REF/ALT mode] \\\n  --make-pfile \\\n  --out [output pfile name]\n\n## .vcf\n\nbcftools view -Oz \\\n  -i “ID = @variants.extract” \\\n  [path to your per-chromosome vcf file] \\\n  > [per-chromosome outname>.vcf.gz]\nbcftools concat -Oz [per-chromosome vcf files] > [all-chromosome outname].vcf.gz\nplink2 \\\n  --vcf [all-chromosome outname].vcf.gz \\\n  dosage=[dosage field name] \\\n  --make-pfile \\\n  --out [outname]\n\n# project and plot PC\n\nplink2 \\\n  ${input_command} \\\n  --score ${PCA_LOADINGS} \\\n  center \\\n  cols=-scoreavgs,+scoresums \\\n  list-variants \\\n  header-read \\\n  --score-col-nums 3-22 \\\n  --read-freq ${PCA_AF} \\\n  --out ${OUTNAME}\n\nRscript -e \"install.packages(c(\"data.table\", \"hexbin\", \"optparse\", \"patchwork\", \"R.utils\", \"tidyverse\"))\"\nRscript plot_projected_pc.R \\\n  --sscore [path to .sscore output] \\\n  --phenotype-file [path to phenotype file] \\\n  --phenotype-col [phenotype column name]\n  --covariate-file [path to covariate file] \\\n  --pc-prefix [prefix of PC columns] \\\n  --pc-num [number of PCs used in GWAS] \\\n  --ancestry [ancestry code: AFR, AMR, EAS, EUR, MID, or SAS] \\\n  --study [your study name] \\\n  --out [output name prefix]\n\n# --ancestry-file [path to ancestry file] \\\n# --ancestry-col [ancestry column name]\n# --reference-score-file\n```\n\n## Implementation\n\nThis is detailed below by section.\n\n```bash\n#!/usr/bin/bash\n\nexport TMPDIR=${HPC_WORK}/work\n# genotype\nexport autosomes=~/rds/post_qc_data/interval/imputed/uk10k_1000g_b37/\n# location of PCs\nexport ref=~/rds/post_qc_data/interval/reference_files/genetic/reference_files_genotyped_imputed/\n# HGI working directory\nexport prefix=~/rds/rds-asb38-ceu-restricted/projects/covid/HGI\nexport dir=20201201-ANA_C2_V2\n\nexport PCA_projection=pca_projection\nexport PCA_loadings=hgdp_tgp_pca_covid19hgi_snps_loadings.GRCh37.plink.tsv\nexport PCA_af=hgdp_tgp_pca_covid19hgi_snps_loadings.GRCh37.plink.afreq\nexport sscore=hgdp_tgp_pca_covid19hgi_snps_scores.txt.gz\n\nmodule load plink/2.00-alpha ceuadmin/stata\n\nfunction extract_data()\n{\n  cut -f1 ${PCA_projection}/${PCA_loadings} | tail -n +2 > variants.extract\n  (\n    cat variants.extract\n    awk '{split($1,a,\":\");print \"chr\"a[1]\":\"a[2]\"_\"a[4]\"_\"a[3]}' variants.extract\n  ) > variants.extract2\n\n  cp ${prefix}/${dir}/output/INTERVAL-*.bgen.bgi ${TMPDIR}\n  seq 22 | \\\n  parallel -C' ' --env prefix --env dir '\n    ln -sf ${prefix}/${dir}/output/INTERVAL-{}.bgen ${TMPDIR}/INTERVAL-{}.bgen\n    python update_bgi.py --bgi ${TMPDIR}/INTERVAL-{}.bgen.bgi\n    bgenix -g ${TMPDIR}/INTERVAL-{}.bgen -incl-rsids variants.extract2 > ${prefix}/work/INTERVAL-{}.bgen\n  '\n}\n\nfunction twist()\n{\n  cat-bgen -g $(echo ${prefix}/work/INTERVAL-{1..22}.bgen) -og INTERVAL.bgen -clobber\n  bgenix -g INTERVAL.bgen -index -clobber\n  export csvfile=INTERVAL.csv\n  python update_bgi.py --bgi INTERVAL.bgen.bgi\n  plink2 --bgen INTERVAL.bgen ref-first --make-pfile --out INTERVAL\n  cp INTERVAL.p??? ${prefix}/work\n  (\n    head -1 INTERVAL.pvar\n    paste <(sed '1d' INTERVAL.pvar | cut -f1,2) \\\n          <(sed '1d' INTERVAL.csv | cut -d, -f3) | \\\n    paste - <(sed '1d' INTERVAL.pvar | cut -f4,5)\n  ) > ${prefix}/work/INTERVAL.pvar\n}\n\nfunction project_pc()\n{\n#!/bin/bash\n\n  set -eu\n################################################################################\n# Please fill in the below variables\n################################################################################\n# Metadata\n  STUDY_NAME=\"INTERVAL\"\n  ANALYST_LAST_NAME=\"ZHAO\"\n  DATE=\"$(date +'%Y%m%d')\"\n  OUTNAME=\"${prefix}/work/${STUDY_NAME}.${ANALYST_LAST_NAME}.${DATE}\"\n################################################################################\n# Location of downloaded input files\n  PCA_LOADINGS=\"${PCA_projection}/${PCA_loadings}\"\n  PCA_AF=\"${PCA_projection}/${PCA_af}\"\n################################################################################\n# Location of imputed genotype files\n# [Recommended]\n# PLINK 2 binary format: a prefix (with directories) of .pgen/.pvar/.psam files\n  PFILE=\"${prefix}/work/INTERVAL\"\n# [Acceptable]\n# PLINK 1 binary format: a prefix of .bed/.bim/.fam files\n  BFILE=\"\"\n################################################################################\n\n  function error_exit() {\n    echo \"${1:-\"Unknown Error\"}\" 1>&2\n    exit 1\n  }\n\n# Input checks\n  if [[ -z \"${STUDY_NAME}\" ]]; then\n    error_exit \"Please specify \\$STUDY_NAME.\"\n  fi\n\n  if [[ -z \"${ANALYST_LAST_NAME}\" ]]; then\n    error_exit \"Please specify \\$ANALYST_LAST_NAME.\"\n  fi\n\n  if [[ -z \"${PCA_LOADINGS}\" ]]; then\n    error_exit \"Please specify \\$PCA_LOADINGS.\"\n  fi\n\n  if [[ -z \"${PCA_AF}\" ]]; then\n    error_exit \"Please specify \\$PCA_AF.\"\n  fi\n\n  if [[ -n \"${PFILE}\" ]]; then\n    input_command=\"--pfile ${PFILE}\"\n  elif [[ -n \"${BFILE}\" ]]; then\n    input_command=\"--bfile ${BFILE}\"\n  else\n    error_exit \"Either \\$PFILE or \\$BFILE should be specified\"\n  fi\n\n# Run plink2 --score\n  plink2 \\\n    ${input_command} \\\n    --score ${PCA_LOADINGS} \\\n            variance-standardize \\\n            cols=-scoreavgs,+scoresums \\\n            list-variants \\\n            header-read \\\n    --score-col-nums 3-12 \\\n    --read-freq ${PCA_AF} \\\n    --out ${OUTNAME}\n\n# The score file does not have FID (=0)\n  awk '\n  {\n    if (NR==1) $1=\"#FID IID\"\n    else $1=0\" \"$1\n    print\n  }' ${OUTNAME}.sscore > ${prefix}/work/snpid.sscore\n  ln -sf ${OUTNAME}.sscore.vars ${prefix}/work/snpid.sscore.vars\n\n# Our PCs are PC_# rather than PC#\n# The phenotype and covariate file missed FID (=0) column\n  awk '\n  {\n    if (NR==1)\n    {\n      $1=\"FID I\"$1\n      gsub(/PC_/,\"PC\",$0)\n    }\n    else $1=0\" \" $1\n  };1' ${dir}/work/INTERVAL-covid.txt | \\\n  tr ' ' '\\t' > ${prefix}/work/snpid.txt\n  cut -f1-3 ${prefix}/work/snpid.txt > ${prefix}/work/snpid.pheno\n  cut -f3 --complement ${prefix}/work/snpid.txt > ${prefix}/work/snpid.covars\n\n  stata -b do ethnic.do\n\n  Rscript ${PCA_projection}/plot_projected_pc.R \\\n        --sscore ${prefix}/work/snpid.sscore \\\n        --phenotype-file ${prefix}/work/snpid.pheno \\\n        --phenotype-col SARS_CoV \\\n        --covariate-file ${prefix}/work/snpid.covars \\\n        --pc-prefix PC \\\n        --pc-num 20 \\\n        --ancestry-file ethnic.txt \\\n        --ancestry-col ethnic \\\n        --study ${STUDY_NAME} \\\n        --out ${OUTNAME}\n}\n\nextract_data;twist;project_pc\n```\n\n### update_bgi.py\n\nThe magic `update_bgi.py` is as follows\n\n```python\nimport argparse\nimport os\nimport pandas as pd\nimport sqlite3\n\n# mapping chromosome string (incluing 01-09, 23, and X) and correct string\nCHROM_MAPPING_STR = dict([(str(i), str(i)) for i in range(1, 23)] + [('0' + str(i), str(i)) for i in range(1, 10)] +\n                         [('X', 'X')])\ncsvfile=os.environ['csvfile']\n\ndef main(args):\n    conn = sqlite3.connect(args.bgi)\n    c = conn.cursor()\n    df = pd.read_sql(\"select * from Variant\", conn)\n    print(df,flush=True)\n    df.rsid = df.apply(lambda x: \"{}:{}:{}:{}\".format(CHROM_MAPPING_STR[x[0]], x[1], x[4], x[5]), axis=1)\n    print(df,flush=True)\n    df.to_sql(\"Variant\", conn, if_exists=\"replace\", index=False)\n    if csvfile != '':\n       df.to_csv(csvfile,index=False)\n    conn.close()\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--bgi', type=str, required=True)\n    args = parser.parse_args()\n    main(args)\n```\n\n### bgi.sql\n\nOne may wonder the information regarding `.bgen.bgi` at all, which can be viewed as follows,\n\n```bash\nsqlite3 INTERVAL.bgen.bgi < bgi.sql\n```\n\nwhile `bgi.sql` has the following lines\n\n```sqlite3\n.tables\n.separator \"\\t\"\n.header on\n.output metadata.txt\nselect * from metadata;\n.output Variant.txt\nselect * from Variant;\n```\n\n### ethnic.do\n\nThe Stata program generates files containing FID, IID and ethnicity\n\n```stata\ngzuse work/INTERVAL-omics-covid.dta.gz\nsort identifier\ngzsave INTERVAL-omics-covid, replace\ninsheet using 20201201/INTERVALdata_01DEC2020.csv, case clear\nsort identifier\ngzmerge identifier using INTERVAL-omics-covid.dta.gz\ntabulate ethnicPulse SARS_CoV, all\ngen str20 ethnic=ethnicPulse\nreplace ethnic=\"EUR\" if inlist(ethnicPulse,\"Eng/W/Scot/NI/Brit\",\"White Irish\")==1\nreplace ethnic=\"EAS\" if inlist(ethnicPulse,\"Asian- Bangladeshi\",\"Asian- Indian\",\"Asian- Pakistani\",\"Chinese\")==1\nreplace ethnic=\"MID\" if ethnicPulse==\"Arab\"\ngen ethnic_NA=ethnic\nreplace ethnic_NA=\"NA\" if inlist(ethnic,\"EUR\",\"EAS\",\"MID\")==0\ntab ethnic SARS_CoV, all\ntab ethnic_NA SARS_CoV, all\ngen FID=0\nrename ID IID\noutsheet FID IID ethnic using ethnic.txt if IID!=., noquote replace\noutsheet FID IID ethnic_NA using ethnic_NA.txt if IID!=., noquote replace\nrm INTERVAL-omics-covid.dta.gz\n```\n","dir":"/applications/","name":"pca_projection.md","path":"applications/pca_projection.md","url":"/applications/pca_projection.html"},{"sort":25,"layout":"default","title":"PLINK","content":"# PLINK\n\nAs with GCTA, we cover direct download, source compilation but provision of detailed screen summary which can serve for lookup.\n\n## Executables\n\nDownload from [https://www.cog-genomics.org/plink/1.9/](https://www.cog-genomics.org/plink/1.9/) or [https://www.cog-genomics.org/plink/2.0/](https://www.cog-genomics.org/plink/2.0/).\n\n## Sources\n\nThis is useful for obtaining the latest version.\n\n### 1.9\n\nThe stumbling point is zlib and it suffices with 1.2.11 on csd3. The location of `zlib.h`, i.e., `/usr/local/Cluster-Apps/zlib/1.2.11/include/zlib.h`, needs to be changed with `plink_common.h` and `pigz.c`. It also requires explicit with the whereabout of `libcblas.a` in our case at `${HPC_WORK}/lib`. One can follows instructions in `plink_first_compile` to download and install `zlib-1.2.11` but it is preferable to create a symbolic link.\n\nPutting together, we have\n\n```bash\nln -s /usr/local/Cluster-Apps/zlib/1.2.11 zlib-1.2.11\ncd 1.9\n# edit pigz.c and plink_common.h for ../zlib-1.2.11/include/zlib.h\nmake\n/usr/local/software/archive/linux-scientific7-x86_64/gcc-9/gcc-6.5.0-dtb6lagchexqdijlx6xgkin3zlfddpzi/bin/g++ plink.o plink_assoc.o plink_calc.o plink_cluster.o plink_cnv.o plink_common.o plink_data.o plink_dosage.o plink_family.o plink_filter.o plink_glm.o plink_help.o plink_homozyg.o plink_lasso.o plink_ld.o plink_matrix.o plink_misc.o plink_perm.o plink_rserve.o plink_set.o plink_stats.o SFMT.o dcdflib.o pigz.o yarn.o Rconnection.o hfile.o bgzf.o  -L/usr/lib64/atlas -llapack -lblas -lcblas -latlas -lm -lpthread -ldl -L/usr/local/Cluster-Apps/zlib/1.2.11/lib/libz.so.1.2.11 -L$HPC_WORK/lib64 -o plink\n```\n\nNote that the last line is duplicated adding `-L/usr/local/Cluster-Apps/zlib/1.2.11/lib/libz.so.1.2.11 -L$HPC_WORK/lib64`.\n\n### 2.0\n\nAssume that LAPACK (in particular `libcblas.a`) is available.\n\nIt is necessary to modify `include/plink2_zstfile.h` to point to the whereabout of \\<zstd.h\\>, say \"../zstd/lib/zstd.h\" or \"/rds/user/jhz22/hpc-work/include/zstd.h\".\nIt still requires addition of -L${HPC_WORK}/lib64 for `libcblas.a`, which can be done by pasting and annexing to the last command from the console.\n\n```bash\nmodule load zlib/1.2.11\ngit clone https://github.com/chrchang/plink-ng\ncd plink-ng/2.0\nbuild.sh netlib\nmake\n/usr/local/software/archive/linux-scientific7-x86_64/gcc-9/gcc-6.5.0-dtb6lagchexqdijlx6xgkin3zlfddpzi/bin/g++ include/SFMT.o libdeflate/lib/adler32.o libdeflate/lib/crc32.o libdeflate/lib/deflate_compress.o libdeflate/lib/deflate_decompress.o libdeflate/lib/gzip_compress.o libdeflate/lib/gzip_decompress.o libdeflate/lib/utils.o libdeflate/lib/zlib_compress.o libdeflate/lib/zlib_decompress.o libdeflate/lib/arm/arm_cpu_features.o libdeflate/lib/x86/x86_cpu_features.o zstd/lib/common/debug.o zstd/lib/common/entropy_common.o zstd/lib/common/zstd_common.o zstd/lib/common/error_private.o zstd/lib/common/xxhash.o zstd/lib/common/fse_decompress.o zstd/lib/common/pool.o zstd/lib/common/threading.o zstd/lib/compress/fse_compress.o zstd/lib/compress/hist.o zstd/lib/compress/huf_compress.o zstd/lib/compress/zstd_double_fast.o zstd/lib/compress/zstd_fast.o zstd/lib/compress/zstd_lazy.o zstd/lib/compress/zstd_ldm.o zstd/lib/compress/zstd_opt.o zstd/lib/compress/zstd_compress.o zstd/lib/compress/zstd_compress_literals.o zstd/lib/compress/zstd_compress_sequences.o zstd/lib/compress/zstd_compress_superblock.o zstd/lib/compress/zstdmt_compress.o zstd/lib/decompress/huf_decompress.o zstd/lib/decompress/zstd_decompress.o zstd/lib/decompress/zstd_ddict.o zstd/lib/decompress/zstd_decompress_block.o include/plink2_base.o include/plink2_bits.o include/pgenlib_misc.o include/pgenlib_read.o include/pgenlib_write.o include/plink2_bgzf.o include/plink2_stats.o include/plink2_string.o include/plink2_text.o include/plink2_thread.o include/plink2_zstfile.o plink2.o plink2_adjust.o plink2_cmdline.o plink2_common.o plink2_compress_stream.o plink2_data.o plink2_decompress.o plink2_export.o plink2_fasta.o plink2_filter.o plink2_glm.o plink2_help.o plink2_import.o plink2_ld.o plink2_matrix.o plink2_matrix_calc.o plink2_merge.o plink2_misc.o plink2_psam.o plink2_pvar.o plink2_random.o plink2_set.o -o bin/plink2  -llapack -lcblas -lblas -llapack -lcblas -lblas -lm -lpthread -lz -L$HPC_WORK/lib64\n```\n\nand we have from `bin/plink2 -help`\n\n```\nPLINK v2.00a3 AVX2 (25 Oct 2021)               www.cog-genomics.org/plink/2.0/\n(C) 2005-2021 Shaun Purcell, Christopher Chang   GNU General Public License v3\n\nIn the command line flag definitions that follow,\n  * <angle brackets> denote a required parameter, where the text between the\n    angle brackets describes its nature.\n  * ['square brackets + single-quotes'] denotes an optional modifier.  Use the\n    EXACT text in the quotes.\n  * [{bar|separated|braced|bracketed|values}] denotes a collection of mutually\n    exclusive optional modifiers (again, the exact text must be used).  When\n    there are no outer square brackets, one of the choices must be selected.\n  * ['quoted_text='<description of value>] denotes an optional modifier that\n    must begin with the quoted text, and be followed by a value with no\n    whitespace in between.  '|' may also be used here to indicate mutually\n    exclusive options.\n  * [square brackets without quotes or braces] denote an optional parameter,\n    where the text between the brackets describes its nature.\n  * An ellipsis (...) indicates that you may enter multiple arguments of the\n    specified type.\n  * A \"column set descriptor\" is either\n    1. a comma-separated sequence of column set names; this is interpreted as\n       the full list of column sets to include.\n    2. a comma-separated sequence of column set names, all preceded by '+' or\n       '-'; this is interpreted as a list of changes to the default.\n\n  plink2 <input flag(s)...> [command flag(s)...] [other flag(s)...]\n  plink2 --help [flag name(s)...]\n\nMost PLINK runs require exactly one main input fileset.  The following flags\nare available for defining its form and location:\n\n  --pfile fix> ['vzs']  : Specify .pgen + .pvar[.zst] + .psam prefix.\n  --pgen <filename>         : Specify full name of .pgen/.bed file.\n  --pvar <filename>         : Specify full name of .pvar/.bim file.\n  --psam <filename>         : Specify full name of .psam/.fam file.\n\n  --bfile  fix> ['vzs'] : Specify .bed + .bim[.zst] + .fam prefix.\n  --bpfile fix> ['vzs'] : Specify .pgen + .bim[.zst] + .fam prefix.\n\n  --keep-autoconv ['vzs']   : When importing non-PLINK-binary data, don't\n                              delete autogenerated fileset at end of run.\n\n  --no-fid           : .fam file does not contain column 1 (family ID).\n  --no-parents       : .fam file does not contain columns 3-4 (parents).\n  --no-sex           : .fam file does not contain column 5 (sex).\n\n  --vcf <filename> ['dosage='<field>]\n  --bcf <filename> ['dosage='<field>] :\n    Specify full name of .vcf{|.gz|.zst} or BCF2 file to import.\n    * These can be used with --psam/--fam.\n    * By default, dosage information is not imported.  To import the GP field\n      (must be VCFv4.3-style 0..1, one probability per possible genotype), add\n      'dosage=GP' (or 'dosage=GP-force', see below).  To import Minimac3-style\n      DS+HDS phased dosage, add 'dosage=HDS'.  'dosage=DS' (or anything else\n      for now) causes the named field to be interpreted as a Minimac3-style\n      dosage.\n      Note that, in the dosage=GP case, PLINK 2 collapses the probabilities\n      down to dosages; you cannot use PLINK 2 to losslessly convert VCF\n      FORMAT/GP data to e.g. BGEN format.  To make this more obvious, PLINK 2\n      now errors out when dosage=GP is used on a file with a FORMAT/DS header\n      line and --import-dosage-certainty wasn't specified, since dosage=DS\n      extracts the same information more quickly in this situation.  You can\n      suppress this error with 'dosage=GP-force'.\n      In all of these cases, hardcalls are regenerated from scratch from the\n      dosages.  As a consequence, variants with no GT field can now be\n      imported; they will be assumed to contain only diploid calls when HDS is\n      also absent.\n\n  --data <filename prefix> <REF/ALT mode> ['gzs']\n  --bgen <filename> <REF/ALT mode> ['snpid-chr']\n  --gen <filename> <REF/ALT mode>\n  --sample <filename> :\n    Specify an Oxford-format dataset to import.  --data specifies a .gen[.zst]\n    + .sample pair, while --bgen specifies a BGEN v1.1+ file.\n    * If a BGEN v1.2+ file contains sample IDs, it may be imported without a\n      companion .sample file.\n    * With 'snpid-chr', chromosome codes are read from the 'SNP ID' field\n      instead of the usual chromosome field.\n    * The following REF/ALT modes are supported:\n      'ref-first': The first allele for each variant is REF.\n      'ref-last': The last allele for each variant is REF.\n      'ref-unknown': The last allele for each variant is treated as\n                     provisional-REF.\n\n  --haps <filename> [{ref-first | ref-last}]\n  --legend <filename> <chr code> :\n    Specify .haps [+ .legend] file(s) to import.\n    * When --legend is specified, it's assumed that the --haps file doesn't\n      contain header columns.\n    * On chrX, the second male column may contain dummy '-' entries.  (However,\n      PLINK 2 currently cannot handle omitted male columns.)\n    * If not used with --sample, new sample IDs are of the form 'per#/per#'.\n\n  --map <filename>   : Specify full name of .map file.\n  --import-dosage <allele dosage file> ['noheader'] ['id-delim='<char>]\n                  ['skip0='<i>] ['skip1='<j>] ['skip2='<k>] ['dose1']\n                  ['format='<m>] [{ref-first | ref-last}]\n                  ['single-chr='<code>] ['chr-col-num='<#>]\n                  ['pos-col-num='<#>] :\n    Specify PLINK 1.x-style dosage file to import.\n    * You must also specify a companion .psam/.fam file.\n    * By default, PLINK assumes that the file contains a header line, which has\n      'SNP' in (1-based) column i+1, 'A1' in column i+j+2, 'A2' in column\n      i+j+3, and sample FID/IIDs starting from column i+j+k+4.  (i/j/k are\n      normally zero, but can be changed with 'skip0', 'skip1', and 'skip2'\n      respectively.  FID/IID are normally assumed to be separate tokens, but if\n      they're merged into a single token you can specify the delimiter with\n      'id-delim='.)  If such a header line is not present, use the 'noheader'\n      modifier; samples will then be assumed to appear in the same order as\n      they do in the .psam/.fam file.\n    * You may specify a companion .map file.  If you do not,\n      * 'single-chr=' can be used to specify that all variants are on the named\n        chromosome.  Otherwise, you can use 'chr-col-num=' to read chromosome\n        codes from the given (1-based) column number.\n      * 'pos-col-num=' causes bp coordinates to be read from the given column\n        number.\n    * The 'format=' modifier lets you specify the number of values used to\n      represent each dosage.  'format=1' normally indicates a single 0..2 A1\n      expected count; 'dose1' modifies this to a 0..1 frequency.  'format=2'\n      indicates a 0..1 homozygous A1 likelihood followed by a 0..1 het\n      likelihood.  'format=3' indicates 0..1 hom A1, 0..1 het, 0..1 hom A2.\n      'format=infer' (the default) infers the format from the number of columns\n      in the first nonheader line.\n\n  --dummy <sample ct> <SNP ct> [missing dosage freq] [missing pheno freq]\n          [{acgt | 1234 | 12}] ['pheno-ct='<count>] ['scalar-pheno']\n          ['phase-freq='<rate>] ['dosage-freq='<rate>]\n    This generates a fake input dataset with the specified number of samples\n    and SNPs.\n    * By default, the missing dosage and phenotype frequencies are zero.\n      These can be changed by providing 3rd and 4th numeric arguments.\n    * By default, allele codes are As and Bs; this can be changed with the\n      'acgt', '1234', or '12' modifier.\n    * By default, one binary phenotype is generated.  'pheno-ct=' can be used\n      to change the number of phenotypes, and 'scalar-pheno' causes these\n      phenotypes to be normally distributed scalars.\n    * By default, all genotypes/dosages are unphased.  To phase some of them,\n      use 'phase-freq='.\n    * By default, all dosages are in {0,1,2}.  To make some of them take on\n      decimal values, use 'dosage-freq='.  (These dosages are affected by\n      --hard-call-threshold and --dosage-erase-threshold.)\n\n  --fa <filename>    : Specify full name of reference FASTA file.\n\nOutput files have names of the form 'plink2.<extension>' by default.  You can\nchange the 'plink2' prefix with\n\n  --out fix>     : Specify prefix for output files.\n\nMost runs also require at least one of the following commands:\n\n  --rm-dup [mode] ['list']\n    Remove all but one instance of each duplicate-ID variant (ignoring the\n    missing ID), and (with the 'list' modifier) write a list of duplicated IDs\n    to <output prefix>.rmdup.list.\n    The following modes of operation are supported:\n    * 'error' (default) causes this to error out when there's a genotype data\n      or other mismatch between the records.  A list of affected IDs is written\n      to <output prefix>.rmdup.mismatch.\n    * 'retain-mismatch' causes all instances of a duplicate-ID variant to be\n      retained when there's a genotype data or variant info mismatch; otherwise\n      one instance is kept.  The .rmdup.mismatch file is also written.\n    * 'exclude-mismatch' removes all instances of duplicate-ID mismatched\n      variants instead.\n    * 'exclude-all' causes all instances of duplicate-ID variants to be\n      removed, even when the actual records are identical.\n    * 'force-first' causes only the first instance of duplicate-ID variants to\n      be kept, under all circumstances.\n\n  --make-pgen ['vzs'] ['format='<code>] ['trim-alts'] ['erase-phase']\n              ['erase-dosage'] ['fill-missing-from-dosage']\n              ['pvar-cols='<col set desc>] ['psam-cols='<col set desc>]\n  --make-bpgen ['vzs'] ['format='<code>] ['trim-alts'] ['erase-phase']\n               ['erase-dosage'] ['fill-missing-from-dosage']\n  --make-bed ['vzs'] ['trim-alts']\n    Create a new PLINK 2 binary fileset (--make-pgen = .pgen + .pvar[.zst] +\n    .psam, --make-bpgen = .pgen + .bim[.zst] + .fam).\n    * Unlike the automatic text-to-binary converters (which only heed\n      chromosome filters), this supports all of PLINK's filtering flags.\n    * The 'vzs' modifier causes the variant file (.pvar/.bim) to be\n      Zstd-compressed.\n    * The 'format' modifier requests an uncompressed fixed-variant-width .pgen\n      file.  (These do not directly support multiallelic variants.)  The\n      following format code is currently supported:\n        2: just like .bed, except with an extended (12-byte instead of 3-byte)\n           header containing variant/sample counts, and rotated genotype codes\n           (00 = hom ref, 01 = het, 10 = hom alt, 11 = missing).\n    * The 'erase-phase' and 'erase-dosage' modifiers prevent phase and dosage\n      information from being written to the new .pgen.\n    * When a hardcall is missing but the corresponding dosage is present,\n      'fill-missing-from-dosage' causes the (Euclidean-)nearest hardcall to be\n      filled in, with ties broken in favor of the lower-index allele.\n    * The first five columns of a .pvar file are always #CHROM/POS/ID/REF/ALT.\n      Supported optional .pvar column sets are:\n        xheader: All pre-#CHROM header lines (yeah, this is technically not a\n                 column), except for possibly FILTER/INFO definitions when\n                 those column(s) have been removed.  Without this, only the\n                 #CHROM header line is kept.\n        vcfheader: xheader, with additions to make it a valid VCF header.\n        maybequal: QUAL.  Omitted if all remaining values are missing.\n        qual: Force QUAL column to be written even when empty.\n        maybefilter: FILTER.  Omitted if all remaining values are missing.\n        filter: Force FILTER column to be written even when empty.\n        maybeinfo: INFO.  Omitted if all remaining values are missing, or if\n                   INFO/PR is the only subfield.\n        info: Force INFO column to be written.\n        maybecm: Centimorgan coordinate.  Omitted if all remaining values = 0.\n        cm: Force CM column to be written even when empty.\n      The default is xheader,maybequal,maybefilter,maybeinfo,maybecm.\n    * Supported column sets for the .psam file are:\n        maybefid: Family ID, '0' = missing.  Omitted if all values missing.\n        fid: Force FID column to be written even when empty.\n        maybesid: Source ID, '0' = missing.  Omitted if all values missing.\n        sid: Force SID column to be written even when empty.\n        maybeparents: Father and mother IIDs.  Omitted if all values missing.\n        parents: Force PAT and MAT columns to be written even when empty.\n        sex: '1' = male, '2' = female, 'NA' = missing.\n        pheno1: First active phenotype.  If none, all column entries are set to\n                the --output-missing-phenotype string.\n        phenos: All active phenotypes, if any.  (Can be combined with pheno1 to\n                force at least one phenotype column to be written.)\n      The default is maybefid,maybesid,maybeparents,sex,phenos.\n\n  --make-just-pvar ['zs'] ['cols='<column set descriptor>]\n  --make-just-psam ['cols='<column set descriptor>]\n  --make-just-bim ['zs']\n  --make-just-fam\n    Variants of --make-pgen/--make-bed which only write a new variant or sample\n    file.  These don't always require an input genotype file.\n    USE THESE CAUTIOUSLY.  It is very easy to desynchronize your binary\n    genotype data and your sample/variant indexes if you use these commands\n    improperly.  If you have any doubt, stick with --make-[b]pgen/--make-bed.\n\n  --export <output format(s)...> [{01 | 12}] ['bgz'] ['id-delim='<char>]\n           ['id-paste='<column set descriptor>] ['include-alt']\n           ['omit-nonmale-y'] ['spaces'] ['vcf-dosage='<field>] ['ref-first']\n           ['bits='<#>] ['sample-v2']\n    Create a new fileset with all filters applied.  The following output\n    formats are supported:\n    (actually, only A, AD, A-transpose, bcf, bgen-1.x, haps, hapslegend,\n    ind-major-bed, oxford, and vcf are implemented for now)\n    * '23': 23andMe 4-column format.  This can only be used on a single\n            sample's data (--keep may be handy), and does not support\n            multicharacter allele codes.\n    * 'A': Sample-major additive (0/1/2) coding, suitable for loading from R.\n           If you need uncounted alleles to be named in the header line, add\n           the 'include-alt' modifier.\n    * 'AD': Sample-major additive (0/1/2) + dominant (het=1/hom=0) coding.\n            Also supports 'include-alt'.\n    * 'A-transpose': Variant-major 0/1/2.\n    * 'beagle': Unphased per-autosome .dat and .map files, readable by early\n                BEAGLE versions.\n    * 'beagle-nomap': Single .beagle.dat file.\n    * 'bgen-1.x': Oxford-format .bgen + .sample.  For v1.2/v1.3, sample\n                  identifiers are stored in the .bgen (with id-delim and\n                  id-paste settings applied), and default precision is 16-bit\n                  (use the 'bits' modifier to reduce this).\n    * 'bimbam': Regular BIMBAM format.\n    * 'bimbam-1chr': BIMBAM format, with a two-column .pos.txt file.  Does not\n                     support multiple chromosomes.\n    * 'fastphase': Per-chromosome fastPHASE files, with\n                   .chr-<chr #>.phase.inp filename extensions.\n    * 'fastphase-1chr': Single .phase.inp file.  Does not support\n                        multiple chromosomes.\n    * 'haps', 'hapslegend': Oxford-format .haps + .sample[ + .legend].  All\n                            data must be biallelic and phased.  When the 'bgz'\n                            modifier is present, the .haps file is\n                            block-gzipped.\n    * 'HV': Per-chromosome Haploview files, with .chr-<chr #>{.ped,.info}\n            filename extensions.\n    * 'HV-1chr': Single Haploview .ped + .info file pair.  Does not support\n                 multiple chromosomes.\n    * 'ind-major-bed': PLINK 1 sample-major .bed (+ .bim + .fam).\n    * 'lgen': PLINK 1 long-format (.lgen + .fam + .map), loadable with --lfile.\n    * 'lgen-ref': .lgen + .fam + .map + .ref, loadable with --lfile +\n                  --reference.\n    * 'list': Single genotype-based list, up to 4 lines per variant.  To omit\n              nonmale genotypes on the Y chromosome, add the 'omit-nonmale-y'\n              modifier.\n    * 'rlist': .rlist + .fam + .map fileset, where the .rlist file is a\n                genotype-based list which omits the most common genotype for\n                each variant.  Also supports 'omit-nonmale-y'.\n    * 'oxford', 'oxford-v2': Oxford-format .gen + .sample.  When the 'bgz'\n                             modifier is present, the .gen file is\n                             block-gzipped.  'oxford' requests the original\n                             .gen file format with 5 leading columns\n                             (understood by older PLINK builds); 'oxford-v2'\n                             requests the current 6-leading-column flavor.\n    * 'ped': PLINK 1 sample-major (.ped + .map), loadable with --file.\n    * 'compound-genotypes': Same as 'ped', except that the space between each\n                            pair of same-variant allele codes is removed.\n    * 'structure': Structure-format.\n    * 'transpose': PLINK 1 variant-major (.tped + .tfam), loadable with\n                   --tfile.\n    * 'vcf',     : VCF (default version 4.3).  If PAR1 and PAR2 are present,\n      'vcf-4.2',   they are automatically merged with chrX, with proper\n      'bcf',       handling of chromosome codes and male ploidy.\n      'bcf-4.2'    When the 'bgz' modifier is present, the VCF file is\n                   block-gzipped.  (This always happens with BCF output.)\n                   The 'id-paste' modifier controls which .psam columns are\n                   used to construct sample IDs (choices are maybefid, fid,\n                   iid, maybesid, and sid; default is maybefid,iid,maybesid),\n                   while the 'id-delim' modifier sets the character between the\n                   ID pieces (default '_').\n                   Genotypes are always exported.  If you want to export a\n                   sites-only VCF instead, see --make-pgen/--make-just-pvar's\n                   'vcfheader' column set.\n                   Dosages are not exported unless the 'vcf-dosage=' modifier\n                   is present.  The following five dosage export modes are\n                   supported:\n                   'GP': genotype posterior probabilities (v4.3 only).\n                   'DS': Minimac3-style dosages, omitted for hardcalls.\n                   'DS-force': Minimac3-style dosages, never omit.\n                   'HDS': Minimac3-style phased dosages, omitted for hardcalls\n                          and unphased calls.  Also includes 'DS' output.\n                   'HDS-force': Always report DS and HDS.\n    In addition,\n    * The '12' modifier causes alt1 alleles to be coded as '1' and ref alleles\n      to be coded as '2', while '01' maps alt1 -> 0 and ref -> 1.\n    * The 'spaces' modifier makes the output space-delimited instead of\n      tab-delimited, whenever both are permitted.\n    * For biallelic formats where it's unspecified whether the reference/major\n      allele should appear first or second, --export defaults to second for\n      compatibility with PLINK 1.9.  Use 'ref-first' to change this.\n      (Note that this doesn't apply to the 'A', 'AD', and 'A-transpose'\n      formats; use --export-allele to control which alleles are counted there.)\n    * 'sample-v2' exports .sample files according to the QCTOOLv2 rather than\n      the original specification.  Only one ID column is exported ('id-paste'\n      and 'id-delim' settings apply), parental IDs are exported if present, and\n      category names are preserved rather than converted to positive integers.\n\n  --freq ['zs'] ['counts'] ['cols='<column set descriptor>] ['bins-only']\n         ['refbins='<comma-separated bin boundaries> | 'refbins-file='<file>]\n         ['alt1bins='<comma-separated bin boundaries> | 'alt1bins-file='<file>]\n    Empirical allele frequency report.  By default, only founders are\n    considered.  Dosages are taken into account (e.g. heterozygous haploid\n    calls count as 0.5).\n    Supported column sets are:\n      chrom: Chromosome ID.\n      pos: Base-pair coordinate.\n      (ID is always present, and positioned here.)\n      ref: Reference allele.\n      alt1: Alternate allele 1.\n      alt: All alternate alleles, comma-separated.\n      reffreq: Reference allele frequency/dosage.\n      alt1freq: Alt1 frequency/dosage.\n      altfreq: Comma-separated frequencies/dosages for all alternate alleles.\n      freq: Similar to altfreq, except ref is also included at the start.\n      eq: Comma-separated <allele>=<freq> for all present alleles.  (If no\n          alleles are present, the column contains a single '.'.)\n      eqz: Same as eq, except zero-counts are included.\n      alteq/alteqz: Same as eq/eqz, except reference allele is omitted.\n      numeq: 0=<freq>,1=<freq>, etc.  Zero-counts are omitted.\n      altnumeq: Same as numeq, except reference allele is omitted.\n      machr2: Unphased MaCH imputation quality metric.\n      minimac3r2: Phased Minimac3 imputation quality.\n      nobs: Number of allele observations.\n    The default is chrom,ref,alt,altfreq,nobs.\n    Additional .afreq.{ref,alt1}.bins (or .acount.{ref,alt1}.bins with\n    'counts') file(s) are generated when 'refbins='/'refbins-file=' or\n    'alt1bins='/'alt1bins-file=' is present; these report the total number of\n    frequencies or counts in each left-closed, right-open interval.  (If you\n    only want these histogram(s), and not the main report, add 'bins-only'.)\n\n  --geno-counts ['zs'] ['cols='<column set descriptor>]\n    Variant-based hardcall genotype count report (considering both alleles\n    simultaneously in the diploid case).  Nonfounders are now included; use\n    --keep-founders if this is a problem.  Heterozygous haploid calls are\n    treated as missing.\n    Supported column sets are:\n      chrom: Chromosome ID.\n      pos: Base-pair coordinate.\n      (ID is always present, and positioned here.)\n      ref: Reference allele.\n      alt1: Alternate allele 1.\n      alt: All alternate alleles, comma-separated.\n      homref: Homozygous-ref count.\n      refalt1: Heterozygous ref-alt1 count.\n      refalt: Comma-separated het ref-altx counts.\n      homalt1: Homozygous-alt1 count.\n      altxy: Comma-separated altx-alty counts, in (1/1)-(1/2)-(2/2)-(1/3)-...\n             order.\n      xy: Similar to altxy, except the reference allele is treated as alt0,\n          and the sequence starts (0/0)-(0/1)-(1/1)-(0/2)-...\n      hapref: Haploid-ref count.\n      hapalt1: Haploid-alt1 count.\n      hapalt: Comma-separated haploid-altx counts.\n      hap: Similar to hapalts, except ref is also included at the start.\n      numeq: 0/0=<hom ref ct>,0/1=<het ref-alt1>,1/1=<hom alt1>,...,0=<hap ref>\n             etc.  Zero-counts are omitted.  (If all genotypes are missing, the\n             column contains a single '.'.)\n      missing: Number of missing genotypes.\n      nobs: Number of (nonmissing) genotype observations.\n    The default is chrom,ref,alt,homref,refalt,altxy,hapref,hapalt,missing.\n\n  --sample-counts ['zs'] ['cols='<column set descriptor>]\n    Sample-based hardcall genotype count report.\n    * Unknown-sex samples are treated as female.\n    * Heterozygous haploid calls (MT included) are treated as missing.\n    * As with other PLINK 2 commands, SNPs that have not been left-normalized\n      are counted as non-SNP non-symbolic.  (Use e.g. --normalize when that's a\n      problem.)\n    * Supported column sets are:\n        maybefid: FID, if that column was present in the input.\n        fid: Force FID column to be written even when absent in the input.\n        (IID is always present, and positioned here.)\n        maybesid: SID, if that column was present in the input.\n        sid: Force SID column to be written even when absent in the input.\n        sex: '1' = male, '2' = female, 'NA' = missing.\n        hom: Homozygous genotype count.\n        homref: Homozygous-ref genotype count.\n        homalt: Homozygous-alt genotype count.\n        homaltsnp: Homozygous-alt SNP count.\n        het: Heterozygous genotype count.\n        refalt: Heterozygous ref-altx genotype count.\n        het2alt: Heterozygous altx-alty genotype count.\n        hetsnp: Heterozygous SNP count.\n        dipts: Diploid SNP transition count.\n        ts: SNP transition count (excluding chrY for females).\n        diptv: Diploid SNP transversion count.\n        tv: SNP transversion count.\n        dipnonsnpsymb: Diploid non-SNP, non-symbolic count.\n        nonsnpsymb: Non-SNP, non-symbolic count.\n        symbolic: Symbolic variant count.\n        nonsnp: Non-SNP count.\n        dipsingle: Number of singletons relative to this dataset, across just\n                   diploid calls.  (Note that if the ALT allele in a chrX\n                   biallelic variant appears in exactly one female and one\n                   male, that counts as a singleton for just the female.)\n        single: Number of singletons relative to this dataset.\n        haprefwfemaley: Haploid-ref count, counting chrY for everyone.\n        hapref: Haploid-ref count, excluding chrY for females.\n        hapaltwfemaley: Haploid-alt count, counting chrY for everyone.\n        hapalt: Haploid-alt count, excluding chrY for females.\n        missingwfemaley: Missing call count, counting chrY for everyone.\n        missing: Missing call count, excluding chrY for females.\n      The default is maybefid,maybesid,homref,homaltsnp,hetsnp,dipts,diptv,\n      dipnonsnpsymb,dipsingle,haprefwfemaley,hapaltwfemaley,missingwfemaley.\n    * The 'hetsnp', 'dipts'/'ts'/'diptv'/'tv', 'dipnonsnpsymb'/'nonsnpsymb',\n      'symbolic', and 'nonsnp' columns count each ALT allele in a heterozygous\n      altx-alty call separately, since they can be of different subtypes.\n      (I.e. if they are of the same subtype, the corresponding count is\n      incremented by 2.)  As a consequence, these columns are unaffected by\n      variant split/join.\n\n  --missing ['zs'] [{sample-only | variant-only}]\n            ['scols='<column set descriptor>] ['vcols='<column set descriptor>]\n    Generate sample- and variant-based missing data reports (or just one report\n    if 'sample-only'/'variant-only' is specified).\n    Mixed MT hardcalls appear in the heterozygous haploid stats.\n    Supported column sets in the sample-based report are:\n      maybefid: FID, if that column was present in the input.\n      fid: Force FID column to be written even when absent in the input.\n      (IID is always present, and positioned here.)\n      maybesid: SID, if that column was present in the input.\n      sid: Force SID column to be written even when absent in the input.\n      misspheno1: First active phenotype missing (Y/N)?  Always 'Y' if no\n                  phenotypes are loaded.\n      missphenos: A Y/N column for each loaded phenotype.  (Can be combined\n                  with misspheno1 to force at least one such column.)\n      nmissdosage: Number of missing dosages.\n      nmiss: Number of missing hardcalls, not counting het haploids.\n      nmisshh: Number of missing hardcalls, counting het haploids.\n      hethap: Number of heterozygous haploid hardcalls.\n      nobs: Denominator (male count on chrY, otherwise total sample count).\n      fmissdosage: Missing dosage rate.\n      fmiss: Missing hardcall rate, not counting het haploids.\n      fmisshh: Missing hardcall rate, counting het haploids.\n    The default is maybefid,maybesid,missphenos,nmiss,nobs,fmiss.\n    Supported column sets in the variant-based report are:\n      chrom: Chromosome ID.\n      pos: Base-pair coordinate.\n      (ID is always present, and positioned here.)\n      ref: Reference allele.\n      alt1: Alternate allele 1.\n      alt: All alternate alleles, comma-separated.\n      nmissdosage: Number of missing dosages.\n      nmiss: Number of missing hardcalls, not counting het haploids.\n      nmisshh: Number of missing hardcalls, counting het haploids.\n      hethap: Number of heterozygous haploid calls.\n      nobs: Number of potentially valid calls.\n      fmissdosage: Missing dosage rate.\n      fmiss: Missing hardcall rate, not counting het haploids.\n      fmisshh: Missing hardcall rate, counting het haploids.\n      fhethap: Heterozygous haploid rate.\n    The default is chrom,nmiss,nobs,fmiss.\n\n  --hardy ['zs'] ['midp'] ['redundant'] ['cols='<column set descriptor>]\n    Hardy-Weinberg exact test p-value report(s).\n    * By default, only founders are considered; change this with --nonfounders.\n    * chrX is now omitted from the main <output prefix>.hardy report.  Instead,\n      (if present) it gets its own <output prefix>.hardy.x report based on the\n      method described in Graffelman J, Weir BS (2016) Hardy-Weinberg\n      equilibrium and the X chromosome.\n    * For variants with k alleles where k>2, k separate 'biallelic' tests are\n      performed, each reported on its own line.  However, biallelic variants\n      are normally reported on a single line, since the counts/frequencies\n      would be mirror-images and the p-values would be the same.  You can add\n      the 'redundant' modifier to force biallelic variant results to be\n      reported on two lines for parsing convenience.\n    * There is currently no special handling of case/control phenotypes.\n    Supported column sets are:\n      chrom: Chromosome ID.\n      pos: Base-pair coordinate.\n      (ID is always present, and positioned here.)\n      ref: Reference allele.\n      alt1: Alternate allele 1.\n      alt: All alternate alleles, comma-separated.\n      (A1 is always present, and positioned here.)\n      ax: Non-A1 allele(s), comma-separated.\n      gcounts: Hom-A1 count, total number of het-A1 calls, and total number of\n               nonmissing calls with no copies of A1.  On chrX, these are\n               followed by male A1 and male non-A1 counts.\n      gcount1col: gcounts values in a single comma-separated column.\n      hetfreq: Observed and expected het-A1 frequencies.\n      sexaf: Female and male A1 observed allele frequencies (chrX only).\n      femalep: Female-only p/midp-value (chrX only).\n      p: Hardy-Weinberg equilibrium exact test p/midp-value.\n    The default is chrom,ax,gcounts,hetfreq,sexaf,p.\n\n  --het ['zs'] ['small-sample'] ['cols='<column set descriptor>]\n    Inbreeding coefficient report.  Supports multiallelic variants.\n    * This function requires decent MAF estimates.  If there are very few\n      samples in your immediate fileset, --read-freq is practically mandatory\n      since imputed MAFs are wildly inaccurate in that case.\n    * It's usually best to perform this calculation on a variant set in\n      approximate linkage equilibrium.\n    * By default, --het omits the n/(n-1) multiplier in Nei's expected\n      homozygosity formula.  The 'small-sample' modifier causes it to be\n      included, while forcing --het to use MAFs imputed from founders in the\n      immediate dataset.\n    Supported column sets are:\n      maybefid: FID, if that column was present in the input.\n      fid: Force FID column to be written even when absent in the input.\n      (IID is always present, and positioned here.)\n      maybesid: SID, if that column was present in the input.\n      sid: Force SID column to be written even when absent in the input.\n      hom: Observed and expected numbers of homozygous genotypes.\n      het: Observed and expected numbers of heterozygous genotypes.\n      nobs: Number of (nonmissing) genotype observations.\n      f: Method-of-moments F coefficient estimate.\n    The default is maybefid,maybesid,hom,nobs,f.\n\n  --fst <categorical or binary phenotype name> ['method='<method name>]\n        ['blocksize='<jackknife block size>] ['cols='<column set descriptor>]\n        ['report-variants'] ['zs'] ['vcols='<column set descriptor>]\n        ['base='<pop. ID> | 'ids='<pop. ID> | 'file='<pop.-ID-pair file>]\n        [other population ID(s) for base=/ids=...]\n    Estimate Wright's Fst between pairs of populations.\n    * Two methods are supported:\n      * 'hudson': Bhatia G, Patterson N, Sankararaman S, Price AL (2013)\n                  Estimating and interpreting F_{ST}: The impact of rare\n                  variants.  This is now the default.\n      * 'wc': Weir BS, Cockerham CC (1984) Estimating F-statistics for the\n              analysis of population structure.\n    * To get block-jackknife-based standard error estimates, provide a\n      blocksize= value.\n    * There is only one optional column set in the main summary:\n        (POP1 and POP2 are always present, and positioned here.)\n        nobs: Number of variants with valid Fst estimates.\n        (HUDSON_FST or WC_FST is always present, and positioned here.)\n        (SE is present if blocksize= specified, and positioned here.)\n      nobs is not included by default.\n    * You can request per-variant Fst estimates with the 'report-variants'\n      modifier; this generates a separate output file for each population pair.\n      Supported per-variant report column sets are:\n        chrom: Chromosome ID.\n        pos: Base-pair coordinate.\n        (ID is always present, and positioned here.)\n        ref: Reference allele.\n        alt: All alternate alleles, comma-separated.\n        nobs: Number of (nonmissing) genotype observations across pop pair.\n        nallele: Number of nonmissing alleles.\n        fstfrac: Numerator and denominator of Fst estimate.\n        fst: Fst estimate.\n      The default is chrom,pos,nobs,fst.\n    * By default, all pairs of populations are compared.  If you only want to\n      compare some pairs, there are three ways to do this.\n      * base= specifies one base population to be compared with all others; or\n        if you specify more population ID(s) afterward, just the other\n        populations you've listed.\n      * ids= specifies an all-vs.-all comparison within the given set of\n        populations.\n      * file= specifies a file containing one population pair per line.\n      Note that 'base='/'ids='/'file=' must be positioned after all modifiers.\n\n  --indep-pairwise <window size>['kb'] [step size (variant ct)]\n                   <unphased-hardcall-r^2 threshold>\n    Generate a list of variants in approximate linkage equilibrium.\n    * For multiallelic variants, major allele counts are used in the r^2\n      computation.\n    * With the 'kb' modifier, the window size is in kilobase instead of variant\n      count units.  (Pre-'kb' space is optional, i.e.\n      \"--indep-pairwise 500 kb 0.5\" and \"--indep-pairwise 500kb 0.5\" have the\n      same effect.)\n    * The step size now defaults to 1 if it's unspecified, and *must* be 1 if\n      the window is in kilobase units.\n    * Note that you need to rerun PLINK using --extract or --exclude on the\n      .prune.in/.prune.out file to apply the list to another computation... and\n      as with other applications of --extract/--exclude, duplicate variant IDs\n      are a problem.  --indep-pairwise still runs to completion for now when\n      duplicate variant IDs are present, but that will become an error in alpha\n      3.\n\n  --ld <variant ID> <variant ID> ['dosage'] ['hwe-midp']\n    This displays diplotype frequencies, r^2, and D' for a single pair of\n    variants.\n    * For multiallelic variants, major allele counts/dosages are used.\n    * Phase information is used when both variants are on the same chromosome.\n    * When there is at least one sample with unphased het calls for both\n      variants, diplotype frequencies are estimated using the Hill equation.\n      If there are multiple biologically possible local maxima, all are\n      displayed, along with HWE exact test statistics.\n    * By default, only hardcalls are considered.  Add the 'dosage' modifier if\n      you want dosages to be taken into account.  (In the diploid case, an\n      unphased dosage of x is interpreted as P(0/0) = 1 - x, P(0/1) = x when x\n      is in 0..1.)\n\n  --sample-diff ['id-delim='<char>] ['dosage' | 'dosage='<tolerance>]\n                ['include-missing'] [{pairwise | counts-only}]\n                ['fname-id-delim='<c>] ['zs'] ['cols='<column set descriptor>]\n                ['counts-cols='<column set descriptor>]\n                {base= | ids=}<sample ID> <other sample ID(s)...>\n  --sample-diff ['id-delim='<char>] ['dosage' | 'dosage='<tolerance>]\n                ['include-missing'] [{pairwise | counts-only}]\n                ['fname-id-delim='<c>] ['zs'] ['cols='<column set descriptor>]\n                ['counts-cols='<column set descriptor>] file=<ID-pair file>\n    (alias: --sdiff)\n    Report discordances and discordance-counts between pairs of samples.  If\n    chrX or chrY is present, sex must be defined and consistent.\n    * There are three ways to specify which sample pairs to compare.  To\n      compare a single baseline sample against some others, start the\n      (space-delimited) sample ID list with 'base='.  To perform an all-vs.-all\n      comparison, start it with 'ids=' instead.  To compare sample pairs listed\n      in a file, use 'file='.\n      Note that 'base='/'ids='/'file=' must be positioned after all modifiers.\n    * Sample IDs are interpreted as if they were in a VCF header line, with\n      'id-delim=' having the usual effect.\n    * By default, comparisons are based on hardcalls.  Use 'dosage' to compare\n      dosages instead; you can combine this with a tolerance in [0, 0.5).\n    * By default, if one genotype is missing and the other isn't, that doesn't\n      count as a difference; this can be changed with 'include-missing'.\n    * By default, a single main report is written to\n      <output prefix>[.<base ID>].sdiff.  To write separate pairwise\n      <output prefix>.<ID1>.<ID2>.sdiff reports for each compared ID pair, add\n      the 'pairwise' modifier.  To omit the main report, add the 'counts-only'\n      modifier.  (Note that, if you're only interested in nonmissing autosomal\n      biallelic hardcalls, --make-king-table provides a more efficient way to\n      compute just counts.)\n    * By default, if an output filename has a multipart sample ID, the parts\n      will be delimited by '_'; use 'fname-id-delim=' to change this.\n    Supported main-report column sets are:\n      chrom: Chromosome ID.\n      pos: Base-pair coordinate.\n      (Variant ID is always present, and positioned here.)\n      ref: Reference allele.\n      alt: All alternate alleles, comma-separated.\n      maybefid: FID1/FID2, if that column was in the input.  Requires 'id'.\n      fid: Force FID1/FID2 even when FID was absent in the input.\n      id: IID1/IID2.\n      maybesid: SID1/SID2, if that column was in the input.  Requires 'id'.\n      sid: Force SID1/SID2 even when SID was absent in the input.\n      geno: Unphased GT or DS for the two samples.\n    The default is usually chrom,pos,ref,alt,maybefid,id,maybesid,geno; the\n    sample IDs are removed from the default in 'pairwise' mode.\n    Supported discordance-count-summary column sets are:\n      maybefid: FID1/FID2, if that column was in the input.\n      fid: Force FID1/FID2 even when FID was absent in the input.\n      (IID1/IID2 are always present.)\n      maybesid: SID1/SID2, if that column was in the input.\n      sid: Force SID1/SID2 even when SID was absent in the input.\n      nobs: Number of variants considered.  This includes variants where one or\n            both variants are missing iff 'include-missing' was specified.\n      nobsibs: ibs0+ibs1+ibs2.\n      ibs0: Number of diploid variants with no common hardcall alleles.\n      ibs1: Number of diploid variants with exactly 1 common hardcall allele.\n      ibs2: Number of diploid variants with both hardcall alleles matching.\n      halfmiss: Number of variants with exactly 1 missing genotype/dosage.\n                Ignored without 'include-missing'.\n      diff: Total number of differences.\n    The default is maybefid,maybesid,nobs,halfmiss,diff.\n\n  --make-king [{square | square0 | triangle}] [{zs | bin | bin4}]\n    KING-robust kinship estimator, described by Manichaikul A, Mychaleckyj JC,\n    Rich SS, Daly K, Sale M, Chen WM (2010) Robust relationship inference in\n    genome-wide association studies.  By default, this writes a\n    lower-triangular tab-delimited table of kinship coefficients to\n    <output prefix>.king, and a list of the corresponding sample IDs to\n    <output prefix>.king.id.  The first row of the .king file contains a single\n    <genome 1-genome 2> kinship coefficient, the second row has the\n    <genome 1-genome 3> and <genome 2-genome 3> kinship values in that order,\n    etc.\n    * Only autosomes are currently considered.\n    * Pedigree information is currently ignored; the between-family estimator\n      is used for all pairs.\n    * For multiallelic variants, REF allele counts are used.\n    * If the 'square' or 'square0' modifier is present, a square matrix is\n      written instead; 'square0' fills the upper right triangle with zeroes.\n    * If the 'zs' modifier is present, the .king file is Zstd-compressed.\n    * If the 'bin' modifier is present, a binary (square) matrix of\n      double-precision floating point values, suitable for loading from R, is\n      instead written to <output prefix>.king.bin.  ('bin4' specifies\n      single-precision numbers instead.)  This can be combined with 'square0'\n      if you still want the upper right zeroed out, or 'triangle' if you don't\n      want to pad the upper right at all.\n    * The computation can be subdivided with --parallel.\n  --make-king-table ['zs'] ['counts'] ['rel-check'] ['cols='<col set descrip.>]\n    Similar to --make-king, except results are reported in KING's original\n    .kin0 text table format (with minor changes, e.g. row order is more\n    friendly to incremental addition of samples), --king-table-filter can be\n    used to restrict the report to high kinship values, and the 'rel-check'\n    modifier can be used to restrict to same-FID pairs.\n    Supported column sets are:\n      maybefid: FID1/FID2, if that column was in the input.  Requires 'id'.\n      fid: Force FID1/FID2 even when FID was absent in the input.\n      id: IID1/IID2.\n      maybesid: SID1/SID2, if that column was in the input.  Requires 'id'.\n      sid: Force SID1/SID2 even when SID was absent in the input.\n      nsnp: Number of variants considered (autosomal, neither call missing).\n      hethet: Proportion/count of considered call pairs which are het-het.\n      ibs0: Proportion/count of considered call pairs which are opposite homs.\n      ibs1: HET1_HOM2 and HET2_HOM1 proportions/counts.\n      kinship: KING-robust between-family kinship estimator.\n    The default is maybefid,id,maybesid,nsnp,hethet,ibs0,kinship.\n    hethet/ibs0/ibs1 values are proportions unless the 'counts' modifier is\n    present.  If id is omitted, a .kin0.id file is also written.\n\n  --make-rel ['cov'] ['meanimpute'] [{square | square0 | triangle}]\n             [{zs | bin | bin4}]\n    Write a lower-triangular variance-standardized relationship matrix to\n    <output prefix>.rel, and corresponding IDs to <output prefix>.rel.id.\n    * This computation assumes that variants do not have very low MAF, or\n      deviate greatly from Hardy-Weinberg equilibrium.\n    * Also, it's usually best to perform this calculation on a variant set in\n      approximate linkage equilibrium.\n    * The 'cov' modifier replaces the variance-standardization step with basic\n      mean-centering, causing a covariance matrix to be calculated instead.\n    * The computation can be subdivided with --parallel.\n  --make-grm-list ['cov'] ['meanimpute'] ['zs'] [{id-header | iid-only}]\n  --make-grm-bin ['cov'] ['meanimpute'] [{id-header | iid-only}]\n    --make-grm-list causes the relationships to be written to GCTA's original\n    list format, which describes one pair per line, while --make-grm-bin writes\n    them in GCTA 1.1+'s single-precision triangular binary format.  Note that\n    these formats explicitly report the number of valid observations (where\n    neither sample has a missing call) for each pair, which is useful input for\n    some scripts.\n\n  --pca [count] [{approx | meanimpute}] ['scols='<col set descriptor>]\n  --pca [{allele-wts | biallelic-var-wts}] [count] [{approx | meanimpute}]\n        ['vzs'] ['scols='<col set descriptor>] ['vcols='<col set descriptor>]\n    Extracts top principal components from the variance-standardized\n    relationship matrix.\n    * It is usually best to perform this calculation on a variant set in\n      approximate linkage equilibrium, with no very-low-MAF variants.\n    * By default, 10 PCs are extracted; you can adjust this by passing a\n      numeric argument.  (Note that 10 is lower than the PLINK 1.9 default of\n      20; this is due to the randomized algorithm's memory footprint growing\n      quadratically w.r.t. the PC count.)\n    * The 'approx' modifier causes the standard deterministic computation to be\n      replaced with the randomized algorithm originally implemented for\n      Galinsky KJ, Bhatia G, Loh PR, Georgiev S, Mukherjee S, Patterson NJ,\n      Price AL (2016) Fast Principal-Component Analysis Reveals Convergent\n      Evolution of ADH1B in Europe and East Asia.  This can be a good idea when\n      you have >5k samples, and is almost required with >50k.\n    * The randomized algorithm always uses mean imputation for missing genotype\n      calls.  For comparison purposes, you can use the 'meanimpute' modifier to\n      request this behavior for the standard computation.\n    * 'scols=' can be used to customize how sample IDs appear in the .eigenvec\n      file.  (maybefid, fid, maybesid, and sid supported; default is\n      maybefid,maybesid.)\n    * The 'allele-wts' modifier requests an additional one-line-per-allele\n      .eigenvec.allele file with PCs expressed as allele weights instead of\n      sample weights.  When it's present, 'vzs' causes the .eigenvec.allele\n      file to be Zstd-compressed.\n      'vcols=' can be used to customize the report columns; supported column\n      sets are:\n        chrom: Chromosome ID.\n        pos: Base-pair coordinate.\n        (ID is always present, and positioned here.)\n        ref: Reference allele.\n        alt1: Alternate allele 1.\n        alt: All alternate alleles, comma-separated.\n        (A1 is always present, and positioned here.)\n        ax: Non-A1 alleles, comma-separated.\n        (PCs are always present, and positioned here.)\n      Default is chrom,ref,alt.\n    * For datasets with no multiallelic variants, the 'biallelic-var-wts'\n      modifier requests the old .eigenvec.var format, which only reports\n      weights for major alleles.  (These weights are 2x the corresponding\n      .eigenvec.allele weights.)  Supported column sets are:\n        chrom: Chromosome ID.\n        pos: Base-pair coordinate.\n        (ID is always present, and positioned here.)\n        ref: Reference allele.\n        alt1: Alternate allele 1.\n        alt: All alternate alleles, comma-separated.\n        maj: Major allele.\n        nonmaj: Minor allele.\n        (PCs are always present, and positioned here.  Signs are w.r.t. the\n        major, not necessarily reference, allele.)\n      Default is chrom,maj,nonmaj.\n\n  --king-cutoff [.king.bin + .king.id fileset prefix] <threshold>\n    Exclude one member of each pair of samples with KING-robust kinship greater\n    than the given threshold.  Remaining/excluded sample IDs are written to\n    <output prefix>.king.cutoff.in.id + .king.cutoff.out.id.\n    If present, the .king.bin file must be triangular (either precision is ok).\n\n  --write-covar ['cols='<column set descriptor>]\n    If covariates are defined, an updated version (with all filters applied) is\n    automatically written to <output prefix>.cov whenever --make-pgen,\n    --make-just-psam, --export, or a similar command is present.  However, if\n    you do not wish to simultaneously generate a new sample file, you can use\n    --write-covar to just produce a pruned covariate file.\n    Supported column sets are:\n      maybefid: FID, if that column was in the input.\n      fid: Force FID column to be written even when absent in the input.\n      maybesid: SID, if that column was in the input.\n      sid: Force SID column to be written even when absent in the input.\n      maybeparents: Father/mother IIDs ('0' = missing), if columns in input.\n      parents: Force PAT/MAT columns to be written even when absent in input.\n      sex: '1' = male, '2' = female, 'NA' = missing.\n      pheno1: First active phenotype.  If none, all column entries are set to\n              the --output-missing-phenotype string.\n      phenos: All active phenotypes, if any.  (Can be combined with pheno1 to\n              force at least one phenotype column to be written.)\n      (Covariates are always present, and positioned here.)\n    The default is maybefid,maybesid.\n\n  --pmerge <.pgen/.bed filename> <.pvar/.bim> <.psam/.fam>\n  --pmerge <.pgen + .pvar + .psam fileset prefix> ['vzs']\n    Merge the given fileset with the initially loaded fileset, writing the\n    result to <output prefix>.pgen + .pvar + .psam.\n  --pmerge-list <filename> [mode]\n    Merge all filesets named in the text file.  Also merge with the initially\n    loaded fileset if one was specified.\n    When a line in the text file contains three entries, they are interpreted\n    as full filenames for a binary fileset (.pgen/.bed, then .pvar/.bim, then\n    .psam/.fam).  If it contains exactly one entry, its interpretation depends\n    on the mode:\n    * 'bfile': Prefix for .bed + .bim + .fam fileset.\n    * 'bpfile': Prefix for .pgen + .bim + .fam fileset.\n    * 'pfile' (default): Prefix for .pgen + .pvar + .psam fileset.\n    * 'pfile-vzs': Prefix for .pgen + .pvar.zst + .psam fileset.\n    For both --pmerge and --pmerge-list:\n    * All input filesets must be sorted by position, and have the same\n      chromosome order.  (When this isn't true, use --make-pgen + --sort-vars\n      on each fileset first.)\n    * Variants are only merged if their IDs AND positions match.  (This is a\n      change from PLINK 1.x.)\n\n  --pgen-diff <.pgen/.bed filename> <.pvar/.bim> <.psam/.fam>\n              ['include-missing'] ['zs'] ['dosage' | 'dosage='<tolerance>]\n              ['cols='<column set descriptor>]\n  --pgen-diff <.pgen + .pvar + .psam prefix> ['vzs'] ['include-missing'] ['zs']\n              ['dosage' | 'dosage='<tolerance>] ['cols='<col set descriptor>]\n    Report unphased genotype/dosage differences in the intersection of two\n    filesets.  (Sample and variant filters are also applied.)\n    * If chrX or chrY is present, sex must be defined and consistent.\n    * Variants are only compared if their IDs AND positions match.  An error\n      is reported if any such match is not unique.\n    * By default, comparisons are based on hardcalls.  Use 'dosage' to compare\n      dosages instead; you can combine this with a tolerance in [0, 0.5).\n    * By default, if one genotype is missing and the other isn't, that doesn't\n      count as a difference; this can be changed with 'include-missing'.\n    Supported column sets are:\n      chrom: Chromosome ID.\n      pos: Base-pair coordinate.\n      id: Variant ID.\n      ref: Reference allele.\n      alt: All alternate alleles across both filesets, comma-separated.\n      maybefid: FID, if that column was present in the input.\n      fid: Force FID column to be written even when absent in the input.\n      (IID is always present, and positioned here.)\n      maybesid: SID, if that column was present in the input.\n      sid: Force SID column to be written even when absent in the input.\n      geno: Unphased GT or DS.\n    The default is id,maybefid,maybesid,geno.\n\n  --write-samples\n    Report IDs of all samples which pass your filters/inclusion thresholds.\n\n  --write-snplist ['zs'] ['allow-dups']\n    List all variants which pass your filters/inclusion thresholds.  Unless the\n    'allow-dups' modifier is provided, this now errors out when duplicate\n    variant ID(s) remain.\n\n  --glm ['zs'] ['omit-ref'] [{sex | no-x-sex}] ['log10'] ['pheno-ids']\n        [{genotypic | hethom | dominant | recessive | hetonly}] ['interaction']\n        ['hide-covar'] ['skip-invalid-pheno'] ['allow-no-covars']\n        [{intercept | cc-residualize | firth-residualize}]\n        [{no-firth | firth-fallback | firth}] ['cols='<col set desc>]\n        ['local-covar='<file>] ['local-psam='<file>]\n        ['local-pos-cols='<key col #s> | 'local-pvar='<file>] ['local-haps']\n        ['local-omit-last' | 'local-cats[0]='<category ct>]\n    Basic association analysis on quantitative and/or case/control phenotypes.\n    For each variant, a linear (for quantitative traits) or logistic (for\n    case/control) regression is run with the phenotype as the dependent\n    variable, and nonmajor allele dosage(s) and a constant-1 column as\n    predictors.\n    * There is usually an additive effect line for every nonmajor allele, and\n      no such line for the major allele.  To omit REF alleles instead of major\n      alleles, add the 'omit-ref' modifier.  (When performing interaction\n      testing, this tends to cause the multicollinearity check to fail for\n      low-ref-frequency variants.)\n    * By default, sex (male = 1, female = 2; note that this is a change from\n      PLINK 1.x) is automatically added as a predictor for X chromosome\n      variants, and no others.  The 'sex' modifier causes it to be added\n      everywhere (except chrY), while 'no-x-sex' excludes it entirely.\n    * The 'log10' modifier causes p-values to be reported in -log10(p) form.\n    * 'pheno-ids' causes the samples used in each set of regressions to be\n      written to an .id file.  (When the samples differ on chrX or chrY, .x.id\n      and/or .y.id files are also written.)\n    * The 'genotypic' modifier adds an additive effect/dominance deviation 2df\n      joint test (0-2 and 0..1..0 coding), while 'hethom' uses 0..0..1 and\n      0..1..0 coding instead.\n    * 'dominant' and 'recessive' specify a model assuming full dominance or\n      recessiveness, respectively, for the ref allele.  I.e. the genotype\n      column is recoded as 0..1..1 or 0..0..1, respectively.\n    * 'hetonly' replaces the genotype column with a dominance-deviation column.\n    * 'interaction' adds genotype x covariate interactions to the model.  Note\n      that this tends to produce 'NA' results (due to the multicollinearity\n      check) when the reference allele is 'wrong'; --maj-ref can be used to\n      enable analysis of those variants.\n    * Additional predictors can be added with --covar.  By default, association\n      statistics are reported for all nonconstant predictors; 'hide-covar'\n      suppresses covariate-only results, while 'intercept' causes intercepts\n      to be reported.\n      Since running --glm without at least e.g. principal component covariates\n      is usually an analytical mistake, the 'allow-no-covars' modifier is now\n      required when you're intentionally running --glm without a covariate\n      file.\n    * By default, if the current phenotype and covariates are such that every\n      regression on a chromosome will fail, PLINK 2 errors out.  To just skip\n      the phenotype or chromosome instead, add the 'skip-invalid-pheno'\n      modifier.\n    * There are now three regression modes for case/control phenotypes:\n      * 'no-firth' requests PLINK 1.x's behavior, where a NA result is reported\n        when basic logistic regression fails to converge.\n      * 'firth-fallback' requests logistic regression, followed by Firth\n        regression whenever the logistic regression fails to converge.  This is\n        now the default.\n      * 'firth' requests Firth regression all the time.\n    * Firth regression can be slow.  To trade off some accuracy for speed, you\n      can use the 'firth-residualize' modifier, which implements the shortcut\n      described in Mbatchou J et al. (2020) Computationally efficient whole\n      genome regression for quantitative and binary traits.  (You can also use\n      'cc-residualize' to force this shortcut to be applied to basic logistic\n      regression as well.)\n      * This must be used with 'hide-covar', and disables some other --glm\n        features.\n    * To add covariates which are not constant across all variants, add the\n      'local-covar=' and 'local-psam=' modifiers, use full filenames for each,\n      and use either 'local-pvar=' or 'local-pos-cols=' to provide variant ID\n      or position information.\n      Normally, the local-covar file should have c * n real-valued columns,\n      where the first c columns correspond to the first sample in the\n      local-psam file, columns (c+1) to 2c correspond to the second sample,\n      etc.; and the mth line corresponds to the mth nonheader line of the\n      local-pvar file when there is one.  (Variants outside of the local-pvar\n      file are excluded from the regression.)  The local covariates are\n      assigned the names LOCAL1, LOCAL2, etc.; to exclude the last local\n      covariate from the regression (necessary if they are e.g. local ancestry\n      coefficients which sum to 1), add 'local-omit-last'.\n      Alternatively, with 'local-cats='<k>, the local-covar file is expected to\n      have n columns with integer-valued entries in [1, k].  (This range is\n      [0, k-1] with 'local-cats0='.)  These category assignments are expanded\n      into (k-1) local covariates in the usual manner.\n      When position information is in the local-covar file, this should be\n      indicated by 'local-pos-cols='<number of header rows>,<chrom col #>,<pos\n      start col #>,<first covariate col #>.\n      'local-haps' indicates that there's one column or column-group per\n      haplotype instead of per sample; they are averaged by --glm.\n    The main report supports the following column sets:\n      chrom: Chromosome ID.\n      pos: Base-pair coordinate.\n      (ID is always present, and positioned here.)\n      ref: Reference allele.\n      alt1: Alternate allele 1.\n      alt: All alternate alleles, comma-separated.\n      (A1 is always present, and positioned here.  For multiallelic variants,\n      this column may contain multiple comma-separated alleles when the result\n      doesn't depend on which allele is A1.)\n      ax: Non-A1 alleles, comma-separated.\n      a1count: A1 allele count (can be decimal with dosage data).\n      totallele: Allele observation count (can be higher than --freq value, due\n                 to inclusion of het haploids and chrX model).\n      a1countcc: A1 count in cases, then controls (case/control only).\n      totallelecc: Case and control allele observation counts.\n      gcountcc: Genotype hardcall counts (neither-A1, het-A1, A1-A1) in cases,\n                then controls (case/control only).\n      a1freq: A1 allele frequency.\n      a1freqcc: A1 frequency in cases, then controls (case/control only).\n      machr2: Unphased MaCH imputation quality (frequently labeled 'INFO').\n      firth: Reports whether Firth regression was used (firth-fallback only).\n      test: Test identifier.  (Required unless only one test is run.)\n      nobs: Number of samples in the regression.\n      beta: Regression coefficient (for A1 if additive test).\n      orbeta: Odds ratio for case/control, beta for quantitative traits.\n              (Ignored if 'beta' column set included.)\n      se: Standard error of beta.\n      ci: Bounds of symmetric approximate confidence interval (requires --ci).\n      tz: T-statistic for linear regression, Wald Z-score for logistic/Firth.\n      p: Asymptotic p-value (or -log10(p)) for T/Z-statistic.\n      err: Error code for NA results.\n    The default is chrom,pos,ref,alt,firth,test,nobs,orbeta,se,ci,tz,p,err.\n\n  --score <filename> [i] [j] [k] [{header | header-read}]\n          [{center | variance-standardize | dominant | recessive}]\n          ['no-mean-imputation'] ['se'] ['zs'] ['ignore-dup-ids']\n          [{list-variants | list-variants-zs}] ['cols='<col set descriptor>]\n    Apply linear scoring system(s) to each sample.\n    The input file should have one line per scored (variant, allele) pair.\n    Variant IDs are read from column #i and allele codes are read from column\n    #j, where i defaults to 1 and j defaults to i+1.\n    * By default, a single column of input coefficients is read from column #k,\n      where k defaults to j+1.  (--score-col-nums can be used to specify\n      multiple columns.)\n    * 'header-read' causes the first line of the input file to be treated as a\n      header line containing score names.  Otherwise, score(s) are assigned the\n      names 'SCORE1', 'SCORE2', etc.; and 'header' just causes the first line\n      to be entirely ignored.\n    * By default, copies of unnamed alleles contribute zero to score, while\n      missing genotypes contribute an amount proportional to the loaded (via\n      --read-freq) or imputed allele frequency.  To throw out missing\n      observations instead (decreasing the denominator in the final average\n      when this happens), use the 'no-mean-imputation' modifier.\n    * You can use the 'center' modifier to shift all genotypes to mean zero, or\n      'variance-standardize' to linearly transform the genotypes to mean-0,\n      variance-1.\n    * The 'dominant' modifier causes dosages greater than 1 to be treated as 1,\n      while 'recessive' uses max(dosage - 1, 0) on diploid chromosomes.\n      ('dominant', 'recessive', and 'variance-standardize' cannot be used with\n      chrX.)\n    * The 'se' modifier causes the input coefficients to be treated as\n      independent standard errors; in this case, standard errors for the score\n      average/sum are reported, under a Gaussian approximation.  (Note that\n      this will systematically underestimate standard errors when scored\n      variants are in LD.)\n    * By default, --score errors out if a variant ID in the input file appears\n      multiple times in the main dataset.  Use the 'ignore-dup-ids' modifier to\n      skip them instead (a warning is still printed if such variants are\n      present).\n    * The 'list-variants[-zs]' modifier causes variant IDs used for scoring to\n      be written to <output prefix>.sscore.vars[.zst].\n    The main report supports the following column sets:\n      maybefid: FID, if that column was in the input.\n      fid: Force FID column to be written even when absent in the input.\n      (IID is always present, and positioned here.)\n      maybesid: SID, if that column was in the input.\n      sid: Force SID column to be written even when absent in the input.\n      pheno1: First active phenotype.\n      phenos: All active phenotypes, if any.\n      nallele: Number of nonmissing alleles.\n      denom: Denominator of score average (equal to nallele value when\n             'no-mean-imputation' specified).\n      dosagesum: Sum of named allele dosages.\n      scoreavgs: Score averages.\n      scoresums: Score sums.\n    The default is maybefid,maybesid,phenos,nallele,dosagesum,scoreavgs.\n    For more sophisticated polygenic risk scoring, we recommend looking at the\n    LDpred2 (https://privefl.github.io/bigsnpr/articles/LDpred2.html ) and\n    PRSice-2 (https://www.prsice.info/ ) software packages.\n\n  --variant-score <filename> ['bin' | 'bin4' | 'cols='<col set descriptor>]\n                  ['zs'] ['single-prec']\n    (alias: --vscore)\n    Apply linear scoring system(s) to each variant.  Each reported variant\n    score is the dot product of a sample-weight vector with the\n    total-ALT-dosage vector, with MAF-based mean imputation applied to missing\n    dosages.\n    Input file format: one line per sample, each starting with an ID and\n    followed by scoring weight(s); it can also have a header line with the\n    sample ID representation and the score name(s).\n    The usual .vscore text report supports the following column sets:\n      chrom: Chromosome ID.\n      pos: Base-pair coordinate.\n      (ID is always present, and positioned here.)\n      ref: Reference allele.\n      alt1: Alternate allele 1.\n      alt: All alternate alleles, comma-separated.\n      altfreq: ALT allele frequency used for mean-imputation.\n      nmiss: Number of missing (and thus mean-imputed) dosages.\n      nobs: Number of (nonmissing) sample observations.\n      (Variant scores are always present, and positioned here.)\n    Default is chrom,pos,ref,alt.\n    If binary output is requested instead, the main .vscore.bin matrix contains\n    floating-point values, column (score) ID(s) are saved to\n    <output prefix>.vscore.cols, and variant IDs are saved to\n    <output prefix>.vscore.vars[.zst].\n    'single-prec' causes the computation to use single- instead of\n    double-precision floating-point values internally.\n\n  --adjust-file <filename> ['zs'] ['gc'] ['cols='<column set descriptor>]\n                ['log10'] ['input-log10'] ['test='<test name, case-sensitive>]\n    Given a file with unfiltered association test results, report some basic\n    multiple-testing corrections, sorted in increasing-p-value order.\n    * 'gc' causes genomic-controlled p-values to be used in the formulas.\n      (This tends to be overly conservative.  We note that LD Score regression\n      usually does a better job of calibrating lambda; see Lee JJ, McGue M,\n      Iacono WG, Chow CC (2018) The accuracy of LD Score regression as an\n      estimator of confounding and genetic correlations in genome-wide\n      association studies.)\n    * 'log10' causes negative base 10 logs of p-values to be reported, instead\n      of raw p-values.  'input-log10' specifies that the input file contains\n      -log10(p) values.\n    * If the input file contains multiple tests per variant which are\n      distinguished by a 'TEST' column (true for --linear/--logistic/--glm),\n      you must use 'test=' to select the test to process.\n    The following column sets are supported:\n      chrom: Chromosome ID.\n      pos: Base-pair coordinate.\n      (ID is always present, and positioned here.)\n      ref: Reference allele.\n      alt1: Alternate allele 1.\n      alt: All alternate alleles, comma-separated.\n      a1: Tested allele.  (Omitted if missing from input file.)\n      unadj: Unadjusted p-value.\n      gc: Devlin & Roeder (1999) genomic control corrected p-value (additive\n          models only).\n      qq: P-value quantile.\n      bonf: Bonferroni correction.\n      holm: Holm-Bonferroni (1979) adjusted p-value.\n      sidakss: Sidak single-step adjusted p-value.\n      sidaksd: Sidak step-down adjusted p-value.\n      fdrbh: Benjamini & Hochberg (1995) step-up false discovery control.\n      fdrby: Benjamini & Yekutieli (2001) step-up false discovery control.\n    Default set is chrom,a1,unadj,gc,bonf,holm,sidakss,sidaksd,fdrbh,fdrby.\n  --genotyping-rate ['dosage']\n    Report genotyping rate in log (this was automatic in PLINK 1.x).\n\n  --pgen-info\n    Reports basic information about a .pgen file.\n\n  --validate\n    Validates all variant records in a .pgen file.\n\n  --zst-decompress <.zst file> [output filename]\n    (alias: --zd)\n    Decompress a Zstd-compressed file.  If no output filename is specified, the\n    file is decompressed to standard output.\n    This cannot be used with any other flags, and does not cause a log file to\n    be generated.\n\nThe following other flags are supported.\n  --script <fname>    : Include command-line options from file.\n  --rerun [log]       : Rerun commands in log (default 'plink2.log').\n  --version           : Display only version number before exiting.\n  --silent            : Suppress regular output to console.  (Error-output is\n                        not suppressed.)\n  --double-id         : When importing single-part sample IDs, set both FID and\n                        IID to the original ID.\n  --const-fid [ID]    : When importing single-part sample IDs, set FID to the\n                        given constant and IID to the original ID.\n  --id-delim [d]      : Normally parses single-delimiter sample IDs as\n                        <FID><d><IID>, and double-delimiter IDs as\n                        <FID><d><IID><d><SID>; default delimiter is '_'.\n                        --id-delim can no longer be used with\n                        --double-id/--const-fid; it will error out if any ID\n                        lacks the delimiter.\n  --idspace-to <c>    : Convert spaces in VCF/.bgen sample IDs to the given\n                        character.\n  --iid-sid           : Make --id-delim and --sample-diff interpret two-token\n                        sample IDs as IID-SID instead of FID-IID.\n  --vcf-require-gt    : Skip variants with no GT field.\n  --vcf-min-gq <val>  : No-call genotypes when GQ is present and below the\n                        threshold.\n  --vcf-max-dp <val>  : No-call genotypes when DP is present and above/below\n  --vcf-min-dp <val>    the threshold.\n  --vcf-half-call <m> : Specify how '0/.' and similar VCF GT values should be\n                        handled.  The following four modes are supported:\n                        * 'error'/'e' (default) errors out and reports line #.\n                        * 'haploid'/'h' treats them as haploid calls.\n                        * 'missing'/'m' treats them as missing.\n                        * 'reference'/'r' treats the missing value as 0.\n  --vcf-ref-n-missing : Import VCF 'N' REF alleles as missing alleles.  This\n                        can be appropriate for .ped-derived VCFs.\n  --oxford-single-chr <chr name>  : Specify single-chromosome .gen/.bgen file\n                                    with no useful chromosome info inside.\n  --missing-code [string list]    : Comma-delimited list of missing phenotype\n    (alias: --missing_code)         values for Oxford-format import (default\n                                    'NA').\n  --hard-call-threshold <val>     : When importing dosage data, a hardcall is\n                                    normally saved when the distance from the\n                                    nearest hardcall, defined as\n                                      0.5 * sum_i |x_i - round(x_i)|\n                                    (where the x_i's are 0..2 allele dosages),\n                                    is not greater than 0.1.  You can adjust\n                                    this threshold by providing a numeric\n                                    argument to --hard-call-threshold.\n                                    You can also use this with --make-[b]pgen\n                                    to alter the saved hardcalls while leaving\n                                    the dosages untouched, or --make-bed to\n                                    tweak hardcall export.\n  --dosage-erase-threshold <val>  : --hard-call-threshold normally preserves\n                                    the original dosages, and several PLINK 2\n                                    commands use them when they're available.\n                                    Use --dosage-erase-threshold to make PLINK\n                                    2 erase dosages and keep only hardcalls\n                                    when distance-from-hardcall <= the given\n                                    level.\n  --import-dosage-certainty <val> : The PLINK 2 file format currently supports\n                                    a single dosage for each allele.  Some\n                                    other dosage file formats include a\n                                    separate probability for every possible\n                                    genotype, e.g. {P(0/0)=0.2, P(0/1)=0.52,\n                                    P(1/1)=0.28}, a highly uncertain call that\n                                    is nevertheless treated as a hardcall under\n                                    '--hard-call-threshold 0.1'.  To make PLINK\n                                    2 treat a dosage as missing whenever the\n                                    largest probability is less than a\n                                    threshold, use --import-dosage-certainty.\n  --input-missing-genotype <c> : '.' is always interpreted as a missing\n                                 genotype code in input files.  By default, '0'\n                                 also is; you can change this second missing\n                                 code with --input-missing-genotype.\n  --allow-extra-chr  : Permit unrecognized chromosome codes (alias --aec).\n  --chr-set <autosome ct> ['no-x'] ['no-y'] ['no-xy'] ['no-mt'] :\n    Specify a nonhuman chromosome set.  The first parameter sets the number of\n    diploid autosome pairs if positive, or haploid chromosomes if negative.\n    Given diploid autosomes, the remaining modifiers indicate the absence of\n    the named non-autosomal chromosomes.\n  --cow/--dog/--horse/--mouse/--rice/--sheep : Shortcuts for those species.\n  --autosome-num <val>    : Alias for '--chr-set <value> no-y no-xy no-mt'.\n  --human                 : Explicitly specify human chromosome set, and make\n                            output .pvar/VCF files include a ##chrSet header\n                            line.  (.pvar/VCF output files automatically\n                            include ##chrSet when a nonhuman set is specified.)\n  --chr-override ['file'] : By default, if --chr-set/--autosome-num/--cow/etc.\n                            conflicts with an input file ##chrSet header line,\n                            PLINK 2 will error out.  --chr-override with no\n                            argument causes the command line to take\n                            precedence; '--chr-override file' defers to the\n                            file.\n  --var-min-qual <val>             : Skip variants with low/missing QUAL.\n  --var-filter [exception(s)...]   : Skip variants which have FILTER failures.\n  --extract-if-info <key> <op> <val> : Exclude variants which don't/do satisfy\n  --exclude-if-info <key> <op> <val>   a comparison predicate on an INFO key,\n  (aliases: --extract-if,              e.g.\n            --exclude-if)                --extract-if-info \"VT == SNP\"\n                                       Unless the operator is !=, the predicate\n                                       always evaluates to false when the key\n                                       is missing.\n  --require-info <key(s)...>         : Exclude variants based on nonexistence\n  --require-no-info <key(s)...>        or existence of an INFO key.  \"<key>=.\"\n                                       is treated as nonexistence.\n  --extract-col-cond <f> [valcol] [IDcol] [skip] :\n  --extract-col-cond-match <(sub)string(s)...>\n  --extract-col-cond-mismatch <(sub)string(s)...>\n  --extract-col-cond-substr\n  --extract-col-cond-min <min>\n  --extract-col-cond-max <max> :\n    Exclude all variants without a value-column entry satisfying a condition.\n    * By default, values are read from column 2 of the file, and variant IDs\n      are read from column 1.\n    * Three types of conditions are supported:\n      * When --extract-col-cond-match is specified without\n        --extract-col-cond-substr, the value is checked for equality with the\n        given strings, and kept iff one of them matches.  Similarly,\n        --extract-col-cond-mismatch without --extract-col-cond-substr causes\n        the variant to be kept iff the value matches none of the given strings.\n      * When --extract-col-cond-match and/or -mismatch are specified with\n        --extract-col-cond-substr, the variant is kept iff none of the\n        --extract-col-cond-mismatch substrings are contained in the value, and\n        either --extract-col-cond-match was unspecified or at least one of its\n        substrings is contained.\n      * Otherwise, the value is interpreted as a number, and the variant is\n        kept if the number is in [<min>, <max>] (default min=0, max=DBL_MAX).\n  --pheno ['iid-only'] <f> : Specify additional phenotype/covariate file.\n                             Comma-delimited files with a header line are now\n                             permitted.\n  --pheno-name <name...>   : Only load the designated phenotype(s) from the\n                             --pheno (if one was specified) or .psam (if no\n                             --pheno) file.  Separate multiple names with\n                             spaces or commas, and use dashes to designate\n                             ranges.\n  --pheno-col-nums <#...>  : Only load the phenotype(s) in the designated\n                             column number(s) from the --pheno file.\n  --no-psam-pheno          : Ignore phenotype(s) in .psam/.fam file.\n  --strict-sid0      : By default, if there is no SID column in the .psam/.fam\n                       (or --update-ids) file, but there is one in another\n                       input file (for e.g. --keep/--remove), the latter SID\n                       column is ignored; sample IDs are considered matching as\n                       long as FID and IID are equal (with missing FID treated\n                       as '0').  If you also want to require SID = '0' for a\n                       sample ID match in this situation, add --strict-sid0.\n  --input-missing-phenotype <v> : Set nonzero number to treat as a missing\n                                  pheno/covar in input files (default -9).\n  --no-input-missing-phenotype  : Don't treat any nonzero number as a missing\n                                  pheno/covar.  ('NA'/'nan' are still treated\n                                  as missing.)\n  --1                           : Expect case/control phenotypes in input files\n                                  to be coded as 0 = control, 1 = case, instead\n                                  of the usual 0 = missing, 1 = ctrl, 2 = case.\n                                  (Unlike PLINK 1.x, this does not force all\n                                  phenotypes to be interpreted as case/ctrl.)\n  --missing-catname <str>       : Set missing-categorical-phenotype string\n                                  (case-sensitive, default 'NONE').\n  --covar ['iid-only'] <f> : Specify additional covariate file.\n                             Comma-delimited files with a header line are now\n                             permitted.\n  --covar-name <name...>   : Only load the designated covariate(s) from the\n                             --covar (if one was specified), --pheno (if no\n                             --covar), or .psam (if no --covar or --pheno)\n                             file.\n  --covar-col-nums <#...>  : Only load the covariate(s) in the designated\n                             column number(s) from the --covar (if one was\n                             specified) or --pheno (if no --covar) file.\n  --within <f> [new pheno name] : Import a PLINK 1.x categorical phenotype.\n                                  (Phenotype name defaults to 'CATPHENO'.)\n                                  * If any numeric values are present, ALL\n                                    values must be numeric.  In that case, 'C'\n                                    is added in front of all category names.\n                                  * 'NA' is treated as a missing value.\n  --mwithin <n>                 : Load --within categories from column n+2.\n  --family [new pheno name]     : Create a categorical phenotype from FID.\n                                  Restrictions on and handling of numeric\n                                  values are the same as for --within.\n  --family-missing-catname <nm> : Make --family treat the specified FID as\n                                  missing.\n  --keep <fname...>    : Exclude all samples not named in a file.\n  --remove <fname...>  : Exclude all samples named in a file.\n  --keep-fam <fn...>   : Exclude all families not named in a file.\n  --remove-fam <f...>  : Exclude all families named in a file.\n  --extract [{bed0 | bed1}] <f...> : Usually excludes all variants (not) named\n  --exclude [{bed0 | bed1}] <f...>   in the given file(s).  When multiple files\n                                     are named, they are concatenated.\n                                     With the 'bed0' or 'bed1' modifier,\n                                     variants outside/inside the positional\n                                     ranges in the interval-BED file(s) are\n                                     excluded instead.  'bed0' tells PLINK 2 to\n                                     assume the interval bounds follow the UCSC\n                                     0-based half-open convention, while 'bed1'\n                                     (equivalent to PLINK 1.9 'range')\n                                     specifies 1-based fully-closed.\n  --extract-intersect [{bed0 | bed1}] <f...> : Just like --extract, except that\n                                               a variant must be in the\n                                               intersection, rather than just\n                                               the union, of the files to\n                                               remain.\n  --bed-border-bp <n>      : Stretch BED intervals by the given amount on each\n  --bed-border-kb <n>        side.\n  --keep-cats <filename>   : These can be used individually or in combination\n  --keep-cat-names <nm...>   to define a list of categories to keep; all\n                             samples not in one of the named categories are\n                             excluded.  Use spaces to separate category names\n                             for --keep-cat-names.  Use the --missing-catname\n                             value (default 'NONE') to refer to the group of\n                             uncategorized samples.\n  --keep-cat-pheno <pheno> : If more than one categorical phenotype is loaded,\n                             or you wish to filter on a categorical covariate,\n                             --keep-cat-pheno must be used to specify which\n                             phenotype/covariate --keep-cats and\n                             --keep-cat-names apply to.\n  --remove-cats <filename> : Exclude all categories named in the file.\n  --remove-cat-names <...> : Exclude named categories.\n  --remove-cat-pheno <phe> : Specify pheno for --remove-cats/remove-cat-names.\n  --split-cat-pheno [{omit-most | omit-last}] ['covar-01']\n                    [cat. pheno/covar name(s)...] :\n    Split n-category phenotype(s) into n (or n-1, with 'omit-most'/'omit-last')\n    binary phenotypes, with names of the form <orig. pheno name>=<cat. name>.\n    (As a consequence, affected phenotypes and categories are not permitted to\n    contain the '=' character.)\n    * This happens after all sample filters.\n    * If no phenotype or covariate names are provided, all categorical\n      phenotypes (but not covariates) are processed.\n    * By default, generated covariates are coded as 1=false, 2=true.  To code\n      them as 0=false, 1=true instead, add the 'covar-01' modifier.\n  --loop-cats <pheno/covar>   : Run variant filters and subsequent operations\n                                on just the samples in the first category; then\n                                just the samples in the second category; and so\n                                on, for all categories in the named categorical\n                                phenotype.\n  --no-id-header ['iid-only'] : Don't include a header line in .id output\n                                files.  This normally forces two-column FID/IID\n                                output; add 'iid-only' to force just\n                                single-column IID.\n  --variance-standardize [pheno/covar name(s)...]\n  --covar-variance-standardize [covar name(s)...] :\n    Linearly transform named covariates (and quantitative phenotypes, if\n    --variance-standardize) to mean-zero, variance 1.  If no arguments are\n    provided, all possible phenotypes/covariates are affected.\n    This is frequently necessary to prevent the multicollinearity check from\n    failing when dealing with covariates where abs(mean) is much larger than\n    the standard deviation, such as year of birth.\n  --quantile-normalize [...]       : Force named covariates and quantitative\n  --pheno-quantile-normalize [...]   phenotypes to a N(0,1) distribution,\n  --covar-quantile-normalize [...]   preserving only the original rank orders.\n  --chr <chr(s)...>  : Exclude all variants not on the given chromosome(s).\n                       Valid choices for humans are 0 (unplaced), 1-22, X, Y,\n                       XY, MT, PAR1, and PAR2.  Separate multiple chromosomes\n                       with spaces and/or commas, and use a dash (no adjacent\n                       spaces permitted) to denote a range, e.g.\n                       '--chr 1-4, 22, par1, x, par2'.\n  --not-chr <...>    : Reverse of --chr (exclude variants on listed\n                       chromosomes).\n  --autosome         : Exclude all non-autosomal variants.\n  --autosome-par     : Exclude all non-autosomal variants, except those in a\n                       pseudo-autosomal region.\n  --snps-only ['just-acgt'] : Exclude non-SNP variants.  By default, SNP = all\n                              allele codes are single-character (so\n                              multiallelic variants with a mix of SNPs and\n                              non-SNPs are excluded; split your variants first\n                              if that's a problem).\n                              The 'just-acgt' modifier restricts SNP codes to\n                              {A,C,G,T,a,c,g,t,<missing>}.\n  --from <var ID>    : Use ID(s) to specify a variant range to load.  When used\n  --to   <var ID>      together, both variants must be on the same chromosome.\n                       (--snps can be used to specify intervals which cross\n                       chromosome boundaries.)\n  --snp  <var ID>    : Specify a single variant to load.\n  --exclude-snp <ID> : Specify a single variant to exclude.\n  --window  <kbs>    : With --snp/--exclude-snp, loads/excludes all variants\n                       within half the specified kb distance of the named one.\n  --from-bp <pos>    : Use base-pair coordinates to define a variant range to\n  --to-bp   <pos>      load.\n  --from-kb <pos>      * You must use these with --chr, specifying a single\n  --to-kb   <pos>        chromosome.\n  --from-mb <pos>      * Decimals and negative numbers are permitted.\n  --to-mb   <pos>      * The --to-bp(/-kb/-mb) position is no longer permitted\n                         to be smaller than the --from-bp position.\n  --snps <var IDs...>  : Use IDs to specify variant range(s) to load or\n  --exclude-snps <...>   exclude.  E.g. '--snps rs1111-rs2222, rs3333, rs4444'.\n  --force-intersect    : PLINK 2 normally errors out when multiple variant\n                         inclusion filters (--extract, --extract-col-cond,\n                         --extract-intersect, --from/--to, --from-bp/--to-bp,\n                         --snp, --snps) are specified.  --force-intersect\n                         allows the run to proceed; the set intersection will\n                         be taken.\n  --thin <p>           : Randomly remove variants, retaining each with prob. p.\n  --thin-count <n>     : Randomly remove variants until n of them remain.\n  --bp-space <bps>     : Remove variants so that each pair is no closer than\n                         the given bp distance.\n  --thin-indiv <p>       : Randomly remove samples, retaining with prob. p.\n  --thin-indiv-count <n> : Randomly remove samples until n of them remain.\n  --keep-col-match <f> <val(s)...> : Exclude all samples without a 3rd column\n                                     entry in the given file exactly matching\n                                     one of the given strings.  (Separate\n                                     multiple strings with spaces.)\n  --keep-col-match-name <col name> : Check column with given name instead.\n  --keep-col-match-num <n>         : Check nth column instead.\n  --geno [val] [{dosage | hh-missing}]\n  --mind [val] [{dosage | hh-missing}] :\n    Exclude variants (--geno) and/or samples (--mind) with missing call\n    frequencies greater than a threshold (default 0.1).  (Note that the default\n    threshold is only applied if --geno/--mind is invoked without an argument;\n    when --geno/--mind is not invoked, no missing call frequency ceiling is\n    enforced at all.  Other inclusion/exclusion default thresholds work the\n    same way.)\n    By default, when a dosage is present but a hardcall is not, the genotype is\n    treated as missing; add the 'dosage' modifier to treat this case as\n    nonmissing.  Alternatively, you can use 'hh-missing' to also treat\n    heterozygous haploid calls as missing.\n  --require-pheno [name(s)...] : Remove samples missing any of the named\n  --require-covar [name(s)...]   phenotype(s)/covariate(s).  If no arguments\n                                 are provided, all phenotype(s)/covariate(s)\n                                 must be present.\n  --maf [freq] [mode]     : Exclude variants with allele frequency lower than a\n    (alias: --min-af)       threshold (default 0.01).  By default, the nonmajor\n                            allele frequency is used; the other supported modes\n                            are 'nref' (non-reference), 'alt1', and 'minor'\n                            (least frequent).  bcftools freq:mode notation is\n                            permitted.\n  --max-maf <freq> [mode] : Exclude variants with MAF greater than the\n    (alias: --max-af)       threshold.\n  --mac <ct> [mode]       : Exclude variants with allele dosage lower than the\n    (alias: --min-ac)       given threshold.\n  --max-mac <ct> [mode]   : Exclude variants with allele dosage greater than\n    (alias: --max-ac)       the given threshold.\n  --af-pseudocount <x>    : Given j observations of one allele and k\n                            observations of the other for a biallelic variant,\n                            infer allele frequencies of (j+x) / (j+k+2x) and\n                            (k+x) / (j+k+2x), rather than the default j / (j+k)\n                            and k / (j+k).\n                            * For multiallelic variants, note that this makes\n                              unobserved ALT alleles matter.\n                            * This does not affect --freq's output.\n  --min-alleles <ct> : Exclude variants with fewer than the given # of alleles.\n                       (When a variant has exactly one ALT allele, and it's\n                       a missing-code, it's excluded by \"--min-alleles 2\".)\n  --max-alleles <ct> : Exclude variants with more than the given # of alleles.\n  --read-freq <file> : Load allele frequency estimates from the given --freq or\n                       --geno-counts (or PLINK 1.9 --freqx) report, instead of\n                       imputing them from the immediate dataset.\n  --hwe <p> ['midp'] ['keep-fewhet'] :\n    Exclude variants with Hardy-Weinberg equilibrium exact test p-values below\n    a threshold.\n    * By default, only founders are considered.\n    * chrX p-values are now computed using Graffelman and Weir's method.\n    * For variants with k alleles with k>2, k separate 'biallelic' tests are\n      performed, and the variant is filtered out if any of them fail.\n    * With 'keep-fewhet', variants which fail the test in the too-few-hets\n      direction are not excluded.  On chrX, this uses the ratio between the\n      Graffelman/Weir p-value and the female-only p-value.\n    * There is currently no special handling of case/control phenotypes.\n  --mach-r2-filter [min] [max] : Exclude variants with MaCH imputation quality\n                                 metric less than min or greater than max\n                                 (defaults 0.1 and 2.0).  (Monomorphic\n                                 variants, with r2 = nan, are not excluded.)\n                                 * This is NOT identical to the R2 metric\n                                   reported by Minimac3 0.1.13+; see below.\n                                 * If a single argument is provided, it is\n                                   treated as the minimum.\n                                 * The metric is not computed on chrX and MT.\n  --minimac3-r2-filter <min> [max] : Compute Minimac3 R2 values from scratch,\n                                     and exclude variants with R2 less than min\n                                     or (if max is provided) greater than max.\n                                     * Note that this requires phased-dosage\n                                       data for all samples and variants;\n                                       otherwise this will systematically\n                                       underestimate imputation quality, since\n                                       unphased hardcalls/dosages are treated\n                                       as if they were maximally uncertain.\n                                       (Use --extract-if-info/--exclude-if-info\n                                       to filter on precomputed Minimac3 R2 in\n                                       a VCF/.pvar INFO column.)\n  --keep-females     : Exclude male and unknown-sex samples.\n  --keep-males       : Exclude female and unknown-sex samples.\n  --keep-nosex       : Exclude all known-sex samples.\n  --remove-females   : Exclude female samples.\n  --remove-males     : Exclude male samples.\n  --remove-nosex     : Exclude unknown-sex samples.\n  --keep-founders    : Exclude nonfounder samples.\n  --keep-nonfounders : Exclude founder samples.\n  --keep-if <pheno/covar> <op> <val> : Exclude samples which don't/do satisfy a\n  --remove-if <pheno/covar> <op> <v>   comparison predicate, e.g.\n                                         --keep-if \"PHENO1 == case\"\n                                       Unless the operator is !=, the predicate\n                                       always evaluates to false when the\n                                       phenotype/covariate is missing.\n  --nonfounders      : Include nonfounders in allele freq/HWE calculations.\n  --bad-freqs        : When PLINK 2 needs decent allele frequencies, it\n                       normally errors out if they aren't provided by\n                       --read-freq and less than 50 founders are available to\n                       impute them from.  Use --bad-freqs to force PLINK 2 to\n                       proceed in this case.\n  --bad-ld           : PLINK 2 normally errors out when it needs to estimate LD\n                       between variants, but there are less than 50 founders to\n                       estimate from.  Use --bad-ld to force PLINK 2 to\n                       proceed.\n  --export-allele <file> : With --export A/A-transpose/AD, count alleles named\n                           in the file, instead of REF alleles.\n  --output-chr <MT code> : Set chromosome coding scheme in output files by\n                           providing the desired human mitochondrial code.\n                           Options are '26', 'M', 'MT', '0M', 'chr26', 'chrM',\n                           and 'chrMT'; default is now 'MT' (note that this is\n                           a change from PLINK 1.x, which defaulted to '26').\n  --output-missing-genotype <ch> : Set the code used to represent missing\n                                   genotypes in PLINK-format files generated by\n                                   --make-[b]pgen/--make-bed/--export (default\n                                   '.').\n  --output-missing-phenotype <s> : Set the string used to represent missing\n                                   phenotypes in PLINK-format files generated\n                                   by --make-[b]pgen/--make-bed/--export\n                                   (default 'NA' for .psam, -9 for older).\n  --sort-vars [mode]      : Sort variants by chromosome, then position, then\n                            ID.  The following string orders are supported:\n                            * 'natural'/'n': Natural sort (default).\n                            * 'ascii'/'a': ASCII.\n                            This must be used with --pmerge[-list] or\n                            --make-[b]pgen/--make-bed.\n  --set-hh-missing ['keep-dosage'] : Make --make-[b]pgen/--make-bed set non-MT\n                                     heterozygous haploid hardcalls, and all\n                                     female chrY calls, to missing.  (Unlike\n                                     PLINK 1.x, this treats unknown-sex chrY\n                                     genotypes like males, not females.)\n                                     By default, all associated dosages are\n                                     also erased; use 'keep-dosage' to keep\n                                     them all.\n  --set-mixed-mt-missing ['keep-dosage'] : Make --make-[b]pgen/--make-bed set\n                                           mixed MT hardcalls to missing.\n  --split-par <bp1> <bp2> : Changes chromosome code of all X chromosome\n  --split-par <build>       variants with bp position <= bp1 to PAR1, and those\n                            with position >= bp2 to PAR2.  The following build\n                            codes are supported as shorthand:\n                            * 'b36'/'hg18' = NCBI 36, 2709521/154584237\n                            * 'b37'/'hg19' = GRCh37, 2699520/154931044\n                            * 'b38'/'hg38' = GRCh38, 2781479/155701383\n  --merge-par             : Merge PAR1/PAR2 back with X.  Requires PAR1 to be\n                            positioned immediately before X, and PAR2 to be\n                            immediately after X.  (Should *not* be used with\n                            \"--export vcf\", since it causes male\n                            homozygous/missing calls in PAR1/PAR2 to be\n                            reported as haploid.)\n  --merge-x               : Merge XY back with X.  This usually has to be\n                            combined with --sort-vars.\n  --set-missing-var-ids <t>  : Given a template string with a '@' where the\n  --set-all-var-ids <t>        chromosome code should go and '#' where the bp\n                               coordinate belongs, --set-missing-var-ids\n                               assigns chromosome-and-bp-based IDs to unnamed\n                               variants, while --set-all-var-ids resets all\n                               IDs.\n                               You may also use '$r'/'$a' to refer to the\n                               ref and alt1 alleles, or '$1'/'$2' to refer to\n                               them in alphabetical order.\n  --var-id-multi <t>         : Specify alternative templates for multiallelic\n  --var-id-multi-nonsnp <t>    variants.  ('$a' and '$1'/'$2' should be avoided\n                               here, though they're technically still allowed.)\n  --new-id-max-allele-len <len> [{error | missing | truncate}] :\n    Specify maximum number of leading characters from allele codes to include\n    in new variant IDs, and behavior on longer codes (defaults 23, error).\n  --missing-var-code <str>   : Change unnamed variant code for --rm-dup,\n                               --set-{missing|all}-var-ids, and\n                               --recover-var-ids (default '.').\n  --update-map  <f> [bpcol]  [IDcol]  [skip] : Update variant bp positions.\n  --update-name <f> [newcol] [oldcol] [skip] : Update variant IDs.\n  --recover-var-ids <file> ['strict-bim-order'] [{rigid | force}] ['partial'] :\n    Undo --set-all-var-ids, given the original .pvar/VCF/.bim file.  Original\n    IDs are looked up by position and allele codes.\n    * By default, if the original-ID file is a .bim, allele order is ignored.\n      Use 'strict-bim-order' to force A1=ALT, A2=REF.\n    * If any variant has multiple matching records in the original-ID file, and\n      the IDs conflict, --recover-var-ids writes the affected (current) ID(s)\n      to <output prefix>.recoverid.dup, and normally errors out.  If the\n      original-ID file has the same number of variants in the same order, you\n      can still recover the old IDs with the 'rigid' modifier in this case.\n      Alternatively, to proceed and assign the missing-ID code to these\n      variants, add the 'force' modifier.  (The .recoverid.dup file is still\n      written when 'rigid' or 'force' is specified.)\n    * --recover-var-ids normally expects to replace all variant IDs, and errors\n      out if any are left untouched.  Add the 'partial' modifier when you\n      actually want to update just a proper subset.\n  --update-alleles <fname>   : Update variant allele codes.\n  --update-ids <fname>       : Update sample IDs.\n  --update-parents <fname>   : Update parental IDs.\n  --update-sex <filename> ['col-num='<n>] ['male0'] :\n    Update sex information.\n    * By default, if there is a header line starting with '#FID'/'#IID', sex is\n      loaded from the first column titled 'SEX' (any capitalization);\n      otherwise, column 3 is assumed.  Use 'col-num=' to force a column number.\n    * Only the first character in the sex column is processed.  By default,\n      '1'/'M'/'m' is interpreted as male, '2'/'F'/'f' is interpreted as female,\n      and '0'/'N'/'U'/'u' is interpreted as unknown-sex.  To change this to\n      '0'/'M'/'m' = male, '1'/'F'/'f' = female, anything else other than '2' =\n      unknown-sex, add 'male0'.\n  --real-ref-alleles  : Treat A2 alleles in a PLINK 1.x fileset as actual REF\n                        alleles; otherwise they're marked as provisional.\n  --maj-ref ['force'] : Set major alleles to reference, like PLINK 1.x\n                        automatically did.  (Note that this is now opt-in\n                        rather than opt-out; --keep-allele-order is no longer\n                        necessary to prevent allele-swapping.)\n                        * This can only be used in runs with\n                          --make-bed/--make-[b]pgen/--export and no other\n                          commands.\n                        * By default, this only affects variants marked as\n                          having 'provisional' reference alleles.  Add 'force'\n                          to apply this to all variants.\n                        * All new reference alleles are marked as provisional.\n  --ref-allele ['force'] <filename> [refcol] [IDcol] [skip]\n  --alt1-allele ['force'] <filename> [alt1col] [IDcol] [skip] :\n    These set the alleles specified in the file to ref (--ref-allele) or alt1\n    (--alt1-allele).  They can be combined in the same run.\n    * These can only be used in runs with --make-bed/--make-[b]pgen/--export\n      and no other commands.\n    * \"--ref-allele <VCF filename> 4 3 '#'\", which scrapes reference allele\n      assignments from a VCF file, is especially useful.\n    * By default, these error out when asked to change a 'known' reference\n      allele.  Add 'force' to permit that (when e.g. switching to a new\n      reference genome).\n    * When --alt1-allele changes the previous ref allele to alt1, the previous\n      alt1 allele is set to reference and marked as provisional.\n  --ref-from-fa ['force'] : This sets reference alleles from the --fa file when\n                            it can be done unambiguously (note that it's never\n                            possible for deletions or some insertions).\n                            By default, it errors out when asked to change a\n                            'known' reference allele; add the 'force' modifier\n                            to permit that.\n  --normalize ['list']    : Left-normalize all variants, using the --fa file.\n    (alias: --norm)         (Assumes no differences in capitalization.)  The\n                            'list' modifier causes a list of affected variant\n                            IDs to be written to <output prefix>.normalized.\n  --indiv-sort <mode> [f] : Specify sample ID sort order for merge and\n                            --make-[b]pgen/--make-bed.  The following four\n                            modes are supported:\n                            * 'none'/'0' keeps samples in the order they were\n                              loaded.  Default for non-merge.\n                            * 'natural'/'n' invokes \"natural sort\", e.g.\n                              'id2' < 'ID3' < 'id10'.  Default when merging.\n                            * 'ascii'/'a' sorts in ASCII order, e.g.\n                              'ID3' < 'id10' < 'id2'.\n                            * 'file'/'f' uses the order in the given file\n                              (named in the last argument).\n  --pmerge-list-dir <dir>  : Specify base dir to join to --pmerge-list entries.\n  --pmerge-output-vzs      : Compress the .pvar file from --pmerge[-list].\n  --sample-inner-join      : By default, --pmerge[-list] performs an 'outer\n  --variant-inner-join       join': the merged fileset contains the union of\n  --pheno-inner-join         the samples in the input filesets, and ditto for\n                             variants and phenotypes.\n                             --{sample,variant,pheno}-inner-join specifies an\n                             intersection instead.\n  --merge-mode <mode>      : Set --pmerge[-list] conflict resolution mode for\n  --merge-parents-mode <m>   genotypes/dosages, parents, sexes, and phenotypes,\n  --merge-sex-mode <mode>    respectively.\n  --merge-pheno-mode <m>     * 'nm-match'/'1' = nonmissing values must match\n                                                (default)\n                             * 'nm-first'/'2' = keep first nonmissing value\n                             * 'first'/'4' = keep first value, even if missing\n  --merge-xheader-mode <m> : Set conflict resolution mode for .pvar header\n                             entries.\n                             * 'erase' = remove all\n                             * 'match' = discard when there's ANY difference in\n                                         the values (even capitalization)\n                             * 'first' = keep first value (default)\n  --merge-qual-mode <mode> : Set conflict resolution mode for\n  --merge-filter-mode <m>    QUAL/FILTER/INFO/CM entries.\n  --merge-info-mode <mode>   * 'erase' = remove column\n  --merge-cm-mode <mode>     * 'nm-match' = nonmissing values must match\n                             * 'nm-first' = keep first nonmissing (info/CM\n                                            default)\n                             * 'first' = keep first value, even if missing\n                             * 'min' = keep minimum value (--merge-qual-mode\n                                       default, not applicable to others)\n                             * 'np-union' = keep all non-PASS values\n                                            (--merge-filter-mode default, not\n                                            applicable to others)\n  --merge-pheno-sort <m>   : Set sort order for phenotype columns and INFO\n  --merge-info-sort <mode>   entries when merging.\n                             * 'none'/'0' = keep in loaded order (default)\n                             * 'ascii'/'a' = ASCII order\n                             * 'natural'/'n' = natural sort\n  --merge-max-allele-ct <> : Exclude merged variants with more than the\n                             specified number of alleles.\n  --multiallelics-already-joined : Prevent --pmerge[-list] from erroring out\n                                   when a .pvar file appears to have a 'split'\n                                   multiallelic variant.\n  --king-table-filter <min>      : Specify minimum kinship coefficient for\n                                   inclusion in --make-king-table report.\n  --king-table-subset <f> [kmin] : Restrict current --make-king-table run to\n                                   sample pairs listed in the given .kin0 file.\n                                   If a second argument is provided, only\n                                   sample pairs with kinship >= that threshold\n                                   (in the input .kin0) are processed.\n  --condition <variant ID> [{dominant | recessive}] ['multiallelic']\n  --condition-list <fname> [{dominant | recessive}] ['multiallelic'] :\n    Add the given variant, or all variants in the given file, as --glm\n    covariates.\n    By default, this errors out if any of the variants are multiallelic; add\n    the 'multiallelic' ('m' for short) modifier to allow them.  They'll\n    effectively be split against the major allele (unless --glm's 'omit-ref'\n    modifier was specified), and all induced covariate names--even for\n    biallelic variants--will have an underscore followed by the allele code at\n    the end.\n  --parameters <...> : Include only the given covariates/interactions in the\n                       --glm model, identified by a list of 1-based indices\n                       and/or ranges of them.\n  --tests <...>      : Perform a (joint) test on the specified term(s) in the\n  --tests all          --glm model, identified by 1-based indices and/or ranges\n                       of them.\n                       * Note that, when --parameters is also present, the\n                         indices refer to the terms remaining AFTER pruning by\n                         --parameters.\n                       * You can use '--tests all' to include all terms.\n  --vif <max VIF>    : Set VIF threshold for --glm multicollinearity check\n                       (default 50).  (This is no longer skipped for\n                       case/control phenotypes.)\n  --max-corr <val>   : Skip --glm regression when the absolute value of the\n                       correlation between two predictors exceeds this value\n                       (default 0.999).\n  --xchr-model <m>   : Set the chrX --glm/--condition[-list]/--[v]score model.\n                       * '0' = skip chrX.\n                       * '1' = add sex as a covar on chrX, code males 0..1.\n                       * '2' (default) = chrX sex covar, code males 0..2.\n                       (Use the --glm 'interaction' modifier to test for\n                       interaction between genotype and sex.)\n  --adjust ['zs'] ['gc'] ['log10'] ['cols='<column set descriptor>] :\n    For each association test in this run, report some basic multiple-testing\n    corrections, sorted in increasing-p-value order.  Modifiers work the same\n    way as they do on --adjust-file.\n  --lambda                   : Set genomic control lambda for --adjust[-file].\n  --adjust-chr-field <n...>  : Set --adjust-file input field names.  When\n  --adjust-pos-field <n...>    multiple arguments are given to these flags,\n  --adjust-id-field <n...>     earlier names take precedence over later ones.\n  --adjust-ref-field <n...>\n  --adjust-alt-field <n...>\n  --adjust-a1-field <n...>\n  --adjust-test-field <n...>\n  --adjust-p-field <n...>\n  --ci <size>        : Report confidence ratios for odds ratios/betas.\n  --pfilter <val>    : Filter out assoc. test results with higher p-values.\n  --score-col-nums <...> : Process all the specified coefficient columns in the\n                           --score file, identified by 1-based indexes and/or\n                           ranges of them.\n  --q-score-range <range file> <data file> [i] [j] ['header'] ['min'] :\n    Apply --score to subset(s) of variants in the primary score list(s) based\n    on e.g. p-value ranges.\n    * The first file should have range labels in the first column, p-value\n      lower bounds in the second column, and upper bounds in the third column.\n      Lines with too few entries, or nonnumeric values in the second or third\n      column, are ignored.\n    * The second file should contain a variant ID and a p-value on each line\n      (except possibly the first).  Variant IDs are read from column #i and\n      p-values are read from column #j, where i defaults to 1 and j defaults to\n      i+1.  The 'header' modifier causes the first nonempty line of this file\n      to be skipped.\n    * By default, --q-score-range errors out when a variant ID appears multiple\n      times in the data file (and is also present in the main dataset).  To use\n      the minimum p-value in this case instead, add the 'min' modifier.\n  --vscore-col-nums <...> : Process all the specified coefficient columns in\n                            the --variant-score file, identified by 1-based\n                            indexes and/or ranges of them.\n  --parallel <k> <n> : Divide the output matrix into n pieces, and only compute\n                       the kth piece.  The primary output file will have the\n                       piece number included in its name, e.g. plink2.king.13\n                       or plink2.king.13.zst if k is 13.  Concatenating these\n                       files in order will yield the full matrix of interest.\n                       (Yes, this can be done before decompression.)\n                       N.B. This generally cannot be used to directly write a\n                       symmetric square matrix.  Choose square0 or triangle\n                       shape instead, and postprocess as necessary.\n  --memory <val> ['require'] : Set size, in MiB, of initial workspace malloc\n                               attempt.  To error out instead of reducing the\n                               request size when the initial attempt fails, add\n                               the 'require' modifier.\n  --threads <val>    : Set maximum number of compute threads.\n  --d <char>         : Change variant/covariate range delimiter (normally '-').\n  --seed <val...>    : Set random number seed(s).  Each value must be an\n                       integer between 0 and 4294967295 inclusive.\n                       Note that --threads and \"--memory require\" may also be\n                       needed to reproduce some randomized runs.\n  --native           : Allow Intel MKL to use processor-dependent code paths.\n  --output-min-p <p> : Specify minimum p-value to write to reports.  (2.23e-308\n                       is useful for preventing underflow in some programs.)\n  --debug            : Use slower, more crash-resistant logging method.\n  --randmem          : Randomize initial workspace memory (helps catch\n                       uninitialized-memory bugs).\n  --warning-errcode  : Return a nonzero error code to the OS when a run\n                       completes with warning(s).\n  --zst-level <lvl>  : Set the Zstd compression level (1-22, default 3).\n\nPrimary methods paper:\nChang CC, Chow CC, Tellier LCAM, Vattikuti S, Purcell SM, Lee JJ (2015)\nSecond-generation PLINK: rising to the challenge of larger and richer datasets.\nGigaScience, 4.\n```\n","dir":"/applications/","name":"plink.md","path":"applications/plink.md","url":"/applications/plink.html"},{"sort":26,"layout":"default","title":"polyphen-2","content":"# polyphen-2\n\nOfficial page: [http://genetics.bwh.harvard.edu/pph2/dokuwiki/start](http://genetics.bwh.harvard.edu/pph2/dokuwiki/start).\n\nThe setup can be furnished as follows,\n\n```bash\ncd $HPC_WORK\nwget -qO- http://genetics.bwh.harvard.edu/pph2/dokuwiki/_media/polyphen-2.2.2r405c.tar.gz | tar xfz\nwget -qO- ftp://genetics.bwh.harvard.edu/pph2/bundled/polyphen-2.2.2-databases-2011_12.tar.bz2 | tar xjf\nwget -qO- ftp://genetics.bwh.harvard.edu/pph2/bundled/polyphen-2.2.2-alignments-mlc-2011_12.tar.bz2 | tar xjf\nwget -qO- ftp://genetics.bwh.harvard.edu/pph2/bundled/polyphen-2.2.2-alignments-multiz-2009_10.tar.bz2 | tar xjf\nls  | sed 's/\\*//g' | parallel -C' ' 'ln -sf $HPC_WORK/polyphen-2.2.2/bin/{} $HPC_WORK/bin/{}'\ncd polyphen-2.2.2\n# set up BLAST/nrdb/PDB as decribed below\ncd src\nmake\nmake install\ncd -\nconfigure\ncd bin\nrsync -aP rsync://hgdownload.soe.ucsc.edu/genome/admin/exe/linux.x86_64/ ./\ncd -\n```\n\nThe MLC/MULTIZ databases need to be extracted to $HOME and symbolically linked if the number of files exceed 1 million\n(limit on RDS). Then these are necessary,\n\n```bash\ncd $HPC_WORK/polyphen-2.2.2\nln -s $HOME/polyphen-2.2.2/precompiled\ncd ucsc/hg19/multiz\nln -s $HOME/polyphen-2.2.2/ucsc/hg19/multiz/precomputed\n```\n\nThe availability of MLC/MULTIZ databases make the annotation considerably faster.\n\nThe command `configure` creates files at config/ which can be changed maunaually. There is also\n[user's guide](http://genetics.bwh.harvard.edu/pph2/dokuwiki/_media/hg0720.pdf). The line `rsync` obtains\nprograms such as `twoBitToFa` as required by the example below.\n\nBLAST and nrdb can be set up as follows,\n\n```bash\nrmdir blast\nln -sf /usr/local/Cluster-Apps/blast/2.4.0 blast\ncd nrdb\nwget -qO- ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/uniref/uniref100/uniref100.fasta.gz | \\\ngunzip -c > uniref100.fasta\n../update/format_defline.pl uniref100.fasta >uniref100-formatted.fasta\n../blast/bin/makeblastdb -in uniref100-formatted.fasta -dbtype prot -out uniref100 -parse_seqids\nrm -f uniref100.fasta uniref100-formatted.fasta\n```\n\nand for PDB\n\n```bash\nrsync -rltv --delete-after --port=33444 \\\n      rsync.wwpdb.org::ftp/data/structures/divided/pdb/ wwpdb/divided/pdb/\nrsync -rltv --delete-after --port=33444 \\\n      rsync.wwpdb.org::ftp/data/structures/all/pdb/ wwpdb/all/pdb/\n```\n\nOur test is then,\n\n```bash\ncd $HPC_WORK/polyphen-2.2.2\nrun_pph.pl sets/test.input 1>test.pph.output 2>test.pph.log\nrun_weka.pl test.pph.output >test.humdiv.output\nrun_weka.pl -l models/HumVar.UniRef100.NBd.f11.model test.pph.output >test.humvar.output\n\nsdiff test.humdiv.output sets/test.humdiv.output\nsdiff test.humvar.output sets/test.humvar.output\n```\n\nNow we turn to an genomic SNPs query examples with snps.pph.list containing the following line,\n\n> chr1:154426970 A/C\n\nto be called by `mapsnps.pl` and others.\n\n```bash\nmapsnps.pl -g hg19 -m -U -y snps.pph.input snps.pph.list 1>snps.pph.features 2>snps.log\nrun_pph.pl snps.pph.input 1>snps.pph.output 2>snps.pph.log\nrun_weka.pl snps.pph.output >snps.humdiv.output\nrun_weka.pl -l models/HumVar.UniRef100.NBd.f11.model snps.pph.output >snps.humvar.output\n```\n\nfor [.pph.input](/applications/files/snps.pph.input), [.pph.features](/applications/files/snps.pph.features), [.pph.output](/applications/files/snps.pph.output), [.humvar.output](/applications/files/snps.humvar.output) and [.humdiv.output](/applications/files/snps.humdiv.output).\n","dir":"/applications/","name":"polyphen-2.md","path":"applications/polyphen-2.md","url":"/applications/polyphen-2.html"},{"sort":27,"layout":"default","title":"poppler","content":"# poppler\n\nOfficial page: [https://poppler.freedesktop.org/](https://poppler.freedesktop.org/).\n\nWe work on the latest version, 0.84.0.\n\n```bash\nmodule load xz/5.2.2\nwget https://poppler.freedesktop.org/poppler-0.84.0.tar.xz\ntar xf poppler-0.84.0.tar.xz\ncd poppler-0.84.0\nmkdir build\ncd build\nmodule load gcc/5\nmodule load openjpeg-2.1-gcc-5.4.0-myd2p3o\ncmake ..\nmake\nmake install\n```\n\nNote it is necessary to change prefix, cc and c++ to gcc and g++ in line with gcc/5, e.g.,\n\n```\nCMAKE_INSTALL_PREFIX:PATH=/rds/user/$USER/hpc-work\nCMAKE_C_COMPILER:FILEPATH=/usr/local/software/master/gcc/5/bin/gcc\nCMAKE_CXX_COMPILER:FILEPATH=/usr/local/software/master/gcc/5/bin/g++\n```\n\nwhich could be done by editing CMakeCache.txt and/or calling `ccmake .`. After these we have\nthe followihg utilities:\npdfattach, pdfdetach, pdffonts, pdfimages, pdfinfo, pdfseparate, pdftocairo, pdftohtml, pdftoppm, pdftops, pdftotext, pdfunite,\nand following libraries: libpoppler.so, libpoppler-cpp.so, libpoppler-glibc.so, libpoppler-qt5.so.\n\nNote that `poppler-cpp.pc` is installed at `hpc-work/lib64/pkgconfig` and preferably in the search path with\n\n```bash\nexport PKG_CONFIG_PATH=$HPC_WORK/lib/pkgconfig:$HPC_WORK/lib64/pkgconfig:$PKG_CONFIG_PATH\n```\n\nafter which we could install R/pdftools from CRAN.\n\nNote also the packages `R/Rpoppler`, `pdftools` and `qpdf` from CRAN.\n","dir":"/applications/","name":"poppler.md","path":"applications/poppler.md","url":"/applications/poppler.html"},{"sort":28,"layout":"default","title":"PRSice","content":"<h1 id=\"prsice\">PRSice</h1>\n\n<p>Web page: https://github.com/choishingwan/PRSice and http://www.prsice.info/.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">cd</span> <span class=\"nv\">$HPC_WORK</span>\ngit clone https://github.com/choishingwan/PRSice\n<span class=\"nb\">cd </span>PRSice\n<span class=\"nb\">mkdir </span>build\n<span class=\"nb\">cd </span>build\nmodule load cmake/3.9\ncmake ..\nmake\n<span class=\"nb\">ln</span> <span class=\"nt\">-sf</span> <span class=\"nv\">$HPC_WORK</span>/PRSice/bin/PRSice <span class=\"nv\">$HPC_WORK</span>/bin/PRSice\nwget https://github.com/choishingwan/PRS-Tutorial/raw/master/resources/GIANT.height.gz\nwget <span class=\"nt\">-qO-</span> https://github.com/choishingwan/PRS-Tutorial/raw/master/resources/EUR.zip | jar xv\n</code></pre> </div></div>\n\n<p>The last two commands download/unpack the documentation example, which is described here, https://choishingwan.github.io/PRS-Tutorial/, whose scripts are partly extracted <a href=\"/applications/files/pgs.sh\">here</a>.</p>\n","dir":"/applications/","name":"PRSice.md","path":"applications/PRSice.md","url":"/applications/PRSice.html"},{"sort":29,"layout":"default","title":"PRSoS","content":"<h1 id=\"prsos\">PRSoS</h1>\n\n<p>Web page: https://github.com/MeaneyLab/PRSoS.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>git clone https://github.com/MeaneyLab/PRSoS.git\n<span class=\"nb\">cd </span>PRSoS\npip <span class=\"nb\">install</span> –r requirements.txt\n~/.local/bin/spark-submit <span class=\"nt\">--master</span> <span class=\"nb\">local</span><span class=\"o\">[</span><span class=\"k\">*</span><span class=\"o\">]</span> PRSoS.py examples/example.vcf examples/gwasfile.txt test_output\n</code></pre> </div></div>\n","dir":"/applications/","name":"PRSoS.md","path":"applications/PRSoS.md","url":"/applications/PRSoS.html"},{"sort":30,"layout":"default","title":"pspp","content":"# pspp\n\nOfficial page: [https://www.gnu.org/software/pspp/](https://www.gnu.org/software/pspp/).\n\n```bash\nmodule load pspp/1.2.0\npspp example.sps\n```\n\nwhere [example.sps](/applications/files/example.sps) is the documentation example. Nevertheless `psppire` is not yet functioning.\n\nHowever, it is possible to compile it directly by using\n\n- gtksourceview 4.0.3 (4.4.0 is more demanding with Python 3.5, meson, Vala, etc.) and use PKG_CONFIG_PATH when appropriate\n- spread-sheet-widget-0.3\n- fribidi-1.0.8\n- GTKSOURVIEW_CFLAGS and GTKSOURVIEW_LIBS in the configuration.\n\n```bash\nexport PREFIX=/rds/user/$USER/hpc-work\nexport GTKSOURCEVIEW_CFLAGS=-I${PREFIX}/include/gtksourceview-4\nexport GTKSOURCEVIEW_LIBS=\"-L${PREFIX}/lib -lgtksourceview-4\"\n./configure --prefix=${PREFIX}\nmake\nmake install\n```\n\nnote that it is necessary to comment on the statement `kludge = gtk_source_view_get_type ();` from `src/ui/gui/widgets.c`\nand to remove the `PREFIX=` specification in the Perl part of compiling, i.e,\n\n```\ncd perl-module\n/usr/bin/perl Makefile.PL PREFIX=/rds/user/$USER/hpc-work OPTIMIZE=\"-g -O2 -I/rds-d4/user/$USER/hpc-work/include/fribidi -I/usr/include/cairo -I/usr/include/glib-2.0 -I/usr/lib64/glib-2.0/include -I/usr/include/pixman-1 -I/usr/include/freetype2 -I/usr/include/libpng15 -I/usr/include/uuid -I/usr/include/libdrm -I/usr/include/pango-1.0 -I/usr/include/harfbuzz  \"\n```\n\nNow we can execute [plot.sps](/applications/files/plot.sps)\n\n```bash\npsppire plot.ps &\n```\n\nMore documentation examples are in the [examples](files/examples) directory.\n\nVersion 1.4.1 requires spread-sheet-widget 0.6 and we follow the following steps,\n\n```bash\nwget http://alpha.gnu.org/gnu/ssw/spread-sheet-widget-0.7.tar.gz\ntar xvfz spread-sheet-widget-0.7.tar.gz\ncd spread-sheet-widget-0.7\n./configure --prefix=$HPC_WORK\nmake\nmake install\nexport PREFIX=/rds/user/$USER/hpc-work\nexport GTKSOURCEVIEW_CFLAGS=-I${PREFIX}/include/gtksourceview-4\nexport GTKSOURCEVIEW_LIBS=\"-L${PREFIX}/lib -lgtksourceview-4\"\nwget http://mirror.lyrahosting.com/gnu/pspp/pspp-1.4.1.tar.gz\ntar xvfz pspp-1.4.1.tar.gz\ncd pspp-1.4.1\n./configure --prefix=${PREFIX}\n```\n\nAgain we removed PREFIX= in call to perl-module/Makefile.PL.\n\nFinally, the LD = flag was not set in perl-module/Makefile and we use LD = 'gcc'.\n\n---\n\nThe package seems really attractive with its support for over 1 billion cases and 1 billion variables, while giving the appeal of data visualisation in SPSS. The following is an example code generated by the package to read/list GREAT annotation.\n\n```pspp\nGET DATA\n  /TYPE=TXT\n  /FILE=\"IL12B-all.tsv\"\n  /ARRANGEMENT=DELIMITED\n  /DELCASE=LINE\n  /FIRSTCASE=2\n  /DELIMITERS=\"\\t\"\n  /VARIABLES=\n    Ontology A21\n    ID A10\n    Desc A158\n    BinomRank F3.0\n    BinomP F12.7\n    BinomBonfP F9.0\n    BinomFdrQ F10.2\n    RegionFoldEnrich F9.5\n    ExpRegions F12.7\n    ObsRegions F1.0\n    GenomeFrac F12.8\n    SetCov F5.2\n    HyperRank F3.0\n    HyperP F12.7\n    HyperBonfP F9.0\n    HyperFdrQ F9.2\n    GeneFoldEnrich F9.5\n    ExpGenes F12.7\n    ObsGenes F2.0\n    TotalGenes F5.0\n    GeneSetCov F6.3\n    TermCov F12.7\n    Regions A193\n    Genes A98.\nVARIABLE LEVEL Ontology (SCALE).\nVARIABLE ALIGNMENT Ontology (RIGHT).\nVARIABLE WIDTH Ontology (8).\nVARIABLE LEVEL ID (SCALE).\nVARIABLE ALIGNMENT ID (RIGHT).\nVARIABLE WIDTH ID (8).\nVARIABLE LEVEL Desc (SCALE).\nVARIABLE ALIGNMENT Desc (RIGHT).\nVARIABLE WIDTH Desc (8).\nVARIABLE LEVEL Regions (SCALE).\nVARIABLE ALIGNMENT Regions (RIGHT).\nVARIABLE WIDTH Regions (8).\nVARIABLE LEVEL Genes (SCALE).\nVARIABLE ALIGNMENT Genes (RIGHT).\nVARIABLE WIDTH Genes (8).\nlist.\n```\n","dir":"/applications/","name":"pspp.md","path":"applications/pspp.md","url":"/applications/pspp.html"},{"sort":31,"layout":"default","title":"qpdf","content":"# qpdf\n\n## GitHub:\n\nWeb page [https://github.com/qpdf/qpdf](https://github.com/qpdf/qpdf).\n\n```bash\nmodule load zlib/1.2.11\ngit clone https://github.com/qpdf/qpdf\ncd qpdf\nconfigure --prefix=${HPC_WORK}\nmake\nmake install\n```\n\n## Sourceforge\n\nWeb page: [https://sourceforge.net/projects/qpdf/](https://sourceforge.net/projects/qpdf/).\n\n```bash\nmodule load zlib/1.2.11\nexport version=10.4.0\ncd $HPC_WORK\nwget -qO- --no-check-certificate https://sourceforge.net/projects/qpdf/files/qpdf/${version}/qpdf-${version}.tar.gz | \\\ntar xvfz -\ncd qpdf-${version}\n./configure --prefix=$HPC_WORK\nmake\nmake install\n```\n","dir":"/applications/","name":"qpdf.md","path":"applications/qpdf.md","url":"/applications/qpdf.html"},{"sort":32,"layout":"default","title":"regenie","content":"# regenie\n\nWeb: [https://github.com/rgcgithub/regenie](https://github.com/rgcgithub/regenie) ([documentation](https://rgcgithub.github.io/regenie/))\n\nIt is the easiest to use the Centos 7 distribution,\n\n```bash\nexport version=v2.2.4\nwget -qO- https://github.com/rgcgithub/regenie/releases/download/${version}/regenie_${version}.gz_x86_64_Centos7_mkl.zip | \\\ngunzip -c > regenie_${version}\nchmod +x regenie_${version}\nln -sf regenie_${version} regenie\nregenie --help\n```\n\nThe last command gives the following information (Why .gz in the banner?),\n\n```\n              |=============================|\n              |      REGENIE v2.2.4.gz      |\n              |=============================|\n\nCopyright (c) 2020-2021 Joelle Mbatchou, Andrey Ziyatdinov and Jonathan Marchini.\nDistributed under the MIT License.\nCompiled with Boost Iostream library.\nUsing Intel MKL with Eigen.\n\n\nUsage:\n  regenie [OPTION...]\n\n  -h, --help      print list of available options\n      --helpFull  print list of all available options\n\n Main options:\n      --step INT                specify if fitting null model (=1) or\n                                association testing (=2)\n      --bed PREFIX              prefix to PLINK .bed/.bim/.fam files\n      --pgen PREFIX             prefix to PLINK2 .pgen/.pvar/.psam files\n      --bgen FILE               BGEN file\n      --sample FILE             sample file corresponding to BGEN file\n      --ref-first               use the first allele as the reference for\n                                BGEN or PLINK bed/bim/fam input format [default\n                                assumes reference is last]\n      --keep FILE               comma-separated list of files listing samples\n                                to retain in the analysis (no header; starts\n                                with FID IID)\n      --remove FILE             comma-separated list of files listing samples\n                                to remove from the analysis (no header;\n                                starts with FID IID)\n      --extract FILE            comma-separated list of files with IDs of\n                                variants to retain in the analysis\n      --exclude FILE            comma-separated list of files with IDs of\n                                variants to remove from the analysis\n  -p, --phenoFile FILE          phenotype file (header required starting with\n                                FID IID)\n      --phenoCol STRING         phenotype name in header (use for each\n                                phenotype to keep; can use parameter expansion\n                                {i:j})\n      --phenoColList STRING,..,STRING\n                                comma separated list of phenotype names to\n                                keep (can use parameter expansion {i:j})\n  -c, --covarFile FILE          covariate file (header required starting with\n                                FID IID)\n      --covarCol STRING         covariate name in header (use for each\n                                covariate to keep; can use parameter expansion\n                                {i:j})\n      --covarColList STRING,..,STRING\n                                comma separated list of covariate names to\n                                keep (can use parameter expansion {i:j})\n      --catCovarList STRING,..,STRING\n                                comma separated list of categorical\n                                covariates\n  -o, --out PREFIX              prefix for output files\n      --qt                      analyze phenotypes as quantitative\n      --bt                      analyze phenotypes as binary\n  -1, --cc12                    use control=1,case=2,missing=NA encoding for\n                                binary traits\n  -b, --bsize INT               size of genotype blocks\n      --cv INT(=5)              number of cross validation (CV) folds\n      --loocv                   use leave-one out cross validation (LOOCV)\n      --l0 INT(=5)              number of ridge parameters to use when\n                                fitting models within blocks [evenly spaced in\n                                (0,1)]\n      --l1 INT(=5)              number of ridge parameters to use when\n                                fitting model across blocks [evenly spaced in (0,1)]\n      --lowmem                  reduce memory usage by writing level 0\n                                predictions to temporary files\n      --lowmem-prefix PREFIX    prefix where to write the temporary files in\n                                step 1 (default is to use prefix from --out)\n      --split-l0 PREFIX,N       split level 0 across N jobs and set prefix of\n                                output files\n      --run-l0 FILE,K           run level 0 for job K in {1..N} using master\n                                file created from '--split-l0'\n      --run-l1 FILE             run level 1 using master file from\n                                '--split-l0'\n      --keep-l0                 avoid deleting the level 0 predictions\n                                written on disk after fitting the level 1 models\n      --strict                  remove all samples with missingness at any of\n                                the traits\n      --print-prs               also output polygenic predictions without\n                                using LOCO (=whole genome PRS)\n      --gz                      compress output files (gzip format)\n      --apply-rint              apply Rank-Inverse Normal Transformation to\n                                quantitative traits\n      --threads INT             number of threads\n      --pred FILE               file containing the list of predictions files\n                                from step 1\n      --ignore-pred             skip reading predictions from step 1\n                                (equivalent to linear/logistic regression with only\n                                covariates)\n      --use-prs                 when using whole genome PRS step 1 output in\n                                '--pred'\n      --write-samples           write IDs of samples included for each trait\n                                (only in step 2)\n      --minMAC FLOAT(=5)        minimum minor allele count (MAC) for tested\n                                variants\n      --minINFO DOUBLE(=0)      minimum imputation info score (Impute/Mach\n                                R^2) for tested variants\n      --no-split                combine asssociation results into a single\n                                for all traits\n      --firth                   use Firth correction for p-values less than\n                                threshold\n      --approx                  use approximation to Firth correction for\n                                computational speedup\n      --spa                     use Saddlepoint approximation (SPA) for\n                                p-values less than threshold\n      --pThresh FLOAT(=0.05)    P-value threshold below which to apply\n                                Firth/SPA correction\n      --write-null-firth        store coefficients from null models with\n                                approximate Firth for step 2\n      --compute-all             store Firth estimates for all chromosomes\n      --use-null-firth FILE     use stored coefficients for null model in\n                                approximate Firth\n      --chr STRING              specify chromosome to test in step 2 (use for\n                                each chromosome)\n      --chrList STRING,..,STRING\n                                Comma separated list of chromosomes to test\n                                in step 2\n      --range CHR:MINPOS-MAXPOS\n                                to specify a physical position window for\n                                variants to test in step 2\n      --sex-specific STRING     for sex-specific analyses (male/female)\n      --af-cc                   print effect allele frequencies among\n                                cases/controls for step 2\n      --test STRING             'additive', 'dominant' or 'recessive'\n                                (default is additive test)\n      --set-list FILE           file with sets definition\n      --extract-sets FILE       comma-separated list of files with IDs of\n                                sets to retain in the analysis\n      --exclude-sets FILE       comma-separated list of files with IDs of\n                                sets to remove from the analysis\n      --extract-setlist STRING  comma separated list of sets to retain in the\n                                analysis\n      --exclude-setlist STRING  comma separated list of sets to remove from\n                                the analysis\n      --anno-file FILE          file with variant annotations\n      --anno-labels FILE        file with labels to annotations\n      --mask-def FILE           file with mask definitions\n      --aaf-file FILE           file with AAF to use when building masks\n      --aaf-bins FLOAT,..,FLOAT\n                                comma separated list of AAF bins cutoffs for\n                                building masks\n      --build-mask STRING       rule to construct masks, can be 'max', 'sum'\n                                or 'comphet' (default is max)\n      --singleton-carrier       define singletons as variants with a single\n                                carrier in the sample\n      --write-mask              write masks in PLINK bed/bim/fam format\n      --mask-lovo STRING        apply Leave-One-Variant-Out (LOVO) scheme\n                                when building masks\n                                (<set_name>,<mask_name>,<aaf_cutoff>)\n      --mask-lodo STRING        apply Leave-One-Domain-Out (LODO) scheme when\n                                building masks\n                                (<set_name>,<mask_name>,<aaf_cutoff>)\n      --skip-test               skip computing association tests after\n                                building masks\n      --check-burden-files      check annotation file, set list file and mask\n                                file for consistency\n      --strict-check-burden     to exit early if the annotation, set list and\n                                mask definition files don't agree\n\nFor more information, use option '--help' or visit the website: https://rgcgithub.github.io/regenie/\n\n```\n\n## Reference\n\nMbatchou, J., Barnard, L., Backman, J. et al. Computationally efficient whole-genome regression for quantitative and binary traits. Nat Genet 53, 1097–1103 (2021). https://doi.org/10.1038/s41588-021-00870-7\n","dir":"/applications/","name":"regenie.md","path":"applications/regenie.md","url":"/applications/regenie.html"},{"sort":33,"layout":"default","title":"R","content":"<h1 id=\"r\">R</h1>\n\n<p>To compile all the PDF documentations, load texlive.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>module load gcc/6\nmodule load pcre/8.38\nmodule load texlive\nwget https://cran.r-project.org/src/base/R-4/R-4.0.0.tar.gz\n<span class=\"nb\">tar </span>xvfz R-4.0.0.tar.gz\n<span class=\"nb\">cd </span>R-4.0.0\n<span class=\"nb\">export </span><span class=\"nv\">prefix</span><span class=\"o\">=</span>/rds-d4/user/<span class=\"nv\">$USER</span>/hpc-work\n./configure <span class=\"nt\">--prefix</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">prefix</span><span class=\"k\">}</span> <span class=\"se\">\\</span>\n            <span class=\"nt\">--with-pcre1</span> <span class=\"se\">\\</span>\n            <span class=\"nt\">--enable-R-shlib</span> <span class=\"nv\">CPPFLAGS</span><span class=\"o\">=</span><span class=\"nt\">-I</span><span class=\"k\">${</span><span class=\"nv\">prefix</span><span class=\"k\">}</span>/include <span class=\"nv\">LDFLAGS</span><span class=\"o\">=</span><span class=\"nt\">-L</span><span class=\"k\">${</span><span class=\"nv\">prefix</span><span class=\"k\">}</span>/lib\nmake\nmake <span class=\"nb\">install</span>\n</code></pre> </div></div>\n\n<p>With this setup, <code class=\"language-plaintext highlighter-rouge\">R CMD check --as-cran</code> for a CRAN package check can be run smoothly.</p>\n\n<p>Package reinstallation could be done with <code class=\"language-plaintext highlighter-rouge\">update.packages(checkBuilt = TRUE, ask = FALSE)</code>.</p>\n\n<h2 id=\"libiconv\">libiconv</h2>\n\n<p>It may complain about <code class=\"language-plaintext highlighter-rouge\">libiconv.so.2</code>,</p>\n\n<blockquote>\n  <p>… R: error while loading shared libraries: libiconv.so.2: cannot open shared object file: No such file or directory</p>\n</blockquote>\n\n<p>which can be installed as follows,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget <span class=\"nt\">-qO-</span> https://ftp.gnu.org/pub/gnu/libiconv/libiconv-1.16.tar.gz | <span class=\"nb\">tar </span>xvfz -\n<span class=\"nb\">cd </span>libiconv-1.16\n./configure <span class=\"nt\">--prefix</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>\nmake\nmake <span class=\"nb\">install</span>\n</code></pre> </div></div>\n\n<p>We could also try from <code class=\"language-plaintext highlighter-rouge\">module avail libiconv</code>.</p>\n\n<h2 id=\"icelake\">icelake</h2>\n\n<p>As CSD3 often experiences problem from the login nodes, it is then desirable to use <code class=\"language-plaintext highlighter-rouge\">login-icelake.hpc.cam.ac.uk</code>.</p>\n\n<p>However, there will be complaints about availability of <code class=\"language-plaintext highlighter-rouge\">libreadline.so.6</code> and then <code class=\"language-plaintext highlighter-rouge\">libiccuuc.so.50</code> which can be got around with their installations.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>error while loading shared libraries: libreadline.so.6: cannot open shared object file: No such file or directory\n...\nerror while loading shared libraries: libicuuc.so.50: cannot open shared object file: No such file or directory\n...\nerror while loading shared libraries: libgnutls.so.28: cannot open shared object file: No such file or directory\n...\nlibpng15.so.15: cannot open shared object file: No such file or directory\n</code></pre> </div></div>\n\n<p>The setup below complies with ordinary login nodes.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget <span class=\"nt\">-qO-</span> <span class=\"nt\">--no-check-certificate</span> https://ftp.gnu.org/pub/pub/gnu/readline/readline-6.3.tar.gz | <span class=\"nb\">tar </span>xfvz -\n<span class=\"nb\">cd </span>readline-6.3/\n./configure <span class=\"nt\">--prefix</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>\nmake\nmake <span class=\"nb\">install</span>\n</code></pre> </div></div>\n\n<p>while <code class=\"language-plaintext highlighter-rouge\">libicuuc.so.50</code> can be installed following similar procedure, their availability is as follows,</p>\n\n<ul>\n  <li><a href=\"https://github.com/unicode-org/icu/releases/tag/release-50-2\">https://github.com/unicode-org/icu/releases/tag/release-50-2</a></li>\n  <li><a href=\"https://github.com/unicode-org/icu/archive/refs/tags/release-50-2.tar.gz\">https://github.com/unicode-org/icu/archive/refs/tags/release-50-2.tar.gz</a></li>\n</ul>\n\n<p>Finally, <code class=\"language-plaintext highlighter-rouge\">libgnutls.so.28</code><sup id=\"fnref:1\" role=\"doc-noteref\"><a href=\"#fn:1\" class=\"footnote\">1</a></sup> requires <code class=\"language-plaintext highlighter-rouge\">nettle-2.7.1</code>.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>module load autogen-5.18.12-gcc-5.4.0-jn2mr4n\nmodule load gettext-0.19.8.1-gcc-5.4.0-zaldouz\n\n<span class=\"nb\">cd</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>\nwget <span class=\"nt\">-qO-</span> wget http://www.lysator.liu.se/~nisse/archive/nettle-2.7.1.tar.gz | <span class=\"se\">\\</span>\n<span class=\"nb\">tar </span>xfvz -\n<span class=\"nb\">cd </span>nettle-2.7.1/\nconfigure <span class=\"nt\">--prefix</span><span class=\"o\">=</span><span class=\"nv\">$HPC_WORK</span>\nmake\nmake <span class=\"nb\">install\n</span>wget <span class=\"nt\">--no-check-certificate</span> https://www.gnupg.org/ftp/gcrypt/gnutls/v3.2/gnutls-3.2.21.tar.xz\n<span class=\"nb\">tar </span>xf gnutls-3.2.21.tar.xz\n<span class=\"nb\">cd </span>gnutls-3.2.21/\n./configure <span class=\"nt\">--prefix</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span> <span class=\"nt\">--enable-shared</span> <span class=\"nt\">--disable-non-suiteb-curves</span>\nmake\nmake <span class=\"nb\">install</span>\n</code></pre> </div></div>\n\n<ul>\n  <li><a href=\"https://gnutls.org/\">https://gnutls.org/</a></li>\n  <li><a href=\"https://www.gnupg.org/ftp/gcrypt/gnutls/v3.2/\">https://www.gnupg.org/ftp/gcrypt/gnutls/v3.2/</a></li>\n  <li><a href=\"https://gitlab.com/gnutls/gnutls/\">https://gitlab.com/gnutls/gnutls/</a></li>\n</ul>\n\n<p>The <code class=\"language-plaintext highlighter-rouge\">libpng15.so</code> can be made available from a standard GNU-ware installation as follows,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">cd</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>\nwget <span class=\"nt\">-qO-</span> https://sourceforge.net/projects/libpng/files/libpng15/1.5.30/libpng-1.5.30.tar.gz | <span class=\"se\">\\</span>\n<span class=\"nb\">tar </span>xvfz -\n<span class=\"nb\">cd </span>libpng-1.5.30\n./configure <span class=\"nt\">--prefix</span><span class=\"o\">=</span><span class=\"nv\">$HPC_WORK</span>\nmake\nmake <span class=\"nb\">install</span>\n</code></pre> </div></div>\n\n<div class=\"footnotes\" role=\"doc-endnotes\">\n  <ol>\n    <li id=\"fn:1\" role=\"doc-endnote\">\n\n      <p>On CSD3, there is a more recent module <code class=\"language-plaintext highlighter-rouge\">nettle-3.4-gcc-5.4.0-2mdpaut</code>. Moreover, with 3.5\nthere is also an option <code class=\"language-plaintext highlighter-rouge\">--with-included-unistring</code> during configuration. <a href=\"#fnref:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>\n","dir":"/applications/","name":"R.md","path":"applications/R.md","url":"/applications/R.html"},{"sort":34,"layout":"default","title":"RStudio","content":"<h1 id=\"rstudio\">RStudio</h1>\n\n<p>Web: <a href=\"https://www.rstudio.com/\">https://www.rstudio.com/</a></p>\n\n<h2 id=\"csd3-modules\">CSD3 modules</h2>\n\n<p>We first check its availability,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>module avail rstudio\n</code></pre> </div></div>\n\n<p>and it obtains</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>---------------------------------------------------------- /usr/local/Cluster-Config/modulefiles -----------------------------------------------------------\nrstudio/0.99/rstudio-0.99 rstudio/1.1.383           rstudio/1.3.1093\n</code></pre> </div></div>\n\n<p>We intend to use the most recent version with <code class=\"language-plaintext highlighter-rouge\">module load rstudio/1.3.1093; rstudio</code> but it fails with messages:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>qt.qpa.plugin: Could not load the Qt platform plugin \"xcb\" in \"\" even though it was found.\nThis application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.\n\nAvailable platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, xcb.\n\nAborted\n</code></pre> </div></div>\n\n<p>Additional information could also be seen from <code class=\"language-plaintext highlighter-rouge\">export QT_DEBUG_PLUGINS=1</code> and run <code class=\"language-plaintext highlighter-rouge\">rstudio</code> again.</p>\n\n<p>The error messages above can be bypassed with</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">export </span><span class=\"nv\">QT_PLUGIN_PATH</span><span class=\"o\">=</span>/usr/lib64/qt5/plugins\nmodule load rstudio/1.3.1093\nrstudio &amp;\n</code></pre> </div></div>\n\n<h2 id=\"rstudio-14\">RStudio 1.4</h2>\n\n<p>This is also the latest version</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget <span class=\"nt\">-qO-</span> https://download1.rstudio.org/desktop/xenial/amd64/rstudio-1.4.1106-amd64-debian.tar.gz | <span class=\"nb\">tar </span>xfz -\n<span class=\"nb\">cd </span>rstudio-1.4\nbin/rstudio\n</code></pre> </div></div>\n\n<p>However it requires openssl.so.1.0.0, which in turn requires specific installation to get away with <code class=\"language-plaintext highlighter-rouge\">No version informaiton for openssl.so.1.0.0</code>, with <code class=\"language-plaintext highlighter-rouge\">openssl.ld</code>, according to https://stackoverflow.com/questions/18390833/no-version-information-available, as follows</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>OPENSSL_1.0.0 {\n    global:\n    *;\n};\n</code></pre> </div></div>\n\n<p>and</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget <span class=\"nt\">-qO-</span> https://www.openssl.org/source/old/1.0.0/openssl-1.0.0s.tar.gz | <span class=\"nb\">tar </span>xfz -\n<span class=\"nb\">cd </span>openssl-1.0.0s\n./config <span class=\"nt\">--prefix</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span> shared <span class=\"nt\">-Wl</span>,--version-script<span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>/openssl-1.0.0s/openssl.ld\nmake\nmake <span class=\"nb\">install</span>\n</code></pre> </div></div>\n\n<p>We can proceed after setting <code class=\"language-plaintext highlighter-rouge\">export QT_PLUGIN_PATH=/usr/lib64/qt5/plugins</code> as above. It is certainly more desirable to do this only once, as follows,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">echo</span> <span class=\"s2\">\"export QT_PLUGIN_PATH=/usr/lib64/qt5/plugins\"</span> <span class=\"o\">&gt;&gt;</span> ~/.bashrc\n</code></pre> </div></div>\n\n<p>and invoke with <code class=\"language-plaintext highlighter-rouge\">source ~/.bashrc</code> from current session or automatically from the next onwards.</p>\n\n<h2 id=\"qt5\">qt/5</h2>\n\n<p>These are side notes on installation of Qt5 according to https://forums.linuxmint.com/viewtopic.php?t=306738, but no longer necessary for reasons above.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>git clone git://code.qt.io/qt/qt5.git\n<span class=\"nb\">cd </span>qt5\ngit checkout 5.12\n./init-repository\n<span class=\"nb\">export </span><span class=\"nv\">LLVM_INSTALL_DIR</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>/llvm\n../qt5/configure <span class=\"nt\">-developer-build</span> <span class=\"nt\">-opensource</span> <span class=\"nt\">-nomake</span> examples <span class=\"nt\">-nomake</span> tests\nmake\n</code></pre> </div></div>\n","dir":"/applications/","name":"RStudio.md","path":"applications/RStudio.md","url":"/applications/RStudio.html"},{"sort":35,"layout":"default","title":"shapeit3","content":"# shapeit3\n\nWeb: [https://jmarchini.org/shapeit3/](https://jmarchini.org/shapeit3/)\n\nIt could be directly used with download from Dropbox; one executable requires boost 1.60.\n\n```bash\n./shapeit3.r884.1\nmodule load boost/1.60/gcc-5.2.0-python-2.7.10\n./shapeit3.r884.2\n```\n","dir":"/applications/","name":"shapeit3.md","path":"applications/shapeit3.md","url":"/applications/shapeit3.html"},{"sort":36,"layout":"default","title":"SMR","content":"<h1 id=\"smr\">SMR</h1>\n\n<p>Web: https://cnsgenomics.com/software/smr/</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget https://cnsgenomics.com/software/smr/download/smr_1.03_src.zip\nunzip <span class=\"nt\">-v</span> smr_1.03_src.zip\n<span class=\"nb\">mkdir </span>smr\n<span class=\"nb\">cd </span>smr\nunzip ../smr_1.03_src.zip\n<span class=\"nb\">rm</span> <span class=\"nt\">-rf</span> __MACOSX/\n<span class=\"nb\">mv </span>smr_1.03_src/ ../..\n<span class=\"nb\">cd</span> ..\n<span class=\"nb\">rmdir </span>smr\n<span class=\"nb\">cd</span> ../smr_1.03_src/\nmodule load eigen/latest\nmodule load zlib/1.2.8\nmake\n./smr_linux\n</code></pre> </div></div>\n\n<p>One may also use the R code for plotting,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget https://cnsgenomics.com/software/smr/download/plot.zip\nunzip plot.zip\n</code></pre> </div></div>\n\n<p>The documentation example is done in R as follows,</p>\n\n<div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">source</span><span class=\"p\">(</span><span class=\"s2\">\"plot_SMR.r\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">SMRData</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">ReadSMRData</span><span class=\"p\">(</span><span class=\"s2\">\"ILMN_1719097.ILMN_1719097.txt\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">SMRLocusPlot</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">SMRData</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">smr_thresh</span><span class=\"o\">=</span><span class=\"m\">8.4e-6</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">heidi_thresh</span><span class=\"o\">=</span><span class=\"m\">0.05</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">plotWindow</span><span class=\"o\">=</span><span class=\"m\">1000</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">max_anno_probe</span><span class=\"o\">=</span><span class=\"m\">16</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre> </div></div>\n","dir":"/applications/","name":"SMR.md","path":"applications/SMR.md","url":"/applications/SMR.html"},{"sort":37,"layout":"default","title":"snakemake","content":"# snakemake\n\nWeb: [https://snakemake.readthedocs.io/en/stable/index.html](https://snakemake.readthedocs.io/en/stable/index.html)\n\nIt is a python-based workflow management system.\n\nWe illustrate installation through `mamba` and designated location.\n\n```bash\nmodule load miniconda3/4.5.1\nexport mypath=${HOME}/COVID-19/miniconda37\nconda create --prefix ${mypath} python=3.7 ipykernel\nconda init bash\nsource ~/.bashrc\nconda activate ${mypath}\nconda install -c conda-forge mamba\nmamba install -c conda-forge -c bioconda snakemake-minimal\nsnakemake --help\nconda deactivate\n```\n\nBy default, the installation path is ${HOME}/.conda/envs/miniconda37.\n\nAfter installation, the call later on will be simpler,\n\n```bash\nmodule load miniconda3/4.5.1\nexport mypath=${HOME}/COVID-19/miniconda37\nconda activate ${mypath}\n```\n\n## Application example\n\n[Mendelian Randomziation](https://github.com/marcoralab/MRPipeline)\n","dir":"/applications/","name":"snakemake.md","path":"applications/snakemake.md","url":"/applications/snakemake.html"},{"sort":38,"layout":"default","title":"SNPTEST","content":"<h1 id=\"snptest\">SNPTEST</h1>\n\n<p>Web: <a href=\"https://www.well.ox.ac.uk/~gav/snptest/\">https://www.well.ox.ac.uk/~gav/snptest/</a> and <a href=\"https://jmarchini.org/snptest/\">https://jmarchini.org/snptest/</a></p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">cd</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>\nwget <span class=\"nt\">-qO-</span> http://www.well.ox.ac.uk/~gav/resources/snptest_v2.5.6_CentOS_Linux7.8-x86_64_dynamic.tgz | <span class=\"se\">\\</span>\n<span class=\"nb\">tar </span>xvfz -\n<span class=\"nb\">ln</span> <span class=\"nt\">-sf</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>/snptest_v2.5.6_CentOS_Linux7.8.2003-x86_64_dynamic/snptest_v2.5.6 <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>/bin/snptest_v2.5.6\n<span class=\"o\">(</span>\n  <span class=\"nb\">echo </span>module load gcc/5\n  <span class=\"nb\">echo</span> <span class=\"s2\">\"snptest_v2.5.6 </span><span class=\"nv\">$@</span><span class=\"s2\">\"</span>\n<span class=\"o\">)</span> <span class=\"o\">&gt;</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>/bin/snptest\n</code></pre> </div></div>\n\n<p>Note that we have wrap up <code class=\"language-plaintext highlighter-rouge\">snptest_v2.5.6</code> inside <code class=\"language-plaintext highlighter-rouge\">snptest</code> since it requires <code class=\"language-plaintext highlighter-rouge\">gcc/5</code> or above to function properly.</p>\n","dir":"/applications/","name":"SNPTEST.md","path":"applications/SNPTEST.md","url":"/applications/SNPTEST.html"},{"sort":39,"layout":"default","title":"tabix","content":"# tabix\n\nWeb: [https://github.com/samtools/htslib](https://github.com/samtools/htslib).\n\nTheir availability on CSD3 can be seen with\n\n```bash\nmodule avail tabix\nmodule avail htslib\n```\n\nIts utility to extract data has only been realised recently\n\n```bash\ntabix ftp://ftp.ebi.ac.uk/pub/databases/spot/eQTL/csv/BLUEPRINT/ge/BLUEPRINT_ge_monocyte.all.tsv.gz 20:46120612-46120613\ntabix https://gwas.mrcieu.ac.uk/files/ebi-a-GCST010776/ebi-a-GCST010776.vcf.gz 1:1-1000000\n```\n\nwith earlier versions possibly giving error messages; we install the latest,\n\n```bash\nmodule load gcc/6\nwget -qO- https://github.com/samtools/htslib/releases/download/1.12/htslib-1.12.tar.bz2 | \\\ntar -vxjf -\ncd htslib-1.12/\nautoreconf -i\n./configure --enable-libcurl --prefix=${HPC_WORK}\nmake\nmake install\n```\n\nOne could obtain a list of outcomes as follows,\n\n```bash\nRscript -e \"write.table(TwoSampleMR::available_outcomes(),file='ao.txt',quote=FALSE,row.names=FALSE,sep='\\t')\"\n```\n\n**NOTE**\n\nSee specific section on bcftools for additional information.\n\n`bcftools query` works similarly on a local VCF file nevertheless the option `-r` is necessary.\n\n```bash\nbcftools query -f '%ID\\t%ALT\\t%REF\\t%AF\\t[%ES]\\t[%SE]\\t[%LP]\\t[%SS]\\t%CHROM\\t%POS\\n' -r 1:1-1000000 \\\n               https://gwas.mrcieu.ac.uk/files/ebi-a-GCST010776/ebi-a-GCST010776.vcf.gz\n```\n\nThe rather perplexing syntax (cut and paste from here in which case) gains its appeal for an output in a well-defined format.\n","dir":"/applications/","name":"tabix.md","path":"applications/tabix.md","url":"/applications/tabix.html"},{"sort":40,"layout":"default","title":"VEP","content":"<h1 id=\"vep\">VEP</h1>\n\n<p>Official page: <a href=\"https://www.ensembl.org/info/docs/tools/vep/index.html\">https://www.ensembl.org/info/docs/tools/vep/index.html</a>\n(<a href=\"https://www.ensembl.org/Tools/VEP\">web interface</a>).</p>\n\n<p>Detailed instructions for installation are available from here,</p>\n\n<p><a href=\"http://www.ensembl.org/info/docs/tools/vep/script/vep_download.html#installer\">http://www.ensembl.org/info/docs/tools/vep/script/vep_download.html#installer</a>.</p>\n\n<p>There are several possible ways to install under csd3: GitHub, R and docker.</p>\n\n<h2 id=\"-github-\">— GitHub —</h2>\n\n<p>GitHub Page: <a href=\"https://github.com/Ensembl/ensembl-vep\">https://github.com/Ensembl/ensembl-vep</a>.</p>\n\n<p>The ease with this option lies with GitHub in that updates can simply be made with <code class=\"language-plaintext highlighter-rouge\">git pull</code> to an exisiting release.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">cd</span> <span class=\"nv\">$HPC_WORK</span>\ngit clone https://github.com/Ensembl/ensembl-vep.git\n<span class=\"nb\">cd </span>ensembl-vep\n<span class=\"c\"># ---</span>\n<span class=\"c\"># release/98</span>\n<span class=\"c\"># module load htslib/1.4</span>\n<span class=\"c\"># perl INSTALL.pl</span>\n<span class=\"c\"># release/104</span>\n<span class=\"c\"># long form:</span>\n<span class=\"c\"># perl INSTALL.pl --DESTDIR Bio --ASSEMBLY GRCh38 --AUTO acfp --PLUGINS all --SPECIES homo_sapiens,homo_sapiens_merged --NO_TEST --CACHEDIR .vep</span>\n<span class=\"c\"># ---</span>\nmodule load htslib-1.9-gcc-5.4.0-p2taavl\n<span class=\"c\"># short form:</span>\nperl INSTALL.pl <span class=\"nt\">-l</span> Bio <span class=\"nt\">-y</span> GRCh38 <span class=\"nt\">-a</span> acfp <span class=\"nt\">-g</span> all <span class=\"nt\">-s</span> homo_sapiens,homo_sapiens_merged <span class=\"nt\">--NO_TEST</span> <span class=\"nt\">-c</span> .vep\n<span class=\"nb\">ln</span> <span class=\"nt\">-sf</span> <span class=\"nv\">$HPC_WORK</span>/ensembl-vep/.vep <span class=\"nv\">$HOME</span>/.vep\n<span class=\"c\"># set up symbolic links to the executables</span>\n<span class=\"k\">for </span>f <span class=\"k\">in </span>convert_cache.pl filter_vep haplo variant_recoder vep<span class=\"p\">;</span>\n    <span class=\"k\">do </span><span class=\"nb\">ln</span> <span class=\"nt\">-sf</span> <span class=\"nv\">$HPC_WORK</span>/ensembl-vep/<span class=\"nv\">$f</span> <span class=\"nv\">$HPC_WORK</span>/bin/<span class=\"nv\">$f</span><span class=\"p\">;</span> <span class=\"k\">done\n</span>vep <span class=\"nt\">-i</span> examples/homo_sapiens_GRCh37.vcf <span class=\"nt\">-o</span> examples/homo_sapiens_GRCh37.txt <span class=\"se\">\\</span>\n    <span class=\"nt\">--force_overwrite</span> <span class=\"nt\">--offline</span> <span class=\"nt\">--pick</span> <span class=\"nt\">--symbol</span>\n</code></pre> </div></div>\n\n<p>A (slightly edited due to two species at -s were installed separately) log file can be found here, <a href=\"/applications/files/VEP.log\">VEP.log</a>.</p>\n\n<p>Note in particular that by default, the cache files will be installed at $HOME which would exceed the quota (&lt;40GB) of an ordinary user,\nand as before the destination was redirected. The setup above facilitates storage of cache files, FASTA files and plugins.</p>\n\n<blockquote>\n  <p>The FASTA file should be automatically detected by the VEP when using –cache or –offline.</p>\n</blockquote>\n\n<blockquote>\n  <p>If it is not, use “–fasta $HOME/.vep/homo_sapiens/98_GRCh37/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa”</p>\n</blockquote>\n\n<blockquote>\n  <p>Remember to use –merged or –refseq when running the VEP with _merged or _refseq cache!</p>\n</blockquote>\n\n<p>Without the htslib module, the <code class=\"language-plaintext highlighter-rouge\">--NO_HTSLIB</code> option is needed but “Cannot use format gff without Bio::DB::HTS::Tabix module installed”.\nBio::DB:HTS is in <a href=\"https://github.com/Ensembl/Bio-DB-HTS\">https://github.com/Ensembl/Bio-DB-HTS</a> and change can be made to the <code class=\"language-plaintext highlighter-rouge\">Makefile</code> of htslibs for a desired location, to be\nused by <code class=\"language-plaintext highlighter-rouge\">Build.PL</code> via its command line parameters.</p>\n\n<p>It is notable that VEP accepts compress (.gz) input. It is worthwhile to check for the VEP plugins: <a href=\"https://github.com/Ensembl/VEP_plugins\">https://github.com/Ensembl/VEP_plugins</a>. For instance, to enable the PolyPhen_SIFT plugin we first generate the database and then annotate locally.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">cd</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>\ngit clone https://github.com/Ensembl/VEP_plugins\n<span class=\"nb\">cd </span>VEP_plugins\n<span class=\"c\"># download of database</span>\nvep <span class=\"nt\">--database</span> <span class=\"nt\">--port</span> 3337 <span class=\"nt\">--force_overwrite</span> <span class=\"nt\">--dir_plugin</span> <span class=\"nb\">.</span> <span class=\"nt\">--plugin</span> PolyPhen_SIFT,create_db<span class=\"o\">=</span>1\n<span class=\"c\"># offline annotation</span>\nvep <span class=\"nt\">-i</span> homo_sapiens_GRCh37.vcf <span class=\"nt\">-o</span> homo_sapiens_GRCh37 <span class=\"nt\">--offline</span> <span class=\"nt\">--force_overwrite</span> <span class=\"nt\">--format</span> vcf <span class=\"nt\">--dir_plugin</span> <span class=\"nb\">.</span> <span class=\"nt\">--plugin</span> PolyPhen_SIFT\n</code></pre> </div></div>\n\n<p>By default, the database port number is 5306. The <code class=\"language-plaintext highlighter-rouge\">create_db=1</code> option downloads <code class=\"language-plaintext highlighter-rouge\">homo_sapiens.PolyPhen_SIFT.db</code> at <code class=\"language-plaintext highlighter-rouge\">${HOME}/.vep</code>.</p>\n\n<p>One may wish to skip the comments (lines started with ##) in processing of the output, e.g., in R,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">export </span><span class=\"nv\">skips</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">grep</span> <span class=\"s1\">'##'</span> examples/homo_sapiens_GRCh37.txt | <span class=\"nb\">wc</span> <span class=\"nt\">-l</span><span class=\"si\">)</span>\nR <span class=\"nt\">--no-save</span> <span class=\"nt\">-q</span> <span class=\"o\">&lt;&lt;</span><span class=\"no\">END</span><span class=\"sh\">\n  vo &lt;- read.delim(\"examples/homo_sapiens_GRCh37.txt\",skip=as.numeric(Sys.getenv(\"skips\")))\n  head(vo)\n</span><span class=\"no\">END\n</span></code></pre> </div></div>\n\n<p>allowing for variable number of lines given various command-line options to be skipped to have</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>  X.Uploaded_variation    Location Allele            Gene         Feature\n1          rs116645811 21:26960070      A ENSG00000154719 ENST00000307301\n2            rs1135638 21:26965148      A ENSG00000154719 ENST00000307301\n3              rs10576 21:26965172      C ENSG00000154719 ENST00000307301\n4            rs1057885 21:26965205      C ENSG00000154719 ENST00000307301\n5          rs116331755 21:26976144      G ENSG00000154719 ENST00000307301\n6            rs7278168 21:26976222      T ENSG00000154719 ENST00000307301\n  Feature_type        Consequence cDNA_position CDS_position Protein_position\n1   Transcript   missense_variant          1043         1001              334\n2   Transcript synonymous_variant           939          897              299\n3   Transcript synonymous_variant           915          873              291\n4   Transcript synonymous_variant           882          840              280\n5   Transcript synonymous_variant           426          384              128\n6   Transcript synonymous_variant           348          306              102\n  Amino_acids  Codons Existing_variation\n1         T/M aCg/aTg                  -\n2           G ggC/ggT                  -\n3           P ccA/ccG                  -\n4           V gtA/gtG                  -\n5           L ctT/ctC                  -\n6           K aaG/aaA                  -\n                                                                     Extra\n1 IMPACT=MODERATE;STRAND=-1;SYMBOL=MRPL39;SYMBOL_SOURCE=HGNC;HGNC_ID=14027\n2      IMPACT=LOW;STRAND=-1;SYMBOL=MRPL39;SYMBOL_SOURCE=HGNC;HGNC_ID=14027\n3      IMPACT=LOW;STRAND=-1;SYMBOL=MRPL39;SYMBOL_SOURCE=HGNC;HGNC_ID=14027\n4      IMPACT=LOW;STRAND=-1;SYMBOL=MRPL39;SYMBOL_SOURCE=HGNC;HGNC_ID=14027\n5      IMPACT=LOW;STRAND=-1;SYMBOL=MRPL39;SYMBOL_SOURCE=HGNC;HGNC_ID=14027\n6      IMPACT=LOW;STRAND=-1;SYMBOL=MRPL39;SYMBOL_SOURCE=HGNC;HGNC_ID=14027\n&gt;\n</code></pre> </div></div>\n\n<p>When a particular release really works well on the system, it is possible to install it, e.g.,</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>git checkout release/98\nperl INSTALL.pl\n</code></pre> </div></div>\n\n<p>for release 98 instead of the latest from <code class=\"language-plaintext highlighter-rouge\">git pull</code>.</p>\n\n<p>It could be useful to filter VEP output, see <a href=\"https://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html\">https://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html</a>.</p>\n\n<h3 id=\"nearest-gene\"><strong>Nearest gene</strong></h3>\n\n<p>This can produces error message</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>-------------------- EXCEPTION --------------------\nMSG: ERROR: --nearest requires Set::IntervalTree perl module to be installed\n</code></pre> </div></div>\n\n<p>and we get around with</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>perl <span class=\"nt\">-MCPAN</span> <span class=\"nt\">-e</span> shell\n<span class=\"nb\">install </span>Set::IntervalTree\n</code></pre> </div></div>\n\n<p>which will enable <code class=\"language-plaintext highlighter-rouge\">--nearest gene</code>.</p>\n\n<h3 id=\"annotation-in-chunks\"><strong>Annotation in chunks</strong></h3>\n\n<p>A toy example, following <a href=\"http://www.ensembl.org/info/docs/tools/vep/script/vep_other.html#faster\">http://www.ensembl.org/info/docs/tools/vep/script/vep_other.html#faster</a>, is given as follows,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">cd </span>examples\nbgzip <span class=\"nt\">-f</span> homo_sapiens_GRCh37.vcf\ntabix <span class=\"nt\">-Cf</span> homo_sapiens_GRCh37.vcf.gz\ntabix <span class=\"nt\">-h</span>  homo_sapiens_GRCh37.vcf.gz 22:50616005-50616006 | <span class=\"se\">\\</span>\nvep <span class=\"nt\">--cache</span> <span class=\"nt\">--fork</span> 4 <span class=\"nt\">--port</span> 3337 <span class=\"nt\">--format</span> vcf <span class=\"nt\">-o</span> - <span class=\"nt\">--tab</span> <span class=\"nt\">--no_stats</span> | <span class=\"se\">\\</span>\n<span class=\"nb\">grep</span> <span class=\"nt\">-v</span> <span class=\"s1\">'##'</span>\n<span class=\"nb\">cd</span> -\n</code></pre> </div></div>\n\n<p>from this we could propagate the idea for autosomes in chunks as follows,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">export </span><span class=\"nv\">ref</span><span class=\"o\">=</span>~/rds/post_qc_data/interval/reference_files/genetic/reference_files_genotyped_imputed/\n<span class=\"nb\">export </span><span class=\"nv\">chunk_size</span><span class=\"o\">=</span>10000\n<span class=\"nb\">seq </span>22 | <span class=\"se\">\\</span>\nparallel <span class=\"nt\">-j1</span> <span class=\"nt\">--env</span> ref <span class=\"nt\">-C</span><span class=\"s1\">' '</span> <span class=\"s1\">'\n  export n=$(awk \"END{print NR-1}\" ${ref}/impute_{}_interval.snpstats)\n  export g=$(expr ${n} / ${chunk_size})\n  export s=$(expr $n - $(( $g * $chunk_size)))\n  (\n    for i in $(seq ${g}); do\n      (\n        awk \"BEGIN{print \\\"##fileformat=VCFv4.0\\\"}\"\n        awk -vOFS=\"\\t\" \"BEGIN{print \\\"#CHROM\\\",\\\"POS\\\",\\\"ID\\\",\\\"REF\\\",\\\"ALT\\\",\\\"QUAL\\\",\\\"FILTER\\\",\\\"INFO\\\"}\"\n        (\n          sed \"1d\" ${ref}/impute_{}_interval.snpstats | \\\n          awk -v i=${i} -v chunk_size=${chunk_size} -v OFS=\"\\t\" \"NR==(i-1)*chunk_size+1,NR==i*chunk_size {\n              if(\\$1==\\\".\\\") \\$1=\\$3+0 \\\":\\\" \\$4 \\\"_\\\" \\$5 \\\"/\\\" \\$6; print \\$3+0,\\$4,\\$1,\\$5,\\$6,\\\".\\\",\\\".\\\",\\$19}\"\n          if [ ${s} -gt 0 ] &amp;&amp;  [ ${i} -eq ${g} ]; then\n             sed \"1d\" ${ref}/impute_{}_interval.snpstats | \\\n             awk -v i=${i} -v chunk_size=${chunk_size} -v OFS=\"\\t\" -v n=${n} \"NR==i*chunk_size+1,NR==n {\n                 if(\\$1==\\\".\\\") \\$1=\\$3+0 \\\":\\\" \\$4 \\\"_\\\" \\$5 \\\"/\\\" \\$6; print \\$3+0,\\$4,\\$1,\\$5,\\$6,\\\".\\\",\\\".\\\",\\$19}\"\n          fi\n        )\n      ) | \\\n      vep  --cache --offline --format vcf -o - --tab --pick --no_stats  \\\n           --species homo_sapiens --assembly GRCh37 --port 3337 | \\\n      (\n        if [ ${i} -eq 1 ]; then cat; else grep -v \"#\"; fi\n      )\n    done\n  ) | \\\n  gzip -f &gt; work/INTERVAL-{}.vep.gz\n'</span>\n</code></pre> </div></div>\n\n<p>Note we use information from <code class=\"language-plaintext highlighter-rouge\">.snpstats</code> files at location <code class=\"language-plaintext highlighter-rouge\">ref</code> to build input in vcf format on the fly and feed into VEP. For instance <code class=\"language-plaintext highlighter-rouge\">impute_22_interval.snpstats</code> contains the following lines,</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>SNPID\tRSID\tchromosome\tposition\tA_allele\tB_allele\tminor_allele\tmajor_allele\tAA\tAB\tBB\tAA_calls\tAB_calls\tBB_calls\tMAF\tHWE\tmissing\tmissing_calls\tinformation\nrs587697622\trs587697622\t22\t16050075\tA\tG\tG\tA\t43058\t1.2499\t0\t43058\t1\t0\t1.4514e-05\t-0\t0\t0\t0.81003\nrs587755077\trs587755077\t22\t16050115\tG\tA\tA\tG\t42485\t572.42\t1.7625\t42105\t306\t1\t0.0066878\t0.14341\t3.5437e-09\t0.015026\t0.68672\nrs587654921\trs587654921\t22\t16050213\tC\tT\tT\tC\t42848\t211.12\t0.16248\t42746\t124\t0\t0.0024553\t-0\t1.4175e-09\t0.0043893\t0.67168\nrs587712275\trs587712275\t22\t16050319\tC\tT\tT\tC\t43022\t36.998\t0\t43022\t23\t0\t0.00042962\t-0\t0\t0.00032514\t0.69071\n</code></pre> </div></div>\n\n<h3 id=\"ensembl-synonym-translation\"><strong>ENSEMBL-synonym translation</strong></h3>\n\n<p>The ENSEMBL-synonym translation is useful to check for the feature types – in the case of ENSG00000160712 (IL6R)\nwe found ENST00000368485 and ENST00000515190, we do</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget http://hgdownload.soe.ucsc.edu/goldenPath/hg19/database/ensemblToGeneName.txt.gz\nzgrep <span class=\"nt\">-e</span> ENST00000368485 <span class=\"nt\">-e</span> ENST00000515190 ensemblToGeneName.txt.gz\n</code></pre> </div></div>\n\n<p>giving</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>ENST00000368485 IL6R\nENST00000515190 IL6R\n</code></pre> </div></div>\n\n<p>though this could also be furnished with R/biomaRt as follows,</p>\n\n<div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">biomaRt</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">ensembl</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">useMart</span><span class=\"p\">(</span><span class=\"s2\">\"ensembl\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"s2\">\"hsapiens_gene_ensembl\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">host</span><span class=\"o\">=</span><span class=\"s2\">\"grch37.ensembl.org\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s2\">\"/biomart/martservice\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">attr</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">listAttributes</span><span class=\"p\">(</span><span class=\"n\">ensembl</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">g</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">c</span><span class=\"p\">(</span><span class=\"s1\">'ensembl_gene_id'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">'chromosome_name'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">'start_position'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">'end_position'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">'description'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">'hgnc_symbol'</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">t</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">c</span><span class=\"p\">(</span><span class=\"s1\">'ensembl_transcript_id'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">'transcription_start_site'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">'transcript_start'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">'transcript_end'</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">u</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"s2\">\"uniprotswissprot\"</span><span class=\"w\">\n</span><span class=\"n\">gtu</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">getBM</span><span class=\"p\">(</span><span class=\"n\">attributes</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nf\">c</span><span class=\"p\">(</span><span class=\"n\">g</span><span class=\"p\">,</span><span class=\"n\">t</span><span class=\"p\">,</span><span class=\"n\">u</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">mart</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">ensembl</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre> </div></div>\n\n<p>For ENSEMBL genes, R/grex is likely to work though it was developed for other purpose, e.g.,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>R <span class=\"nt\">-q</span> <span class=\"nt\">-e</span> <span class=\"s2\">\"grex::grex(</span><span class=\"se\">\\\"</span><span class=\"s2\">ENSG00000160712</span><span class=\"se\">\\\"</span><span class=\"s2\">)\"</span>\n</code></pre> </div></div>\n\n<p>giving</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>       ensembl_id entrez_id hgnc_symbol              hgnc_name cyto_loc\n1 ENSG00000160712      3570        IL6R interleukin 6 receptor   1q21.3\n  uniprot_id   gene_biotype\n1     A0N0L5 protein_coding\n</code></pre> </div></div>\n\n<p>The aligment of ENSG, ENST, ENSP is <code class=\"language-plaintext highlighter-rouge\">ensGtp.txt.gz</code> at the same UCSC directory above.</p>\n\n<p>Other possibilities include UCSC/ensembl MySQL server, e.g.,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>mysql  <span class=\"nt\">--user</span><span class=\"o\">=</span>genome <span class=\"nt\">--host</span><span class=\"o\">=</span>genome-mysql.cse.ucsc.edu <span class=\"nt\">-A</span> <span class=\"nt\">-D</span> hg19 <span class=\"o\">&lt;&lt;</span><span class=\"no\">END</span><span class=\"sh\">\nselect distinct G.gene,N.value from ensGtp as G, ensemblToGeneName as N where\n   G.transcript=N.name and\n   G.gene in (\"ENSG00000160712\");\n</span><span class=\"no\">END\n</span></code></pre> </div></div>\n\n<p>and</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>mysql <span class=\"nt\">-h</span> ensembldb.ensembl.org <span class=\"nt\">--port</span> 5306  <span class=\"nt\">-u</span> anonymous <span class=\"nt\">-D</span> homo_sapiens_core_64_37 <span class=\"nt\">-A</span> <span class=\"o\">&lt;&lt;</span><span class=\"no\">END</span><span class=\"sh\">\nselect distinct\n   G.stable_id,\n   S.synonym\nfrom\n  gene_stable_id as G,\n  object_xref as OX,\n  external_synonym as S,\n  xref as X ,\n  external_db as D\nwhere\n  D.external_db_id=X.external_db_id and\n  X.xref_id=S.xref_id and\n  OX.xref_id=X.xref_id and\n  OX.ensembl_object_type=\"Gene\" and\n  G.gene_id=OX.ensembl_id and\n  G.stable_id in (\"ENSG00000160712\");\n</span><span class=\"no\">END\n</span></code></pre> </div></div>\n\n<p>according to <a href=\"https://www.biostars.org/p/14367/\">https://www.biostars.org/p/14367/</a>.</p>\n\n<p>Perhaps it is the easiest to use <code class=\"language-plaintext highlighter-rouge\">gprofiler2</code>, i.e.,</p>\n\n<div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">gprofiler2</span><span class=\"o\">::</span><span class=\"n\">gconvert</span><span class=\"p\">(</span><span class=\"s2\">\"ENSG00000164761\"</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre> </div></div>\n\n<p>giving</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>  input_number input target_number          target name                                               description                           namespace\n1            1  IL6R           1.1 ENSG00000160712 IL6R interleukin 6 receptor [Source:HGNC Symbol;Acc:HGNC:6019] ENTREZGENE,HGNC,UNIPROT_GN,WIKIGENE\n</code></pre> </div></div>\n\n<h3 id=\"clinvar\"><strong>clinvar</strong></h3>\n\n<p>Web: <a href=\"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/\">https://ftp.ncbi.nlm.nih.gov/pub/clinvar/</a>.</p>\n\n<p>The local installation enables considerable flexibilty, based on\n<a href=\"https://www.ensembl.org/info/docs/tools/vep/script/vep_custom.html#custom_options\">https://www.ensembl.org/info/docs/tools/vep/script/vep_custom.html#custom_options</a>.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># Compressed VCF file/Index file</span>\n<span class=\"c\"># GCRh37</span>\ncurl ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh37/clinvar.vcf.gz <span class=\"nt\">-o</span> clinvar_GRCh37.vcf.gz\ncurl ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh37/clinvar.vcf.gz.tbi <span class=\"nt\">-o</span> clinvar_GRCh37.vcf.gz.tbi\n<span class=\"c\"># GCRh38</span>\ncurl ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz <span class=\"nt\">-o</span> clinvar_GRCh38.vcf.gz\ncurl ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz.tbi <span class=\"nt\">-o</span> clinvar_GRCh38.vcf.gz.tbi\n</code></pre> </div></div>\n\n<p>Information is gathered from the header of the VCF file,</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>ClinVar Variation ID</th>\n      <th>Description</th>\n      <th> </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>AF_ESP</td>\n      <td>allele frequencies from GO-ESP</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>AF_EXAC</td>\n      <td>allele frequencies from ExAC</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>AF_TGP</td>\n      <td>allele frequencies from TGP</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>ALLELEID</td>\n      <td>the ClinVar Allele ID</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>CLNDN</td>\n      <td>ClinVar’s preferred disease name for the concept specified by disease identifiers in CLNDISDB</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>CLNDNINCL</td>\n      <td>For included Variant : ClinVar’s preferred disease name for the concept specified by disease identifiers in CLNDISDB</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>CLNDISDB</td>\n      <td>Tag-value pairs of disease database name and identifier, e.g. OMIM:NNNNNN</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>CLNDISDBINCL</td>\n      <td>For included Variant: Tag-value pairs of disease database name and identifier, e.g. OMIM:NNNNNN</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>CLNHGVS</td>\n      <td>Top-level (primary assembly, alt, or patch) HGVS expression.</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>CLNREVSTAT</td>\n      <td>ClinVar review status for the Variation ID</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>CLNSIG</td>\n      <td>Clinical significance for this single variant</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>CLNSIGCONF</td>\n      <td>Conflicting clinical significance for this single variant</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>CLNSIGINCL</td>\n      <td>Clinical significance for a haplotype or genotype that includes this variant. Reported as pairs of VariationID:clinical significance.</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>CLNVC</td>\n      <td>Variant type</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>CLNVCSO</td>\n      <td>Sequence Ontology id for variant type</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>CLNVI</td>\n      <td>the variant’s clinical sources reported as tag-value pairs of database and variant identifier</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>DBVARID</td>\n      <td>nsv accessions from dbVar for the variant</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>GENEINFO</td>\n      <td>Gene(s) for the variant reported as gene symbol:gene id. The gene symbol and id are delimited by a colon (:) and each pair is delimited by a vertical bar (</td>\n      <td>)</td>\n    </tr>\n    <tr>\n      <td>MC</td>\n      <td>comma separated list of molecular consequence in the form of Sequence Ontology ID</td>\n      <td>molecular_consequence</td>\n    </tr>\n    <tr>\n      <td>ORIGIN</td>\n      <td>Allele origin. One or more of the following values may be added: 0 - unknown; 1 - germline; 2 - somatic ; 4 - inherited; 8 - paternal; 16 - maternal; 32 - de-novo; 64 - biparental; 128 - uniparental; 256 - not-tested; 512 - tested-inconclusive; 1073741824 - other</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>RS</td>\n      <td>dbSNP ID (i.e. rs number)</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>SSR</td>\n      <td>Variant Suspect Reason Codes. One or more of the following values may be added: 0 - unspecified, 1 - Paralog, 2 - byEST, 4 - oldAlign, 8 - Para_EST, 16 - 1kg_failed, 1024 - other</td>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<p>We now query rs2228145,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>vep <span class=\"nt\">--id</span> <span class=\"s2\">\"1 154426970 154426970 A/C 1\"</span> <span class=\"nt\">--species</span> homo_sapiens <span class=\"nt\">-o</span> rs2228145 <span class=\"nt\">--cache</span> <span class=\"nt\">--offline</span> <span class=\"nt\">--force_overwrite</span> <span class=\"se\">\\</span>\n    <span class=\"nt\">--assembly</span> GRCh37 <span class=\"nt\">--custom</span> clinvar_GRCh37.vcf.gz,ClinVar,vcf,exact,0,CLNSIG,CLNREVSTAT,CLNDN\n</code></pre> </div></div>\n\n<p>which gives</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>#Uploaded_variation\tLocation\tAllele\tGene\tFeature\tFeature_type\tConsequence\tcDNA_position\tCDS_position\tProtein_position\tAmino_acids\tCodons\tExisting_variation\tExtra\n1_154426970_A/C\t1:154426970\tC\tENSG00000160712\tENST00000344086\tTranscript\tintron_variant\t-\t-\t-\t-\t-\t-\tIMPACT=MODIFIER;STRAND=1;ClinVar=14660;ClinVar_CLNDN=Interleukin_6,_serum_level_of,_quantitative_trait_locus|Soluble_interleukin-6_receptor,_serum_level_of,_quantitative_trait_locus;ClinVar_CLNREVSTAT=no_assertion_criteria_provided;ClinVar_CLNSIG=association;ClinVar_FILTER=.\n1_154426970_A/C\t1:154426970\tC\tENSG00000160712\tENST00000368485\tTranscript\tmissense_variant\t1510\t1073\t358\tD/A\tgAt/gCt\t-\tIMPACT=MODERATE;STRAND=1;ClinVar=14660;ClinVar_CLNDN=Interleukin_6,_serum_level_of,_quantitative_trait_locus|Soluble_interleukin-6_receptor,_serum_level_of,_quantitative_trait_locus;ClinVar_CLNREVSTAT=no_assertion_criteria_provided;ClinVar_CLNSIG=association;ClinVar_FILTER=.\n1_154426970_A/C\t1:154426970\tC\tENSG00000160712\tENST00000476006\tTranscript\tdownstream_gene_variant\t-\t-\t-\t-\t-\t-\tIMPACT=MODIFIER;DISTANCE=4515;STRAND=1;FLAGS=cds_start_NF,cds_end_NF;ClinVar=14660;ClinVar_CLNDN=Interleukin_6,_serum_level_of,_quantitative_trait_locus|Soluble_interleukin-6_receptor,_serum_level_of,_quantitative_trait_locus;ClinVar_CLNREVSTAT=no_assertion_criteria_provided;ClinVar_CLNSIG=association;ClinVar_FILTER=.\n1_154426970_A/C\t1:154426970\tC\tENSG00000160712\tENST00000502679\tTranscript\tnon_coding_transcript_exon_variant\t386\t-\t-\t-\t-\t-\tIMPACT=MODIFIER;STRAND=1;ClinVar=14660;ClinVar_CLNDN=Interleukin_6,_serum_level_of,_quantitative_trait_locus|Soluble_interleukin-6_receptor,_serum_level_of,_quantitative_trait_locus;ClinVar_CLNREVSTAT=no_assertion_criteria_provided;ClinVar_CLNSIG=association;ClinVar_FILTER=.\n1_154426970_A/C\t1:154426970\tC\tENSG00000160712\tENST00000507256\tTranscript\tnon_coding_transcript_exon_variant\t271\t-\t-\t-\t-\t-\tIMPACT=MODIFIER;STRAND=1;ClinVar=14660;ClinVar_CLNDN=Interleukin_6,_serum_level_of,_quantitative_trait_locus|Soluble_interleukin-6_receptor,_serum_level_of,_quantitative_trait_locus;ClinVar_CLNREVSTAT=no_assertion_criteria_provided;ClinVar_CLNSIG=association;ClinVar_FILTER=.\n1_154426970_A/C\t1:154426970\tC\tENSG00000160712\tENST00000515190\tTranscript\tmissense_variant\t481\t482\t161\tD/A\tgAt/gCt\t-\tIMPACT=MODERATE;STRAND=1;FLAGS=cds_start_NF,cds_end_NF;ClinVar=14660;ClinVar_CLNDN=Interleukin_6,_serum_level_of,_quantitative_trait_locus|Soluble_interleukin-6_receptor,_serum_level_of,_quantitative_trait_locus;ClinVar_CLNREVSTAT=no_assertion_criteria_provided;ClinVar_CLNSIG=association;ClinVar_FILTER=.\n</code></pre> </div></div>\n\n<p>A <a href=\"/applications/files/rs2228145_summary.html\">HTML summary</a> (somehow the web browser may not display the embedded figures) is also available. The <code class=\"language-plaintext highlighter-rouge\">Extra</code> column looks clumsy and we could add the <code class=\"language-plaintext highlighter-rouge\">--tab</code> option to generate a tab-delimited output.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>vep <span class=\"nt\">--id</span> <span class=\"s2\">\"1 154426970 154426970 A/C 1\"</span> <span class=\"nt\">--species</span> homo_sapiens <span class=\"nt\">-o</span> rs2228145 <span class=\"nt\">--cache</span> <span class=\"nt\">--offline</span> <span class=\"nt\">--force_overwrite</span> <span class=\"se\">\\</span>\n    <span class=\"nt\">--custom</span> clinvar_GRCh37.vcf.gz,ClinVar,vcf,exact,0,CLNSIG,CLNREVSTAT,CLNDN <span class=\"nt\">--tab</span> <span class=\"se\">\\</span>\n    <span class=\"nt\">--fields</span> Uploaded_variation,Gene,Consequence,ClinVar_CLNSIG,ClinVar_CLNREVSTAT,ClinVar_CLNDN\n</code></pre> </div></div>\n\n<p>to give neatly</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>#Uploaded_variation\tGene\tConsequence\tClinVar_CLNSIG\tClinVar_CLNREVSTAT\tClinVar_CLNDN\n1_154426970_A/C\tENSG00000160712\tintron_variant\tassociation\tno_assertion_criteria_provided\tInterleukin_6,_serum_level_of,_quantitative_trait_locus|Soluble_interleukin-6_receptor,_serum_level_of,_quantitative_trait_locus\n1_154426970_A/C\tENSG00000160712\tmissense_variant\tassociation\tno_assertion_criteria_provided\tInterleukin_6,_serum_level_of,_quantitative_trait_locus|Soluble_interleukin-6_receptor,_serum_level_of,_quantitative_trait_locus\n1_154426970_A/C\tENSG00000160712\tdownstream_gene_variant\tassociation\tno_assertion_criteria_provided\tInterleukin_6,_serum_level_of,_quantitative_trait_locus|Soluble_interleukin-6_receptor,_serum_level_of,_quantitative_trait_locus\n1_154426970_A/C\tENSG00000160712\tnon_coding_transcript_exon_variant\tassociation\tno_assertion_criteria_provided\tInterleukin_6,_serum_level_of,_quantitative_trait_locus|Soluble_interleukin-6_receptor,_serum_level_of,_quantitative_trait_locus\n1_154426970_A/C\tENSG00000160712\tnon_coding_transcript_exon_variant\tassociation\tno_assertion_criteria_provided\tInterleukin_6,_serum_level_of,_quantitative_trait_locus|Soluble_interleukin-6_receptor,_serum_level_of,_quantitative_trait_locus\n1_154426970_A/C\tENSG00000160712\tmissense_variant\tassociation\tno_assertion_criteria_provided\tInterleukin_6,_serum_level_of,_quantitative_trait_locus|Soluble_interleukin-6_receptor,_serum_level_of,_quantitative_trait_locus\n</code></pre> </div></div>\n\n<h3 id=\"dbnsfp\"><strong>dbNSFP</strong></h3>\n\n<p>Web page: <a href=\"https://sites.google.com/site/jpopgen/dbNSFP\">https://sites.google.com/site/jpopgen/dbNSFP</a>.</p>\n\n<p>This is set up as follows,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget ftp://dbnsfp:dbnsfp@dbnsfp.softgenetics.com/dbNSFP4.1a.zip\nunzip dbNSFP4.1a.zip <span class=\"nt\">-d</span> dbNSFP4.1a\n<span class=\"nb\">cd </span>dbNSFP4.1a\n<span class=\"c\"># code for 4.0a</span>\n<span class=\"c\"># zcat dbNSFP4.0a_variant.chr1.gz | head -n1 &gt; h</span>\n<span class=\"c\"># zgrep -h -v ^#chr dbNSFP4.0a_variant.chr* | sort -k1,1 -k2,2n - | cat h - | bgzip -c &gt; dbNSFP4.0a.gz</span>\n<span class=\"c\"># efficient version</span>\n<span class=\"o\">(</span>\n  zcat dbNSFP4.1a_variant.chr1.gz | <span class=\"nb\">head</span> <span class=\"nt\">-n1</span>\n  <span class=\"nb\">export </span><span class=\"nv\">prefix</span><span class=\"o\">=</span>dbNSFP4.1a_variant.chr\n  <span class=\"nb\">echo</span> <span class=\"k\">${</span><span class=\"nv\">prefix</span><span class=\"k\">}</span><span class=\"o\">{</span>1..22<span class=\"o\">}</span>.gz <span class=\"k\">${</span><span class=\"nv\">prefix</span><span class=\"k\">}</span>M.gz <span class=\"k\">${</span><span class=\"nv\">prefix</span><span class=\"k\">}</span>X.gz <span class=\"k\">${</span><span class=\"nv\">prefix</span><span class=\"k\">}</span>Y.gz | <span class=\"se\">\\</span>\n  xargs <span class=\"nt\">-I</span> <span class=\"o\">{}</span> bash <span class=\"nt\">-c</span> <span class=\"s2\">\"zcat {} | sed '1d'\"</span>\n<span class=\"o\">)</span> | bgzip <span class=\"nt\">-c</span> <span class=\"o\">&gt;</span> dbNSFP4.1a.gz\ntabix <span class=\"nt\">-s</span> 1 <span class=\"nt\">-b</span> 2 <span class=\"nt\">-e</span> 2 dbNSFP4.1a.gz\n<span class=\"nb\">cd</span> -\nvep <span class=\"nt\">-i</span> examples/homo_sapiens_GRCh37.vcf <span class=\"nt\">-o</span> <span class=\"nb\">test</span> <span class=\"nt\">--cache</span> <span class=\"nt\">--force_overwrite</span> <span class=\"nt\">--offline</span> <span class=\"se\">\\</span>\n    <span class=\"nt\">--plugin</span> dbNSFP,dbNSFP4.1a/dbNSFP4.1a.gz,LRT_score,FATHMM_score,MutationTaster_score\n</code></pre> </div></div>\n\n<p>Since this release is frozen on Ensembl 94’s transcript set, one may prefer to use it independently via its Java programs, e.g.,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>java <span class=\"nt\">-jar</span> search_dbNSFP41a.jar <span class=\"nt\">-i</span> tryhg19.in <span class=\"nt\">-o</span> tryhg19.out <span class=\"nt\">-v</span> hg19\njava <span class=\"nt\">-jar</span> search_dbNSFP41a.jar <span class=\"nt\">-i</span> tryhg38.in <span class=\"nt\">-o</span> tryhg38.out\n</code></pre> </div></div>\n\n<h3 id=\"genesplicer\"><strong>GeneSplicer</strong></h3>\n\n<p>Web: <a href=\"https://ccb.jhu.edu/software/genesplicer/\">https://ccb.jhu.edu/software/genesplicer/</a>.</p>\n\n<h4 id=\"setup\">Setup</h4>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget <span class=\"nt\">-qO-</span> ftp://ftp.ccb.jhu.edu/pub/software/genesplicer/GeneSplicer.tar.gz | <span class=\"se\">\\</span>\n<span class=\"nb\">tar </span>xvfz -\n<span class=\"nb\">mv </span>GeneSplicer.pm ~/.vep/Plugins\n./vep <span class=\"nt\">-i</span> variants.vcf <span class=\"nt\">--plugin</span> GeneSplicer,[path_to_genesplicer_bin],[path_to_training_dir],[option1<span class=\"o\">=</span>value],[option2<span class=\"o\">=</span>value]\n</code></pre> </div></div>\n\n<h4 id=\"reference\">Reference</h4>\n\n<p>M. Pertea , X. Lin , S. L. Salzberg. GeneSplicer: a new computational method for splice site prediction. Nucleic Acids Res. 2001 Mar 1;29(5):1185-90.</p>\n\n<h3 id=\"loftee\"><strong>loftee</strong></h3>\n\n<p>GitHub page: <a href=\"https://github.com/konradjk/loftee\">https://github.com/konradjk/loftee</a>.</p>\n\n<p>Reference: MacArthur DG et al (2012). A systematic survey of loss-of-function variants in human protein-coding genes. <em>Science</em> 335:823–828</p>\n\n<p>It is actually part of the standard VEP plugins.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">cd </span>loftee\n<span class=\"c\"># human_ancestor_fa</span>\n<span class=\"c\">## samtools --version gives 0.1.19</span>\nwget http://www.broadinstitute.org/~konradk/loftee/human_ancestor.fa.rz\nwget http://www.broadinstitute.org/~konradk/loftee/human_ancestor.fa.rz.fai\n<span class=\"c\">## samtools --version gives 1.x</span>\nwget https://s3.amazonaws.com/bcbio_nextgen/human_ancestor.fa.gz\nwget https://s3.amazonaws.com/bcbio_nextgen/human_ancestor.fa.gz.fai\nwget https://s3.amazonaws.com/bcbio_nextgen/human_ancestor.fa.gz.gzi\n<span class=\"c\"># conservation_file -- note the data, schema and GERP files are required only to build the sql file</span>\nwget <span class=\"nt\">-qO-</span> https://personal.broadinstitute.org/konradk/loftee_data/GRCh37/phylocsf_gerp.sql.gz | <span class=\"se\">\\</span>\n<span class=\"nb\">gunzip</span> <span class=\"nt\">-c</span> <span class=\"o\">&gt;</span> phylocsf_gerp.sql\n<span class=\"c\"># wget https://www.broadinstitute.org/~konradk/loftee/phylocsf_data.tsv.gz</span>\n<span class=\"c\"># wget https://www.broadinstitute.org/~konradk/loftee/phylocsf_data_schema.sql</span>\n<span class=\"c\"># wget https://personal.broadinstitute.org/konradk/loftee_data/GRCh37/GERP_scores.final.sorted.txt.gz</span>\n<span class=\"c\"># wget https://personal.broadinstitute.org/konradk/loftee_data/GRCh37/GERP_scores.exons.txt.gz</span>\n<span class=\"c\"># annotation</span>\nvep <span class=\"nt\">--id</span> <span class=\"s2\">\"1 154426970 154426970 A/C 1\"</span> <span class=\"nt\">--species</span> homo_sapiens <span class=\"nt\">-o</span> rs2228145 <span class=\"nt\">--cache</span> <span class=\"nt\">--offline</span> <span class=\"nt\">--force_overwrite</span> <span class=\"se\">\\</span>\n    <span class=\"nt\">--assembly</span> GRCh37 <span class=\"nt\">--plugin</span> LoF,loftee_path:.,human_ancestor_fa:human_ancestor.fa.gz,conservation_file:phylocsf_gerp.sql.gz\n<span class=\"c\"># VEP documentation example</span>\nvep <span class=\"nt\">--input_file</span> homo_sapiens_GRCh37.vcf <span class=\"nt\">--output_file</span> <span class=\"nb\">test</span> <span class=\"nt\">--cache</span> <span class=\"nt\">--dir_cache</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>/ensembl-vep/.vep <span class=\"nt\">--dir_plugins</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>/loftee <span class=\"nt\">--offline</span> <span class=\"se\">\\</span>\n    <span class=\"nt\">--pick</span> <span class=\"nt\">--force_overwrite</span> <span class=\"nt\">--species</span> homo_sapiens <span class=\"nt\">--assembly</span> GRCh37 <span class=\"se\">\\</span>\n    <span class=\"nt\">--plugin</span> LoF,loftee_path:.,human_ancestor_fa:human_ancestor.fa.gz,conservation_file:phylocsf_gerp.sql\n</code></pre> </div></div>\n\n<p>If offers implementation for parsing CSQ field but is also possible with R as described below. Note that if loftee_path uses an absolute path, that path should also be within PERL5LIB, e.g.,</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>export PERL5LIB=$PERL5LIB:$HPC_WORK/loftee\n</code></pre> </div></div>\n\n<p>is put in .bashrc. We see now all files are provided, and there are complaints over rz was not generated by bgzip, so we do</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># error</span>\nsamtools faidx human_ancestor.fa.rz\n<span class=\"c\"># rework</span>\n<span class=\"nb\">gunzip</span> <span class=\"nt\">-c</span> human_ancestor.fa.rz | bgzip <span class=\"nt\">-c</span> <span class=\"o\">&gt;</span> output.fa.gz\n<span class=\"c\"># check and put it back</span>\nll human_ancestor.fa.rz output.fa.gz\n<span class=\"nb\">mv </span>output.fa.gz human_ancestor.fa.rz\n<span class=\"c\"># now possible and check</span>\nsamtools faidx human_ancestor.fa.rz\nll human_ancestor.fa.<span class=\"k\">*</span>\n</code></pre> </div></div>\n\n<h3 id=\"bigwig-file\"><strong>BigWig file</strong></h3>\n\n<p>One can have additional features installed such as JSON, Set::IntervalTree, Bio::DB::BigFile, PerlIO::gzip and ensembl-xs. We exemplify JSON and Bio::DB::BigFile here,</p>\n\n<p>Also see <a href=\"https://www.ensembl.org/info/docs/tools/vep/script/vep_download.html#bigfile\">https://www.ensembl.org/info/docs/tools/vep/script/vep_download.html#bigfile</a>.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># 1. Download and extract source code</span>\n<span class=\"nb\">cd</span> <span class=\"nv\">$HOME</span>\nwget <span class=\"nt\">-qO-</span> https://github.com/ucscGenomeBrowser/kent/archive/v335_base.tar.gz | <span class=\"se\">\\</span>\n<span class=\"nb\">tar </span>xzf -\n<span class=\"c\"># 2. Set up compiling flags</span>\n<span class=\"nb\">export </span><span class=\"nv\">KENT_SRC</span><span class=\"o\">=</span><span class=\"nv\">$HOME</span>/kent-335_base/src\n<span class=\"nb\">export </span><span class=\"nv\">MACHTYPE</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">uname</span> <span class=\"nt\">-m</span><span class=\"si\">)</span>\n<span class=\"nb\">export </span><span class=\"nv\">CFLAGS</span><span class=\"o\">=</span><span class=\"s2\">\"-fPIC\"</span>\n<span class=\"nb\">export </span><span class=\"nv\">MYSQLINC</span><span class=\"o\">=</span><span class=\"sb\">`</span>mysql_config <span class=\"nt\">--include</span> | <span class=\"nb\">sed</span> <span class=\"nt\">-e</span> <span class=\"s1\">'s/^-I//g'</span><span class=\"sb\">`</span>\n<span class=\"nb\">export </span><span class=\"nv\">MYSQLLIBS</span><span class=\"o\">=</span><span class=\"sb\">`</span>mysql_config <span class=\"nt\">--libs</span><span class=\"sb\">`</span>\n<span class=\"c\"># 3. Amend parameters</span>\n<span class=\"nb\">cd</span> <span class=\"nv\">$KENT_SRC</span>/lib\n<span class=\"nb\">echo</span> <span class=\"s1\">'CFLAGS=\"-fPIC\"'</span> <span class=\"o\">&gt;</span> ../inc/localEnvironment.mk\n<span class=\"c\"># 4. Build library</span>\nmake clean <span class=\"o\">&amp;&amp;</span> make\n<span class=\"nb\">cd</span> ../jkOwnLib\nmake clean <span class=\"o\">&amp;&amp;</span> make\n<span class=\"c\"># 5. On Mac OSX</span>\n<span class=\"nb\">ln</span> <span class=\"nt\">-s</span> <span class=\"nv\">$KENT_SRC</span>/lib/x86_64/<span class=\"k\">*</span> <span class=\"nv\">$KENT_SRC</span>/lib/\n<span class=\"c\"># 6. Install Perl modules</span>\n<span class=\"nb\">cd</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>/ensembl-vep\ncpan JSON\ncpan Bio::DB::BigFile\n<span class=\"c\"># 7. Test</span>\nperl <span class=\"nt\">-Imodules</span> t/AnnotationSource_File_BigWig.t\n</code></pre> </div></div>\n\n<p>We have from step 7 above</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>ok 1 - use Bio::EnsEMBL::VEP::AnnotationSource::File;\nok 2 - use Bio::EnsEMBL::VEP::AnnotationSource::File::BigWig;\nok 3 - use Bio::EnsEMBL::VEP::Config;\nok 4 - get new config object\nok 5 - new is defined\nCouldn't open foo\nok 6 - new with invalid file throws\nok 7 - use Bio::EnsEMBL::VEP::Parser::VCF;\nok 8 - get parser object\nok 9 - use Bio::EnsEMBL::VEP::InputBuffer;\nok 10 - check class\nok 11 - check buffer next\nok 12 - annotate_InputBuffer - overlap\nok 13 - annotate_InputBuffer - exact, additive\nok 14 - annotate_InputBuffer - out by 1 (5')\nok 15 - annotate_InputBuffer - out by 1 (3')\nok 16 - overlap fixedStep\n1..16\n</code></pre> </div></div>\n\n<p>We have the GRCh38 branch, <code class=\"language-plaintext highlighter-rouge\">git clone --depth 1 --branch grch38 https://github.com/konradjk/loftee.git</code>.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># GRCh38 files</span>\n<span class=\"c\"># wget https://personal.broadinstitute.org/konradk/loftee_data/GRCh38/gerp_conservation_scores.homo_sapiens.GRCh38.bw</span>\n<span class=\"c\"># wget https://personal.broadinstitute.org/konradk/loftee_data/GRCh38/human_ancestor.fa.gz</span>\n<span class=\"c\"># wget https://personal.broadinstitute.org/konradk/loftee_data/GRCh38/human_ancestor.fa.gz.fai</span>\n<span class=\"c\"># wget https://personal.broadinstitute.org/konradk/loftee_data/GRCh38/human_ancestor.fa.gz.gzi</span>\n<span class=\"c\"># wget https://personal.broadinstitute.org/konradk/loftee_data/GRCh38/loftee.sql.gz</span>\n\n<span class=\"c\"># Annotation</span>\n<span class=\"nb\">export </span><span class=\"nv\">ENSEMBL</span><span class=\"o\">=</span>~/rds/rds-jmmh2-public_databases/ensembl-vep\n<span class=\"nb\">export </span><span class=\"nv\">LOFTEE38</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">ENSEMBL</span><span class=\"k\">}</span>/loftee/loftee_data/GRCh38\n<span class=\"nb\">export </span><span class=\"nv\">LOFTEE38GERP</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">LOFTEE38</span><span class=\"k\">}</span>/gerp_conservation_scores.homo_sapiens.GRCh38.bw\n<span class=\"nb\">export </span><span class=\"nv\">LOFTEE38HA</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">LOFTEE38</span><span class=\"k\">}</span>/human_ancestor.fa.gz\n<span class=\"nb\">export </span><span class=\"nv\">LOFTEE38SQL</span><span class=\"o\">=</span><span class=\"k\">${</span><span class=\"nv\">LOFTEE38</span><span class=\"k\">}</span>/loftee.sql\n<span class=\"nb\">export </span><span class=\"nv\">rds</span><span class=\"o\">=</span>..  <span class=\"c\"># ~/rds/rds-jmmh2-public_databases/ensembl-vep will be user-specific</span>\nvep <span class=\"nt\">--input_file</span> for_VEP.txt <span class=\"nt\">--format</span> ensembl <span class=\"nt\">--output_file</span> <span class=\"k\">${</span><span class=\"nv\">rds</span><span class=\"k\">}</span>/for_VEP_output2.txt <span class=\"nt\">--force_overwrite</span> <span class=\"nt\">--offline</span> <span class=\"nt\">--symbol</span> <span class=\"nt\">--merged</span> <span class=\"se\">\\</span>\n    <span class=\"nt\">--fasta</span> Homo_sapiens.GRCh38.dna.toplevel.fa <span class=\"nt\">--dir_cache</span> <span class=\"k\">${</span><span class=\"nv\">rds</span><span class=\"k\">}</span>/.vep <span class=\"nt\">--dir_plugins</span> <span class=\"nb\">.</span> <span class=\"se\">\\</span>\n    <span class=\"nt\">--protein</span> <span class=\"nt\">--symbol</span> <span class=\"nt\">--tsl</span> <span class=\"nt\">--canonical</span> <span class=\"nt\">--mane_select</span> <span class=\"nt\">--biotype</span> <span class=\"nt\">--check_existing</span> <span class=\"nt\">--sift</span> b <span class=\"nt\">--polyphen</span> b <span class=\"se\">\\</span>\n    <span class=\"nt\">--plugin</span> LoF,loftee_path:.,gerp_bigwig:<span class=\"k\">${</span><span class=\"nv\">LOFTEE38GERP</span><span class=\"k\">}</span>,human_ancestor_fa:<span class=\"k\">${</span><span class=\"nv\">LOFTEE38HA</span><span class=\"k\">}</span>,conservation_file:<span class=\"k\">${</span><span class=\"nv\">LOFTEE38SQL</span><span class=\"k\">}</span>\n</code></pre> </div></div>\n\n<p>See also <a href=\"https://docs.databricks.com/applications/genomics/secondary/vep-pipeline.html\">https://docs.databricks.com/applications/genomics/secondary/vep-pipeline.html</a>.</p>\n\n<h3 id=\"revel\"><strong>REVEL</strong></h3>\n\n<p>REVEL: Rare Exome Variant Ensemble Learning</p>\n\n<p>Plugin: <a href=\"https://m.ensembl.org/info/docs/tools/vep/script/vep_plugins.html#revel\">https://m.ensembl.org/info/docs/tools/vep/script/vep_plugins.html#revel</a></p>\n\n<p>Data: <a href=\"https://sites.google.com/site/revelgenomics/downloads\">https://sites.google.com/site/revelgenomics/downloads</a>.</p>\n\n<h4 id=\"setup-1\">Setup</h4>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">mkdir </span>REVEL\n<span class=\"nb\">cd </span>REVEL\nwget https://rothsj06.u.hpc.mssm.edu/revel-v1.3_all_chromosomes.zip\nunzip revel-v1.3_all_chromosomes.zip\n<span class=\"nb\">cat </span>revel_with_transcript_ids | <span class=\"nb\">tr</span> <span class=\"s2\">\",\"</span> <span class=\"s2\">\"</span><span class=\"se\">\\t</span><span class=\"s2\">\"</span> <span class=\"o\">&gt;</span> tabbed_revel.tsv\n<span class=\"nb\">sed</span> <span class=\"s1\">'1s/.*/#&amp;/'</span> tabbed_revel.tsv <span class=\"o\">&gt;</span> new_tabbed_revel.tsv\nbgzip new_tabbed_revel.tsv\n<span class=\"c\"># GRCh37:</span>\ntabix <span class=\"nt\">-f</span> <span class=\"nt\">-s</span> 1 <span class=\"nt\">-b</span> 2 <span class=\"nt\">-e</span> 2 new_tabbed_revel.tsv.gz\n<span class=\"c\"># GRCh38:</span>\nzcat new_tabbed_revel.tsv.gz | <span class=\"nb\">head</span> <span class=\"nt\">-n1</span> <span class=\"o\">&gt;</span> h\nzgrep <span class=\"nt\">-h</span> <span class=\"nt\">-v</span> ^#chr new_tabbed_revel.tsv.gz | <span class=\"nb\">awk</span> <span class=\"s1\">'$3 != \".\" '</span> | <span class=\"nb\">sort</span> <span class=\"nt\">-k1</span>,1 <span class=\"nt\">-k3</span>,3n - | <span class=\"nb\">cat </span>h - | bgzip <span class=\"nt\">-c</span> <span class=\"o\">&gt;</span> new_tabbed_revel_grch38.tsv.gz\ntabix <span class=\"nt\">-f</span> <span class=\"nt\">-s</span> 1 <span class=\"nt\">-b</span> 3 <span class=\"nt\">-e</span> 3 new_tabbed_revel_grch38.tsv.gz\n</code></pre> </div></div>\n\n<h4 id=\"example\">Example</h4>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>vep <span class=\"nt\">--input_file</span> &lt;input&gt; <span class=\"nt\">--plugin</span> REVEL,<span class=\"k\">${</span><span class=\"nv\">ENSEMBL</span><span class=\"k\">}</span>/.vep/Plugins/new_tabbed_revel.tsv.gz\n</code></pre> </div></div>\n\n<h4 id=\"reference-1\">Reference</h4>\n\n<p>Ioannidis NM, et al. REVEL: An ensemble method for predicting the pathogenicity of rare missense variants. <em>Am J Hum Genet</em> 2016; 99(4):877-885. http://dx.doi.org/10.1016/j.ajhg.2016.08.016</p>\n\n<h2 id=\"-r-\">— R —</h2>\n\n<p>This is a wrapper and <code class=\"language-plaintext highlighter-rouge\">the Ensembl VEP perl script must be installed in your path</code>. Expected to be slower than the <code class=\"language-plaintext highlighter-rouge\">--offline</code> mode above, it is\nrelatively easy to set up,</p>\n\n<div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">BiocManager</span><span class=\"o\">::</span><span class=\"n\">install</span><span class=\"p\">(</span><span class=\"s2\">\"ensemblVEP\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">vignette</span><span class=\"p\">(</span><span class=\"s2\">\"ensemblVEP\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">ensemblVEP</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">file</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">system.file</span><span class=\"p\">(</span><span class=\"s2\">\"extdata\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s2\">\"ex2.vcf\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">package</span><span class=\"o\">=</span><span class=\"s2\">\"VariantAnnotation\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">param</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">VEPFlags</span><span class=\"p\">(</span><span class=\"n\">flags</span><span class=\"o\">=</span><span class=\"nf\">list</span><span class=\"p\">(</span><span class=\"n\">vcf</span><span class=\"o\">=</span><span class=\"kc\">TRUE</span><span class=\"p\">,</span><span class=\"n\">check_existing</span><span class=\"o\">=</span><span class=\"kc\">TRUE</span><span class=\"p\">,</span><span class=\"n\">symbol</span><span class=\"o\">=</span><span class=\"kc\">TRUE</span><span class=\"p\">,</span><span class=\"n\">terms</span><span class=\"o\">=</span><span class=\"s2\">\"SO\"</span><span class=\"p\">,</span><span class=\"n\">sift</span><span class=\"o\">=</span><span class=\"s2\">\"b\"</span><span class=\"p\">,</span><span class=\"n\">polyphen</span><span class=\"o\">=</span><span class=\"s2\">\"p\"</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"n\">vep</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">ensemblVEP</span><span class=\"p\">(</span><span class=\"n\">file</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">param</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">vep</span><span class=\"p\">)</span><span class=\"o\">$</span><span class=\"n\">CSQ</span><span class=\"w\">\n</span><span class=\"n\">csq</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">parseCSQToGRanges</span><span class=\"p\">(</span><span class=\"n\">vep</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">head</span><span class=\"p\">(</span><span class=\"n\">csq</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"m\">2</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"c1\"># clinvar</span><span class=\"w\">\n</span><span class=\"n\">param</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">VEPFlags</span><span class=\"p\">(</span><span class=\"n\">flags</span><span class=\"o\">=</span><span class=\"nf\">list</span><span class=\"p\">(</span><span class=\"n\">vcf</span><span class=\"o\">=</span><span class=\"kc\">TRUE</span><span class=\"p\">,</span><span class=\"n\">output_file</span><span class=\"o\">=</span><span class=\"s2\">\"clinvar.vcf\"</span><span class=\"p\">,</span><span class=\"n\">force_overwrite</span><span class=\"o\">=</span><span class=\"kc\">TRUE</span><span class=\"p\">,</span><span class=\"n\">assembly</span><span class=\"o\">=</span><span class=\"s2\">\"GRCh37\"</span><span class=\"p\">,</span><span class=\"n\">port</span><span class=\"o\">=</span><span class=\"m\">3337</span><span class=\"p\">,</span><span class=\"w\">\n                  </span><span class=\"n\">custom</span><span class=\"o\">=</span><span class=\"s2\">\"clinvar_GRCh37.vcf.gz,ClinVar,vcf,exact,0,CLNSIG,CLNREVSTAT,CLNDN\"</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"n\">ensemblVEP</span><span class=\"p\">(</span><span class=\"n\">file</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">param</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre> </div></div>\n\n<p>The second <code class=\"language-plaintext highlighter-rouge\">ensemblVEP</code> obtains <code class=\"language-plaintext highlighter-rouge\">clinvar.vcf</code> and <code class=\"language-plaintext highlighter-rouge\">clinvar.vcf_summary.html</code>. Annotation is made to a VCF file, and returns data with unparsed ‘CSQ’.</p>\n\n<p>The facility to parse the CSQ column of a VCF object is done for the documentation example.</p>\n\n<div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># VCF output from the VEP web interface or the call above</span><span class=\"w\">\n</span><span class=\"n\">vep</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"s2\">\"INF1.merge.trans.vcf\"</span><span class=\"w\">\n</span><span class=\"c1\"># Parse into a GRanges and include the 'VCFRowID' column.</span><span class=\"w\">\n</span><span class=\"n\">vcf</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">readVcf</span><span class=\"p\">(</span><span class=\"n\">vep</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s2\">\"hg19\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">csq</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">parseCSQToGRanges</span><span class=\"p\">(</span><span class=\"n\">vep</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">VCFRowID</span><span class=\"o\">=</span><span class=\"n\">rownames</span><span class=\"p\">(</span><span class=\"n\">vcf</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"n\">write.table</span><span class=\"p\">(</span><span class=\"n\">mcols</span><span class=\"p\">(</span><span class=\"n\">csq</span><span class=\"p\">),</span><span class=\"n\">file</span><span class=\"o\">=</span><span class=\"s2\">\"INF1.merge.trans.txt\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">quote</span><span class=\"o\">=</span><span class=\"kc\">FALSE</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">sep</span><span class=\"o\">=</span><span class=\"s2\">\"\\t\"</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre> </div></div>\n\n<p>The dbNSFP counterpart is also possible</p>\n\n<div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">BiocManager</span><span class=\"o\">::</span><span class=\"n\">install</span><span class=\"p\">(</span><span class=\"s2\">\"myvariant\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">VariantAnnotation</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">file</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">system.file</span><span class=\"p\">(</span><span class=\"s2\">\"extdata\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s2\">\"dbsnp_mini.vcf\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">package</span><span class=\"o\">=</span><span class=\"s2\">\"myvariant\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">vcf</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">readVcf</span><span class=\"p\">(</span><span class=\"n\">file</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">genome</span><span class=\"o\">=</span><span class=\"s2\">\"hg19\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">rowRanges</span><span class=\"p\">(</span><span class=\"n\">vcf</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">myvariant</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">hgvs</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">formatHgvs</span><span class=\"p\">(</span><span class=\"n\">vcf</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">variant_type</span><span class=\"o\">=</span><span class=\"s2\">\"snp\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">head</span><span class=\"p\">(</span><span class=\"n\">hgvs</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">getVariants</span><span class=\"p\">(</span><span class=\"n\">hgvs</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">rsids</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">paste</span><span class=\"p\">(</span><span class=\"s2\">\"rs\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">vcf</span><span class=\"p\">)</span><span class=\"o\">$</span><span class=\"n\">RS</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">sep</span><span class=\"o\">=</span><span class=\"s2\">\"\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">head</span><span class=\"p\">(</span><span class=\"n\">rsids</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">res</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">queryVariants</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"o\">=</span><span class=\"n\">rsids</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">scopes</span><span class=\"o\">=</span><span class=\"s2\">\"dbsnp.rsid\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">fields</span><span class=\"o\">=</span><span class=\"s2\">\"all\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">fields</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">names</span><span class=\"p\">(</span><span class=\"n\">res</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">cadd</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">grep</span><span class=\"p\">(</span><span class=\"s1\">'cadd'</span><span class=\"p\">,</span><span class=\"n\">fields</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">res</span><span class=\"p\">[</span><span class=\"n\">fields</span><span class=\"p\">[</span><span class=\"n\">cadd</span><span class=\"p\">]]</span><span class=\"w\">\n</span></code></pre> </div></div>\n\n<p>Note that the CSQ field could also be handled by bcftools split-vep plugin, see <a href=\"http://samtools.github.io/bcftools/howtos/plugin.split-vep.html\">http://samtools.github.io/bcftools/howtos/plugin.split-vep.html</a>.</p>\n\n<h2 id=\"-docker-\">— docker —</h2>\n\n<p>See <code class=\"language-plaintext highlighter-rouge\">docker/Dockerfile </code> from the GitHub directory above, or <a href=\"https://github.com/Ensembl/ensembl-vep\">https://github.com/Ensembl/ensembl-vep</a>.</p>\n\n<h2 id=\"-virtual-machine-\">— Virtual machine —</h2>\n\n<p>See <a href=\"http://www.ensembl.org/info/data/virtual_machine.html\">http://www.ensembl.org/info/data/virtual_machine.html</a> which is possibly best for MicroSoft Windows and is not pursued here.</p>\n\n<p>ENSEMBL-synonym translation (hg19) file</p>\n","dir":"/applications/","name":"VEP.md","path":"applications/VEP.md","url":"/applications/VEP.html"},{"sort":41,"layout":"default","title":"Visual Studio code","content":"<h1 id=\"visual-studio-code\">Visual Studio code</h1>\n\n<p>Web: <a href=\"https://code.visualstudio.com/\">https://code.visualstudio.com/</a>.</p>\n\n<h2 id=\"installation\">Installation</h2>\n\n<p>Download and extract a copy for Linux x64, e.g., for the latest version 1.58.0,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">cd</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>\n<span class=\"nb\">tar </span>xvfz code-stable-x64-1625728370.tar.gz\n<span class=\"nb\">ln</span> <span class=\"nt\">-sf</span> <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>/VSCode-linux-x64/bin/code <span class=\"k\">${</span><span class=\"nv\">HPC_WORK</span><span class=\"k\">}</span>/bin/code\n</code></pre> </div></div>\n\n<p>By default it requires <code class=\"language-plaintext highlighter-rouge\">chrome-sandbox is owned by root and has mode 4755</code> which could be achieved at CSD3 by</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">sudo chown </span>root:root chrome-sandbox\n<span class=\"nb\">sudo chmod </span>4755 chrome-sandbox\n</code></pre> </div></div>\n\n<p>Unfortunately, as ordinary users this is impossible and we use without this option.</p>\n\n<h2 id=\"execution\">Execution</h2>\n\n<p>Start for reasons above,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>code <span class=\"nt\">--no-sandbox</span>\n</code></pre> </div></div>\n\n<h2 id=\"jupyter-notebook\">Jupyter notebook</h2>\n\n<p>We could start our virtual environment as described in <code class=\"language-plaintext highlighter-rouge\">hail</code>, e.g.,</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">cd</span> ~/rds/results/public/gwas/ukb_exomes/tutorials\ncode <span class=\"nt\">--no-sandbox</span> 01-genome-wide-association-study.ipynb &amp;\n</code></pre> </div></div>\n\n<p>which greatly simplifies the procedure as described for <code class=\"language-plaintext highlighter-rouge\">genebass</code>. MAKE sure various extensions/options suggesed from the session.</p>\n","dir":"/applications/","name":"VScode.md","path":"applications/VScode.md","url":"/applications/VScode.html"}]